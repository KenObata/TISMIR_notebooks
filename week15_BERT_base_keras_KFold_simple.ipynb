{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KenObata/TISMIR_notebooks/blob/main/week15_BERT_base_keras_KFold_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS8WVxEoWZG0"
      },
      "source": [
        "## Week15: This notebook uses Pre-Trained BERT\n",
        "\n",
        "Situation: English only (=multi-class).\n",
        "Split: StratifiedKfold.\n",
        "Reference: https://github.com/jasonwei20/eda_nlp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWL5DlwVTHgV"
      },
      "source": [
        "### set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8Fdw4QzS4FD",
        "outputId": "22287fdb-27bc-4736-c133-7c42a4585be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 6.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install scikit-multilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1d5F3EmWPVWZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from collections import Counter\n",
        "\n",
        "from skmultilearn.model_selection import IterativeStratification   \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.sparse import csr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "DIR = '/content/drive/MyDrive/music4all/'\n",
        "def get_balanced_accuracy(model, McNemar, is_fine_tuning, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning):\n",
        "  test_y = test.map(map_func_only_y)\n",
        "  y_category=np.zeros((TEST_SIZE, ))\n",
        "  counter=0\n",
        "  for label_tensor in test_y.take(len(test_y)):\n",
        "    y_test = np.argmax(label_tensor, axis=1)\n",
        "    for label in y_test:\n",
        "      y_category[counter]=label\n",
        "      counter+=1\n",
        "\n",
        "  X_test, y_test = test.map(map_func_only_X), y_category\n",
        "  y_predict_test = np.asarray(model.predict(X_test))\n",
        "  y_predict_test = np.argmax(y_predict_test, axis=1)\n",
        "  print(classification_report(y_test, y_predict_test) )\n",
        "  print(balanced_accuracy_score(y_test, y_predict_test))\n",
        "\n",
        "  McNemar[(is_fine_tuning, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning)] = []\n",
        "  for ground_truh, pred in zip(y_test, y_predict_test):\n",
        "        if ground_truh==pred:\n",
        "          McNemar[(is_fine_tuning, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning)].append(True)\n",
        "        else:\n",
        "          McNemar[(is_fine_tuning, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning)].append(False)\n",
        "  with open(DIR+ \"BERT_simple_log.txt\", \"a\") as f:\n",
        "    print(\"======================================\", file=f)\n",
        "    print(\"is_fine_tuning?:\", is_fine_tuning, \"drop_out_rate: \", drop_out_rate, \"learning_rate_transfer_learning: \", learning_rate_transfer_learning,\n",
        "          \"learning_rate_fine_tuning: \", learning_rate_fine_tuning, file=f)\n",
        "    print(classification_report(y_test, y_predict_test) , file=f)\n",
        "    print(balanced_accuracy_score(y_test, y_predict_test), file=f)\n",
        "\n",
        "  return balanced_accuracy_score(y_test, y_predict_test), McNemar"
      ],
      "metadata": {
        "id": "K6VTlTxg8JVQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save output into text\n",
        "DIR = '/content/drive/MyDrive/music4all/'"
      ],
      "metadata": {
        "id": "nxtC5_Sgr2tL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWxfNJqYBfjD"
      },
      "source": [
        "### Data Preparation(Kfold split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbT7Qs4whnTX"
      },
      "source": [
        "Create dataframe for Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Le3tiKjOOp19",
        "outputId": "dbf76afe-6fd8-430b-b838-0222365ae610"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0                id             genres lang  \\\n",
              "0               0  0009fFIM1eYThaPg                pop   en   \n",
              "1               1  00P2bHdWFkghmDqz               soul   en   \n",
              "2               2  00b6fV3nx5z2b8Ls                pop   en   \n",
              "3               3  013QDoTqbexEwkHr                pop   en   \n",
              "4               4  01EKNot8qVgZpKM7               rock   en   \n",
              "...           ...               ...                ...  ...   \n",
              "13535       13535  zzT504Z94j1IAuc3         indie rock   en   \n",
              "13536       13536  zzgS4ZqyswamEWNj                pop   en   \n",
              "13537       13537  zzx8CWdM7qkxKQpC         indie rock   en   \n",
              "13538       13538  zzz0n04uuTUA7fNh                pop   en   \n",
              "13539       13539  zzzj3LYaZtYtbzSr  singer-songwriter   en   \n",
              "\n",
              "                                                   lyric  number_of_line  \n",
              "0      a sunny day so I got nowhere to hide Not a clo...              91  \n",
              "1      Tell me a tale that always was Sing me a song ...              36  \n",
              "2      A buh A buh You went to school to learn girl T...              74  \n",
              "3      like a conversation where stops to breathe Is ...              20  \n",
              "4      Say the words I cannot say Say them on another...              31  \n",
              "...                                                  ...             ...  \n",
              "13535  think what afraid of come in you know been mad...              18  \n",
              "13536  Oh yeah yeah Last night I took a walk in the s...              75  \n",
              "13537  Innocence it come easy in a sense it never wil...              34  \n",
              "13538  Girl you know how I feel I really Since you be...              65  \n",
              "13539  wwI oh must go on standing You break that whic...              64  \n",
              "\n",
              "[13540 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b63d636-dfe7-48a6-9abf-9bccb043bbb4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>genres</th>\n",
              "      <th>lang</th>\n",
              "      <th>lyric</th>\n",
              "      <th>number_of_line</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0009fFIM1eYThaPg</td>\n",
              "      <td>pop</td>\n",
              "      <td>en</td>\n",
              "      <td>a sunny day so I got nowhere to hide Not a clo...</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>00P2bHdWFkghmDqz</td>\n",
              "      <td>soul</td>\n",
              "      <td>en</td>\n",
              "      <td>Tell me a tale that always was Sing me a song ...</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>00b6fV3nx5z2b8Ls</td>\n",
              "      <td>pop</td>\n",
              "      <td>en</td>\n",
              "      <td>A buh A buh You went to school to learn girl T...</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>013QDoTqbexEwkHr</td>\n",
              "      <td>pop</td>\n",
              "      <td>en</td>\n",
              "      <td>like a conversation where stops to breathe Is ...</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>01EKNot8qVgZpKM7</td>\n",
              "      <td>rock</td>\n",
              "      <td>en</td>\n",
              "      <td>Say the words I cannot say Say them on another...</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13535</th>\n",
              "      <td>13535</td>\n",
              "      <td>zzT504Z94j1IAuc3</td>\n",
              "      <td>indie rock</td>\n",
              "      <td>en</td>\n",
              "      <td>think what afraid of come in you know been mad...</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13536</th>\n",
              "      <td>13536</td>\n",
              "      <td>zzgS4ZqyswamEWNj</td>\n",
              "      <td>pop</td>\n",
              "      <td>en</td>\n",
              "      <td>Oh yeah yeah Last night I took a walk in the s...</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13537</th>\n",
              "      <td>13537</td>\n",
              "      <td>zzx8CWdM7qkxKQpC</td>\n",
              "      <td>indie rock</td>\n",
              "      <td>en</td>\n",
              "      <td>Innocence it come easy in a sense it never wil...</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13538</th>\n",
              "      <td>13538</td>\n",
              "      <td>zzz0n04uuTUA7fNh</td>\n",
              "      <td>pop</td>\n",
              "      <td>en</td>\n",
              "      <td>Girl you know how I feel I really Since you be...</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13539</th>\n",
              "      <td>13539</td>\n",
              "      <td>zzzj3LYaZtYtbzSr</td>\n",
              "      <td>singer-songwriter</td>\n",
              "      <td>en</td>\n",
              "      <td>wwI oh must go on standing You break that whic...</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13540 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b63d636-dfe7-48a6-9abf-9bccb043bbb4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b63d636-dfe7-48a6-9abf-9bccb043bbb4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b63d636-dfe7-48a6-9abf-9bccb043bbb4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "DIR = '/content/drive/MyDrive/music4all/'\n",
        "df_genre_by_lang = pd.read_csv(DIR + 'df_genre_by_lang_full.csv')\n",
        "df_genre_by_lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PrRidTBHhmYp"
      },
      "outputs": [],
      "source": [
        "def load_data(df_col, y):\n",
        "    texts, labels = [], []\n",
        "    \n",
        "    for line in df_col:\n",
        "        # texts are already tokenized, just split on space\n",
        "        # in a real use-case we would put more effort in preprocessing\n",
        "        texts.append(line.split(' '))\n",
        "    return pd.DataFrame({'texts': texts, 'labels': y})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n5VJWiA6iJu2"
      },
      "outputs": [],
      "source": [
        "data = load_data(df_genre_by_lang[\"lyric\"], df_genre_by_lang[\"genres\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bWI4V7oXiWw6",
        "outputId": "50503078-ce54-4dc6-e642-f03d7390025f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   texts             labels\n",
              "0      [a, sunny, day, so, I, got, nowhere, to, hide,...                pop\n",
              "1      [Tell, me, a, tale, that, always, was, Sing, m...               soul\n",
              "2      [A, buh, A, buh, You, went, to, school, to, le...                pop\n",
              "3      [like, a, conversation, where, stops, to, brea...                pop\n",
              "4      [Say, the, words, I, cannot, say, Say, them, o...               rock\n",
              "...                                                  ...                ...\n",
              "13535  [think, what, afraid, of, come, in, you, know,...         indie rock\n",
              "13536  [Oh, yeah, yeah, Last, night, I, took, a, walk...                pop\n",
              "13537  [Innocence, it, come, easy, in, a, sense, it, ...         indie rock\n",
              "13538  [Girl, you, know, how, I, feel, I, really, Sin...                pop\n",
              "13539  [wwI, oh, must, go, on, standing, You, break, ...  singer-songwriter\n",
              "\n",
              "[13540 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5931d824-85f6-4759-97cb-8c2624843a4a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[a, sunny, day, so, I, got, nowhere, to, hide,...</td>\n",
              "      <td>pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Tell, me, a, tale, that, always, was, Sing, m...</td>\n",
              "      <td>soul</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[A, buh, A, buh, You, went, to, school, to, le...</td>\n",
              "      <td>pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[like, a, conversation, where, stops, to, brea...</td>\n",
              "      <td>pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Say, the, words, I, cannot, say, Say, them, o...</td>\n",
              "      <td>rock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13535</th>\n",
              "      <td>[think, what, afraid, of, come, in, you, know,...</td>\n",
              "      <td>indie rock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13536</th>\n",
              "      <td>[Oh, yeah, yeah, Last, night, I, took, a, walk...</td>\n",
              "      <td>pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13537</th>\n",
              "      <td>[Innocence, it, come, easy, in, a, sense, it, ...</td>\n",
              "      <td>indie rock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13538</th>\n",
              "      <td>[Girl, you, know, how, I, feel, I, really, Sin...</td>\n",
              "      <td>pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13539</th>\n",
              "      <td>[wwI, oh, must, go, on, standing, You, break, ...</td>\n",
              "      <td>singer-songwriter</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13540 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5931d824-85f6-4759-97cb-8c2624843a4a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5931d824-85f6-4759-97cb-8c2624843a4a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5931d824-85f6-4759-97cb-8c2624843a4a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iTROinyfjc6u"
      },
      "outputs": [],
      "source": [
        "data['labels'] = data['labels'].astype('category')\n",
        "label_mapping = data['labels'].cat.categories\n",
        "data['labels'] = data['labels'].cat.codes\n",
        "X = data['texts']\n",
        "y = data['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d32Ub0-kjoOj",
        "outputId": "3be5d300-b3ea-47bb-dcc8-8e7938bf2877"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "type(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GnAEWk_Lza7f"
      },
      "outputs": [],
      "source": [
        "def StratifiedKFold_feature_and_df_glove(df, feature_list, y_name):\n",
        "  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1209)  # 20% for test set \n",
        "  y = df[y_name]\n",
        "  skf.get_n_splits(df[ feature_list ], y)\n",
        "\n",
        "  splits = []\n",
        "\n",
        "  for train_index, test_index in skf.split(df[ feature_list ], y):\n",
        "      print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "      X_train, X_test = df[ feature_list ].loc[train_index], df[ feature_list ].loc[test_index]\n",
        "      y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "      splits.append({'X_train': X_train, 'X_test': X_test, 'y_train':y_train, 'y_test':y_test })\n",
        "  return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1qOv6pF0BcrV"
      },
      "outputs": [],
      "source": [
        "def StratifiedKFold_feature_and_df(X, y):\n",
        "  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1209)  # 20% for test set \n",
        "  #y = df[y_name]\n",
        "  skf.get_n_splits(X, y)#df[ feature_list ]\n",
        "\n",
        "  splits = []\n",
        "\n",
        "  for train_index, test_index in skf.split(X, y):#df[ feature_list ]\n",
        "      print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "      X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "      y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "      splits.append({'X_train': X_train, 'X_test': X_test, 'y_train':y_train, 'y_test':y_test })\n",
        "  return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FGZPLOeBg4R",
        "outputId": "ff335b2e-7459-4daf-b5e3-b44d4b6823b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [    0     1     3 ... 13537 13538 13539] TEST: [    2     4     5 ... 13526 13532 13535]\n",
            "TRAIN: [    0     2     4 ... 13535 13536 13539] TEST: [    1     3     7 ... 13530 13537 13538]\n",
            "TRAIN: [    0     1     2 ... 13537 13538 13539] TEST: [    8    14    22 ... 13521 13531 13536]\n",
            "TRAIN: [    0     1     2 ... 13537 13538 13539] TEST: [   10    12    15 ... 13523 13525 13534]\n",
            "TRAIN: [    1     2     3 ... 13536 13537 13538] TEST: [    0     6    11 ... 13529 13533 13539]\n"
          ]
        }
      ],
      "source": [
        "#feature_list = [\"texts\"] #this is BOW and TF-IDF\n",
        "#splits = StratifiedKFold_feature_and_df( data, feature_list, 'labels')\n",
        "splits = StratifiedKFold_feature_and_df( X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsQYbmVUWPU9",
        "outputId": "74240616-18c0-4a40-fa67-50027d20f987"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDMBs_gWCSLa",
        "outputId": "275ba5b0-5fbe-45be-e50b-9cbf3f18e99d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10832,)\n",
            "(10832,)\n",
            "(2708,)\n",
            "(2708,)\n"
          ]
        }
      ],
      "source": [
        "split0=splits[0]\n",
        "print(split0['X_train'].shape)\n",
        "print(split0['y_train'].shape)\n",
        "print(split0['X_test'].shape)\n",
        "print(split0['y_test'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaMmpM44is_p",
        "outputId": "768fcea7-6fdf-4c27-8759-62939540ddba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [a, sunny, day, so, I, got, nowhere, to, hide,...\n",
              "1        [Tell, me, a, tale, that, always, was, Sing, m...\n",
              "3        [like, a, conversation, where, stops, to, brea...\n",
              "6        [Locked, up, tight, Like, I, would, never, fee...\n",
              "7        [sittin, in, the, crib, dreamin, about, leer, ...\n",
              "                               ...                        \n",
              "13534    [grandma, cookies, nigga, Shout, out, to, fron...\n",
              "13536    [Oh, yeah, yeah, Last, night, I, took, a, walk...\n",
              "13537    [Innocence, it, come, easy, in, a, sense, it, ...\n",
              "13538    [Girl, you, know, how, I, feel, I, really, Sin...\n",
              "13539    [wwI, oh, must, go, on, standing, You, break, ...\n",
              "Name: texts, Length: 10832, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "split0['X_train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qTuHLEe8Aqg",
        "outputId": "35df0482-9085-4673-c802-68d5a287b1c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        4\n",
              "1        9\n",
              "3        4\n",
              "6        4\n",
              "7        6\n",
              "        ..\n",
              "13534    6\n",
              "13536    4\n",
              "13537    3\n",
              "13538    4\n",
              "13539    8\n",
              "Name: labels, Length: 10832, dtype: int8"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "split0['y_train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nONmunMa_h7T"
      },
      "source": [
        "### Use my self programmed balanced accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot3KP7Dl_kdf",
        "outputId": "b39a20db-171b-4ef7-bd3a-e1a9522eb463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "339/339 [==============================] - 225s 645ms/step - loss: 0.5565 - categorical_accuracy: 0.3878 - val_loss: 1.9337 - val_categorical_accuracy: 0.3273\n",
            "Epoch 2/10\n",
            "339/339 [==============================] - 218s 645ms/step - loss: 0.5397 - categorical_accuracy: 0.3626 - val_loss: 2.0651 - val_categorical_accuracy: 0.2764\n",
            "Epoch 3/10\n",
            "339/339 [==============================] - 218s 645ms/step - loss: 0.5375 - categorical_accuracy: 0.3392 - val_loss: 2.0931 - val_categorical_accuracy: 0.2273\n",
            "Epoch 4/10\n",
            "339/339 [==============================] - 218s 645ms/step - loss: 0.5378 - categorical_accuracy: 0.3197 - val_loss: 2.0576 - val_categorical_accuracy: 0.2459\n",
            "Epoch 5/10\n",
            "339/339 [==============================] - 219s 645ms/step - loss: 0.5333 - categorical_accuracy: 0.3077 - val_loss: 2.0292 - val_categorical_accuracy: 0.2816\n",
            "Epoch 6/10\n",
            "339/339 [==============================] - 219s 646ms/step - loss: 0.5373 - categorical_accuracy: 0.2967 - val_loss: 2.1738 - val_categorical_accuracy: 0.1895\n",
            "Epoch 7/10\n",
            "339/339 [==============================] - 219s 646ms/step - loss: 0.5396 - categorical_accuracy: 0.2930 - val_loss: 2.1382 - val_categorical_accuracy: 0.2181\n",
            "Epoch 8/10\n",
            "339/339 [==============================] - 219s 646ms/step - loss: 0.5431 - categorical_accuracy: 0.2710 - val_loss: 2.1512 - val_categorical_accuracy: 0.1965\n",
            "Epoch 9/10\n",
            "339/339 [==============================] - 219s 646ms/step - loss: 0.5450 - categorical_accuracy: 0.2729 - val_loss: 2.1854 - val_categorical_accuracy: 0.1092\n",
            "Epoch 10/10\n",
            "339/339 [==============================] - 219s 646ms/step - loss: 0.5495 - categorical_accuracy: 0.2553 - val_loss: 2.1354 - val_categorical_accuracy: 0.1594\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'balanced_accuracy': 0.09744849916749872},\n",
              " {'balanced_accuracy': 0.10420636932765996},\n",
              " {'balanced_accuracy': 0.10970405736872939},\n",
              " {'balanced_accuracy': 0.10356561320774078},\n",
              " {'balanced_accuracy': 0.10792864804613309},\n",
              " {'balanced_accuracy': 0.09168989857066105},\n",
              " {'balanced_accuracy': 0.09926135499834263},\n",
              " {'balanced_accuracy': 0.10014630125778516},\n",
              " {'balanced_accuracy': 0.09807505457274496},\n",
              " {'balanced_accuracy': 0.09881853716030842}]"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = Metrics()\n",
        "history = model.fit(train, validation_data=val, epochs=10, class_weight=my_weight ,callbacks=[metrics])\n",
        "metrics.get_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzqLBSC6H175"
      },
      "source": [
        "## From here, separate X_train, X_test from KFOldSplit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb0KcNXhbUSN"
      },
      "source": [
        "### Preprocess my lyrics data (Official train and test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "m4lLLyF-bUSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d155fa7-7010-458e-d188-57db8ff96439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 78.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 65.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "!pip3 install transformers\n",
        "SEQ_LEN = 256#512"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split0['X_test']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jUA-ps2gyvt",
        "outputId": "727dcd12-1709-443b-ca54-ed4b473c9535"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2        [A, buh, A, buh, You, went, to, school, to, le...\n",
              "4        [Say, the, words, I, cannot, say, Say, them, o...\n",
              "5        [I, was, alone, I, was, made, of, stone, You, ...\n",
              "9        [Again, the, burden, of, losing, rests, upon, ...\n",
              "20       [only, been, three, weeks, And, a, bag, of, sp...\n",
              "                               ...                        \n",
              "13517    [Like, the, legend, of, the, Phoenix, All, end...\n",
              "13522    [Mr, Telephone, man, something, wrong, with, m...\n",
              "13526    [can, you, imagine, what, it, would, be, like,...\n",
              "13532    [Love, of, my, life, hurt, me, broken, my, hea...\n",
              "13535    [think, what, afraid, of, come, in, you, know,...\n",
              "Name: texts, Length: 2708, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lyrics(X_series):\n",
        "  for i, token_list in X_series.items():\n",
        "    if type(token_list) is list:\n",
        "      X_series.loc[i] = ' '.join(token_list)\n",
        "  return X_series"
      ],
      "metadata": {
        "id": "PqFwOGTIbhiZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "SEQ_LEN=256\n",
        "\n",
        "def get_Xid_Xmask(X_origin):\n",
        "  Xids_train = np.zeros((X_origin.shape[0], SEQ_LEN))\n",
        "  Xmask_train = np.zeros((X_origin.shape[0], SEQ_LEN))\n",
        "\n",
        "  for i, lyric in enumerate(X_origin):\n",
        "    tokens = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\", add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "    Xids_train[i,:], Xmask_train[i,:] = tokens['input_ids'], tokens['attention_mask']\n",
        "  return Xids_train, Xmask_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "8c1765645a5248c19050d3aa66f45bf6",
            "549ea2b3fc784e76ae6318f67aadd130",
            "ecff414594e943c0a608872a58484157",
            "75e741671d414502acb7be89fb232eee",
            "ad9dc752011e4dac88a41adf35e2a311",
            "887c57500fa840fd9ea8764297bf865e",
            "ba32373701ae4e96bbc46b0bde5858aa",
            "c9c6c3933dd444a1b7bfadb2f07a47dd",
            "26d9077e2a974caa8a4fd0097eaa8706",
            "d21f0bf361434ef089b4e07f5990650f",
            "77fe338695c24fe59a1c6b63ccfafb07",
            "922cd02f672d4e6b93fa43dd6496b72d",
            "89476f0f30f04203977b78b763f972e7",
            "8dd855e7caf242ef94285d4b01499c1a",
            "7ee4b508b7054255b7b62a6263ba7355",
            "d2a7e285bb774938b04437a29e79f3db",
            "9eafb86fe78a4aa986e2469f0386dab7",
            "201ca7f3d2cf484485f807fecc065ec7",
            "2711411380834d158bdf6ac911aab51c",
            "f6a527d3443949a0a611238b9ad71f03",
            "8569aee413f845ef8fb9e2f54ca628b8",
            "cbd3fc46977545b8bce278feb2c4f98e",
            "5a68a5eda68b44daa744ee5b1509061d",
            "6ab65500833a4447bc6895d5033da58e",
            "0fa0aba9b120410fa40babcd0c7919eb",
            "5f555ea72bd94802b8d1b7b8bdcd88d3",
            "ff5d27fe27cb4a72ad57ef058b65de4b",
            "78138e4b3889467392a1964ac82d7b46",
            "9d30cd6c250e4f84aa5d8dfa60b81d0d",
            "c0f9139412ea4867921dc3500945fc64",
            "ed704d0e80a8470191b7b9b6c6211171",
            "226d0996b5ef4011b9281c5489dc59ad",
            "941a467e91144143ac20a020c8e66061",
            "fa1704bfa1fa4915a1b3ab1635a44f63",
            "8e19654922b841d79743ac7b8468ff70",
            "4cd010eee588436e999f19c48f111be6",
            "e3c313eade3a4bcda5eed3f17046aa83",
            "9f77f0ba84a84972a80e8d584b9c642b",
            "6403a492ffc3402a906f07a76bb3b8ed",
            "170cbac985ae47bf803ed7f514f7951f",
            "1c316ba86dbf4d128bd954e7cbb163a4",
            "273698cfffaf4a7cbc5764a3b99fca09",
            "5e92a070bb3d45e6a8d21ab377a375df",
            "e4aceffa4e69459bb13bd2249a5f321d"
          ],
          "height": 0
        },
        "id": "hBAL5McoCfw4",
        "outputId": "82d5dd4d-d7dd-48f2-d95c-ae7883b6a166"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c1765645a5248c19050d3aa66f45bf6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "922cd02f672d4e6b93fa43dd6496b72d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a68a5eda68b44daa744ee5b1509061d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/426k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa1704bfa1fa4915a1b3ab1635a44f63"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J7xk5236bUSR"
      },
      "outputs": [],
      "source": [
        "def map_func(input_ids, masks, labels):\n",
        "  return {'input_ids': input_ids, 'attention_mask':masks}, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split0['X_train'] = prepare_lyrics(split0['X_train'] )\n",
        "split0['X_test'] = prepare_lyrics(split0['X_test'] )\n",
        "#tokenizer = AlbertTokenizer.from_pretrained('albert-large-v2')\n",
        "Xids_train, Xmask_train = get_Xid_Xmask(split0['X_train'])\n",
        "Xids_test, Xmask_test = get_Xid_Xmask(split0['X_test'])\n",
        "\n",
        "labels_train = np.zeros((split0['y_train'].shape[0], 10))\n",
        "labels_train[ np.arange(split0['y_train'].shape[0]), split0['y_train'].values] =1\n",
        "labels_test = np.zeros((split0['y_test'].shape[0], 10))\n",
        "labels_test[ np.arange(split0['y_test'].shape[0]), split0['y_test'].values] =1\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((Xids_train, Xmask_train, labels_train))\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((Xids_test, Xmask_test, labels_test))\n",
        "dataset_train = dataset_train.map(map_func)\n",
        "dataset_test = dataset_test.map(map_func)\n",
        "\n",
        "dataset_train = dataset_train.shuffle(42).batch(16)\n",
        "DS_LEN = len(list(dataset_train))\n",
        "SPLIT=0.9\n",
        "train = dataset_train.take(round(DS_LEN*SPLIT))\n",
        "val = dataset_train.skip(round(DS_LEN*SPLIT))\n",
        "test = dataset_test.batch(16)"
      ],
      "metadata": {
        "id": "sW85C_3QCtc5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30617d0-8848-4817-cfe9-f0fddc0cc20c",
        "id": "wZDVrP0QYC_n"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10832"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "counter = Counter(split0['y_train'])\n",
        "SUM=0\n",
        "for item in list(counter.values()) :\n",
        "  SUM+=item\n",
        "#SUM = sum(counter.values())\n",
        "SUM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tutorial\n",
        "#weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "#weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "counter = Counter(split0['y_train'])\n",
        "my_weight2 = {}\n",
        "print(counter)\n",
        "\n",
        "for genre in counter:\n",
        "  #print(genre, counter[genre])\n",
        "  my_weight2[genre] = (1/counter[genre]) * (SUM/10)\n",
        "my_weight2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbDe9DVFAvjP",
        "outputId": "73747cab-a241-400e-b1dd-42bafa91ee18"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({4: 4143, 7: 1159, 9: 1030, 3: 865, 6: 783, 0: 763, 1: 690, 8: 556, 2: 537, 5: 306})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{4: 0.2614530533429882,\n",
              " 9: 1.051650485436893,\n",
              " 6: 1.383397190293742,\n",
              " 2: 2.0171322160148977,\n",
              " 0: 1.4196592398427261,\n",
              " 7: 0.9345987920621226,\n",
              " 1: 1.569855072463768,\n",
              " 5: 3.539869281045752,\n",
              " 8: 1.948201438848921,\n",
              " 3: 1.2522543352601156}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SIZE = len(split0['X_test'])"
      ],
      "metadata": {
        "id": "v2k3GgdtcGz8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9Hb8xkWQ3FOo"
      },
      "outputs": [],
      "source": [
        "def map_func_only_X(val_dictionary, labels):\n",
        "  return {'input_ids': val_dictionary['input_ids'], 'attention_mask':val_dictionary['attention_mask']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3S2rOmEq3FOp"
      },
      "outputs": [],
      "source": [
        "def map_func_only_y(val_dictionary, labels):\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Fold Do parameter tuning for dropout rate"
      ],
      "metadata": {
        "id": "tkFQndSlD2y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del(dataset_train)\n",
        "del(dataset_test)"
      ],
      "metadata": {
        "id": "q2_s_-ikO_T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AlbertTokenizer, TFAlbertModel\n",
        "import numpy as np\n",
        "\n",
        "split0=splits[0]\n",
        "split0['X_train'] = prepare_lyrics(split0['X_train'] )\n",
        "split0['X_test'] = prepare_lyrics(split0['X_test'] )\n",
        "SEQ_LEN=256\n",
        "print(split0['X_train'].shape, split0['X_test'].shape)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "Xids_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xmask_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xids_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "Xmask_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "\n",
        "for i, lyric in enumerate(split0['X_train']):\n",
        "  tokens = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\", add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_train[i,:], Xmask_train[i,:] = tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "for i, lyric in enumerate(split0['X_test']):\n",
        "  tokens_test = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\"\n",
        "    , add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_test[i,:], Xmask_test[i,:] = tokens_test['input_ids'], tokens_test['attention_mask']\n",
        "\n",
        "print(\"Xids_train.shape, Xids_test.shape: \",Xids_train.shape, Xids_test.shape)\n",
        "\n",
        "labels_train = np.zeros((split0['y_train'].shape[0], 10))\n",
        "labels_train[ np.arange(split0['y_train'].shape[0]), split0['y_train'].values] =1\n",
        "labels_test = np.zeros((split0['y_test'].shape[0], 10))\n",
        "labels_test[ np.arange(split0['y_test'].shape[0]), split0['y_test'].values] =1\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((Xids_train, Xmask_train, labels_train))\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((Xids_test, Xmask_test, labels_test))\n",
        "\n",
        "dataset_train = dataset_train.map(map_func)\n",
        "dataset_test = dataset_test.map(map_func)\n",
        "dataset_train = dataset_train.shuffle(42).batch(16)\n",
        "\n",
        "DS_LEN = len(list(dataset_train))\n",
        "SPLIT = 0.9\n",
        "train = dataset_train.take(round(DS_LEN*SPLIT))\n",
        "val = dataset_train.skip(round(DS_LEN*SPLIT))\n",
        "test = dataset_test.batch(16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "z0yzE2a18YHz",
        "outputId": "dc3237e7-1ba8-4d40-a361-5b61028ba173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10832,) (2708,)\n",
            "Xids_train.shape, Xids_test.shape:  (10832, 256) (2708, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have intentionally separated cells for check point purpose, based on dropout rates"
      ],
      "metadata": {
        "id": "LOB9bJ7jdEov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seen_parameter = []#(drop_out_rate, initial_rate, fine_tune_learning_rate)"
      ],
      "metadata": {
        "id": "fTKuKu-KjYwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "McNemar={}\n",
        "balanced_accuracies_transfer_learning=[]\n",
        "balanced_accuracies_fine_tuning = []"
      ],
      "metadata": {
        "id": "bvizO_xTliMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del(model1)"
      ],
      "metadata": {
        "id": "omnIdETZmbg9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "5b05d772-8c22-4b28-8609-b1c23c814de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-36a69b5a0788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "#drop_out_rates = [0.1, 0.2, 0.3,0.4, 0.5]\n",
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "#for drop_out_rate in drop_out_rates:\n",
        "\n",
        "drop_out_rate = 0.1\n",
        "learning_rate_transfer_learnings = [1e-3, 1e-4]\n",
        "learning_rate_fine_tunings = [1e-5, 1e-6]\n",
        "\n",
        "for learning_rate_transfer_learning in learning_rate_transfer_learnings:\n",
        "  for learning_rate_fine_tuning in learning_rate_fine_tunings:\n",
        "    print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        "    , learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "    #step1\n",
        "    bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "    embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "    model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "    model1.layers[2].trainable = False\n",
        "    print(model1.summary())\n",
        "\n",
        "    #step2\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "    #model2.summary() #Check trainable params increased.\n",
        "\n",
        "    #step3: transfer learning\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "    #step4: predict\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "    #step5: fine tune\n",
        "    print(\"Fine tuning---------------\")\n",
        "    model1.layers[2].trainable = True\n",
        "\n",
        "    # It's important to recompile your model after you make any changes\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "    print(\"----------------------------------------\")\n",
        "    del(model1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7a5dda3e451d4d858613f4d9c9b86d8e",
            "c80020e048704b6baa9b4ca1c13fc511",
            "112a6e4e0cba4acfaf914daf6fab9326",
            "8bcd45b9e75d46c8b75503490e1b7b5d",
            "cc733a6be3d341429436714ea26e949d",
            "d1a0792ee3d142aeb84f23125cd93790",
            "580987b69f5c40738440211f9ae9cd84",
            "6f95af0bf95a4ee8b373395a621a11e2",
            "e526f46d89b642fb8a2dd9a111f6e304",
            "78dcfc55a0dd4cdb8388c5afdb7f364a",
            "0692e45fe98e433d92a78dd78c3246e5"
          ]
        },
        "id": "oFsobW4gcvIf",
        "outputId": "b67d3730-cd07-4b50-8cf2-09ec6704517b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tf_model.h5:   0%|          | 0.00/502M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a5dda3e451d4d858613f4d9c9b86d8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          98432       ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 128)         512         ['dense[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 128)          0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 32)           4128        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32)          128         ['dense_1[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 32)           0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 97s 136ms/step - loss: 2.2557 - categorical_accuracy: 0.2153 - val_loss: 2.2306 - val_categorical_accuracy: 0.1903\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.9856 - categorical_accuracy: 0.2764 - val_loss: 2.2102 - val_categorical_accuracy: 0.1939\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.8974 - categorical_accuracy: 0.2932 - val_loss: 2.1987 - val_categorical_accuracy: 0.1985\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.8421 - categorical_accuracy: 0.3012 - val_loss: 2.1562 - val_categorical_accuracy: 0.1976\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.8065 - categorical_accuracy: 0.3131 - val_loss: 2.0991 - val_categorical_accuracy: 0.2123\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.7636 - categorical_accuracy: 0.3188 - val_loss: 2.1152 - val_categorical_accuracy: 0.2224\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.15      0.28      0.19       190\n",
            "         1.0       0.14      0.34      0.20       173\n",
            "         2.0       0.07      0.16      0.10       135\n",
            "         3.0       0.09      0.12      0.10       216\n",
            "         4.0       0.77      0.12      0.21      1036\n",
            "         5.0       0.11      0.42      0.17        76\n",
            "         6.0       0.73      0.66      0.69       195\n",
            "         7.0       0.11      0.03      0.04       290\n",
            "         8.0       0.09      0.24      0.13       139\n",
            "         9.0       0.27      0.25      0.26       258\n",
            "\n",
            "    accuracy                           0.20      2708\n",
            "   macro avg       0.25      0.26      0.21      2708\n",
            "weighted avg       0.42      0.20      0.21      2708\n",
            "\n",
            "0.2616995841037313\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 204s 309ms/step - loss: 1.7137 - categorical_accuracy: 0.3321 - val_loss: 2.0782 - val_categorical_accuracy: 0.2243\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.5607 - categorical_accuracy: 0.3592 - val_loss: 2.0861 - val_categorical_accuracy: 0.2500\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 303ms/step - loss: 1.4482 - categorical_accuracy: 0.4028 - val_loss: 2.1160 - val_categorical_accuracy: 0.2233\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.3561 - categorical_accuracy: 0.4269 - val_loss: 1.9283 - val_categorical_accuracy: 0.2767\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.2561 - categorical_accuracy: 0.4597 - val_loss: 1.8924 - val_categorical_accuracy: 0.2923\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 303ms/step - loss: 1.1755 - categorical_accuracy: 0.4896 - val_loss: 1.9968 - val_categorical_accuracy: 0.2583\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.41      0.21       190\n",
            "         1.0       0.21      0.28      0.24       173\n",
            "         2.0       0.12      0.19      0.15       135\n",
            "         3.0       0.15      0.18      0.17       216\n",
            "         4.0       0.80      0.17      0.28      1036\n",
            "         5.0       0.11      0.47      0.18        76\n",
            "         6.0       0.80      0.74      0.77       195\n",
            "         7.0       0.14      0.06      0.08       290\n",
            "         8.0       0.13      0.27      0.17       139\n",
            "         9.0       0.31      0.38      0.34       258\n",
            "\n",
            "    accuracy                           0.26      2708\n",
            "   macro avg       0.29      0.31      0.26      2708\n",
            "weighted avg       0.46      0.26      0.27      2708\n",
            "\n",
            "0.3138137046741628\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 768)         3072        ['global_max_pooling1d_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 128)          98432       ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 128)         512         ['dense_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_76 (Dropout)           (None, 128)          0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 32)           4128        ['dropout_76[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32)          128         ['dense_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_77 (Dropout)           (None, 32)           0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_77[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 136ms/step - loss: 2.2402 - categorical_accuracy: 0.2010 - val_loss: 2.2454 - val_categorical_accuracy: 0.1829\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.9863 - categorical_accuracy: 0.2602 - val_loss: 2.2084 - val_categorical_accuracy: 0.1875\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 1.9130 - categorical_accuracy: 0.2826 - val_loss: 2.2076 - val_categorical_accuracy: 0.1783\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.8668 - categorical_accuracy: 0.3000 - val_loss: 2.1224 - val_categorical_accuracy: 0.2233\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.8168 - categorical_accuracy: 0.3064 - val_loss: 2.1150 - val_categorical_accuracy: 0.2298\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.7836 - categorical_accuracy: 0.3136 - val_loss: 2.1505 - val_categorical_accuracy: 0.2132\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.13      0.32      0.18       190\n",
            "         1.0       0.19      0.36      0.25       173\n",
            "         2.0       0.07      0.12      0.09       135\n",
            "         3.0       0.12      0.20      0.15       216\n",
            "         4.0       0.81      0.12      0.21      1036\n",
            "         5.0       0.10      0.41      0.16        76\n",
            "         6.0       0.77      0.64      0.70       195\n",
            "         7.0       0.06      0.01      0.02       290\n",
            "         8.0       0.07      0.18      0.10       139\n",
            "         9.0       0.28      0.30      0.29       258\n",
            "\n",
            "    accuracy                           0.21      2708\n",
            "   macro avg       0.26      0.27      0.22      2708\n",
            "weighted avg       0.44      0.21      0.22      2708\n",
            "\n",
            "0.26630449510040444\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 205s 308ms/step - loss: 1.7148 - categorical_accuracy: 0.3334 - val_loss: 2.1114 - val_categorical_accuracy: 0.2169\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.6736 - categorical_accuracy: 0.3454 - val_loss: 2.0948 - val_categorical_accuracy: 0.2233\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.6455 - categorical_accuracy: 0.3502 - val_loss: 2.0955 - val_categorical_accuracy: 0.2169\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.6240 - categorical_accuracy: 0.3592 - val_loss: 2.0770 - val_categorical_accuracy: 0.2279\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.6060 - categorical_accuracy: 0.3573 - val_loss: 2.0802 - val_categorical_accuracy: 0.2224\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.5956 - categorical_accuracy: 0.3590 - val_loss: 2.0996 - val_categorical_accuracy: 0.2178\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.15      0.35      0.21       190\n",
            "         1.0       0.20      0.37      0.26       173\n",
            "         2.0       0.08      0.13      0.10       135\n",
            "         3.0       0.12      0.20      0.15       216\n",
            "         4.0       0.81      0.12      0.21      1036\n",
            "         5.0       0.12      0.55      0.20        76\n",
            "         6.0       0.78      0.69      0.73       195\n",
            "         7.0       0.11      0.02      0.03       290\n",
            "         8.0       0.08      0.16      0.10       139\n",
            "         9.0       0.29      0.36      0.32       258\n",
            "\n",
            "    accuracy                           0.23      2708\n",
            "   macro avg       0.27      0.30      0.23      2708\n",
            "weighted avg       0.45      0.23      0.23      2708\n",
            "\n",
            "0.2957575900115369\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Global  (None, 768)         0           ['tf_bert_model_2[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 768)         3072        ['global_max_pooling1d_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 128)          98432       ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 128)         512         ['dense_4[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_115 (Dropout)          (None, 128)          0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 32)           4128        ['dropout_115[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 32)          128         ['dense_5[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_116 (Dropout)          (None, 32)           0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_116[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 136ms/step - loss: 2.5962 - categorical_accuracy: 0.1606 - val_loss: 2.5762 - val_categorical_accuracy: 0.1535\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.3084 - categorical_accuracy: 0.2083 - val_loss: 2.4455 - val_categorical_accuracy: 0.1636\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.2182 - categorical_accuracy: 0.2266 - val_loss: 2.3779 - val_categorical_accuracy: 0.1691\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.1196 - categorical_accuracy: 0.2389 - val_loss: 2.3384 - val_categorical_accuracy: 0.1875\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.0651 - categorical_accuracy: 0.2535 - val_loss: 2.3194 - val_categorical_accuracy: 0.1866\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.0356 - categorical_accuracy: 0.2491 - val_loss: 2.2964 - val_categorical_accuracy: 0.1949\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.13      0.14       190\n",
            "         1.0       0.11      0.25      0.16       173\n",
            "         2.0       0.05      0.10      0.07       135\n",
            "         3.0       0.12      0.16      0.14       216\n",
            "         4.0       0.74      0.10      0.18      1036\n",
            "         5.0       0.08      0.54      0.14        76\n",
            "         6.0       0.69      0.67      0.68       195\n",
            "         7.0       0.12      0.09      0.11       290\n",
            "         8.0       0.06      0.17      0.09       139\n",
            "         9.0       0.21      0.16      0.18       258\n",
            "\n",
            "    accuracy                           0.18      2708\n",
            "   macro avg       0.23      0.24      0.19      2708\n",
            "weighted avg       0.40      0.18      0.19      2708\n",
            "\n",
            "0.23677781543400472\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 205s 308ms/step - loss: 1.9895 - categorical_accuracy: 0.2717 - val_loss: 2.2086 - val_categorical_accuracy: 0.2059\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.8778 - categorical_accuracy: 0.2936 - val_loss: 2.1508 - val_categorical_accuracy: 0.2325\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7795 - categorical_accuracy: 0.3141 - val_loss: 2.0937 - val_categorical_accuracy: 0.2546\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.6954 - categorical_accuracy: 0.3346 - val_loss: 2.0743 - val_categorical_accuracy: 0.2555\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.5928 - categorical_accuracy: 0.3670 - val_loss: 2.1951 - val_categorical_accuracy: 0.2472\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.5278 - categorical_accuracy: 0.3840 - val_loss: 2.1691 - val_categorical_accuracy: 0.2289\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.16      0.19      0.17       190\n",
            "         1.0       0.18      0.51      0.27       173\n",
            "         2.0       0.07      0.13      0.09       135\n",
            "         3.0       0.14      0.15      0.15       216\n",
            "         4.0       0.77      0.11      0.19      1036\n",
            "         5.0       0.10      0.62      0.17        76\n",
            "         6.0       0.70      0.77      0.73       195\n",
            "         7.0       0.10      0.04      0.06       290\n",
            "         8.0       0.11      0.18      0.14       139\n",
            "         9.0       0.30      0.39      0.34       258\n",
            "\n",
            "    accuracy                           0.23      2708\n",
            "   macro avg       0.26      0.31      0.23      2708\n",
            "weighted avg       0.43      0.23      0.22      2708\n",
            "\n",
            "0.309020211000965\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_3 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Global  (None, 768)         0           ['tf_bert_model_3[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 768)         3072        ['global_max_pooling1d_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          98432       ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 128)         512         ['dense_6[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_154 (Dropout)          (None, 128)          0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 32)           4128        ['dropout_154[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 32)          128         ['dense_7[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_155 (Dropout)          (None, 32)           0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_155[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 137ms/step - loss: 2.6796 - categorical_accuracy: 0.1483 - val_loss: 2.5121 - val_categorical_accuracy: 0.1746\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.3812 - categorical_accuracy: 0.1894 - val_loss: 2.3946 - val_categorical_accuracy: 0.1875\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.2442 - categorical_accuracy: 0.2065 - val_loss: 2.3346 - val_categorical_accuracy: 0.1912\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.1646 - categorical_accuracy: 0.2316 - val_loss: 2.3056 - val_categorical_accuracy: 0.1930\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.1163 - categorical_accuracy: 0.2400 - val_loss: 2.2980 - val_categorical_accuracy: 0.1847\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.0743 - categorical_accuracy: 0.2496 - val_loss: 2.2837 - val_categorical_accuracy: 0.1958\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.22      0.17       190\n",
            "         1.0       0.14      0.30      0.19       173\n",
            "         2.0       0.07      0.13      0.09       135\n",
            "         3.0       0.10      0.07      0.08       216\n",
            "         4.0       0.67      0.13      0.21      1036\n",
            "         5.0       0.07      0.58      0.12        76\n",
            "         6.0       0.64      0.66      0.65       195\n",
            "         7.0       0.07      0.03      0.04       290\n",
            "         8.0       0.05      0.09      0.07       139\n",
            "         9.0       0.21      0.21      0.21       258\n",
            "\n",
            "    accuracy                           0.19      2708\n",
            "   macro avg       0.22      0.24      0.18      2708\n",
            "weighted avg       0.37      0.19      0.19      2708\n",
            "\n",
            "0.2405767021866046\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 206s 309ms/step - loss: 2.0328 - categorical_accuracy: 0.2466 - val_loss: 2.2216 - val_categorical_accuracy: 0.2224\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.0144 - categorical_accuracy: 0.2596 - val_loss: 2.2084 - val_categorical_accuracy: 0.2169\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.9834 - categorical_accuracy: 0.2622 - val_loss: 2.2119 - val_categorical_accuracy: 0.2215\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.9636 - categorical_accuracy: 0.2674 - val_loss: 2.2154 - val_categorical_accuracy: 0.2206\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.23      0.18       190\n",
            "         1.0       0.16      0.32      0.21       173\n",
            "         2.0       0.07      0.10      0.08       135\n",
            "         3.0       0.12      0.08      0.10       216\n",
            "         4.0       0.71      0.17      0.27      1036\n",
            "         5.0       0.08      0.62      0.14        76\n",
            "         6.0       0.61      0.72      0.66       195\n",
            "         7.0       0.09      0.03      0.05       290\n",
            "         8.0       0.08      0.14      0.10       139\n",
            "         9.0       0.21      0.24      0.23       258\n",
            "\n",
            "    accuracy                           0.22      2708\n",
            "   macro avg       0.23      0.27      0.20      2708\n",
            "weighted avg       0.38      0.22      0.23      2708\n",
            "\n",
            "0.2651974924965628\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 H 44 min"
      ],
      "metadata": {
        "id": "XnE6VpL4mv0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "#drop_out_rates = [0.1, 0.2, 0.3,0.4, 0.5]\n",
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "#for drop_out_rate in drop_out_rates:\n",
        "\n",
        "drop_out_rate = 0.2\n",
        "learning_rate_transfer_learnings = [1e-3, 1e-4]\n",
        "learning_rate_fine_tunings = [1e-5, 1e-6]\n",
        "\n",
        "for learning_rate_transfer_learning in learning_rate_transfer_learnings:\n",
        "  for learning_rate_fine_tuning in learning_rate_fine_tunings:\n",
        "    print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        "    , learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "    #step1\n",
        "    bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "    embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "    model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "    model1.layers[2].trainable = False\n",
        "    print(model1.summary())\n",
        "\n",
        "    #step2\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "    #model2.summary() #Check trainable params increased.\n",
        "\n",
        "    #step3: transfer learning\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "    #step4: predict\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "    #step5: fine tune\n",
        "    print(\"Fine tuning---------------\")\n",
        "    model1.layers[2].trainable = True\n",
        "\n",
        "    # It's important to recompile your model after you make any changes\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "    print(\"----------------------------------------\")\n",
        "    del(model1)\n"
      ],
      "metadata": {
        "id": "iOMVmR3_D5W1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d663dc-00f3-4f83-8029-018dac21900e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_4 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Global  (None, 768)         0           ['tf_bert_model_4[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 768)         3072        ['global_max_pooling1d_4[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 128)          98432       ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 128)         512         ['dense_8[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_193 (Dropout)          (None, 128)          0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 32)           4128        ['dropout_193[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 32)          128         ['dense_9[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_194 (Dropout)          (None, 32)           0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_194[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 138ms/step - loss: 2.3373 - categorical_accuracy: 0.1974 - val_loss: 2.2304 - val_categorical_accuracy: 0.1783\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.0442 - categorical_accuracy: 0.2628 - val_loss: 2.2054 - val_categorical_accuracy: 0.1774\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.9610 - categorical_accuracy: 0.2772 - val_loss: 2.1467 - val_categorical_accuracy: 0.2096\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.9034 - categorical_accuracy: 0.2792 - val_loss: 2.1173 - val_categorical_accuracy: 0.2160\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.8654 - categorical_accuracy: 0.2992 - val_loss: 2.1383 - val_categorical_accuracy: 0.2031\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.8361 - categorical_accuracy: 0.3023 - val_loss: 2.0974 - val_categorical_accuracy: 0.2188\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.13      0.26      0.18       190\n",
            "         1.0       0.19      0.35      0.25       173\n",
            "         2.0       0.08      0.17      0.11       135\n",
            "         3.0       0.14      0.27      0.18       216\n",
            "         4.0       0.81      0.16      0.27      1036\n",
            "         5.0       0.11      0.45      0.18        76\n",
            "         6.0       0.77      0.70      0.73       195\n",
            "         7.0       0.16      0.03      0.05       290\n",
            "         8.0       0.13      0.25      0.17       139\n",
            "         9.0       0.27      0.33      0.30       258\n",
            "\n",
            "    accuracy                           0.24      2708\n",
            "   macro avg       0.28      0.30      0.24      2708\n",
            "weighted avg       0.45      0.24      0.25      2708\n",
            "\n",
            "0.2971865498001479\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "609/609 [==============================] - 205s 309ms/step - loss: 1.7462 - categorical_accuracy: 0.3286 - val_loss: 2.0768 - val_categorical_accuracy: 0.2160\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.6618 - categorical_accuracy: 0.3458 - val_loss: 1.9819 - val_categorical_accuracy: 0.2454\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.5568 - categorical_accuracy: 0.3698 - val_loss: 2.0413 - val_categorical_accuracy: 0.2426\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.4737 - categorical_accuracy: 0.3970 - val_loss: 2.0970 - val_categorical_accuracy: 0.2270\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 305ms/step - loss: 1.3981 - categorical_accuracy: 0.4135 - val_loss: 2.0257 - val_categorical_accuracy: 0.2381\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.15      0.36      0.21       190\n",
            "         1.0       0.20      0.34      0.25       173\n",
            "         2.0       0.08      0.19      0.11       135\n",
            "         3.0       0.11      0.13      0.12       216\n",
            "         4.0       0.80      0.11      0.19      1036\n",
            "         5.0       0.12      0.54      0.20        76\n",
            "         6.0       0.85      0.69      0.76       195\n",
            "         7.0       0.18      0.03      0.05       290\n",
            "         8.0       0.13      0.27      0.17       139\n",
            "         9.0       0.29      0.45      0.36       258\n",
            "\n",
            "    accuracy                           0.23      2708\n",
            "   macro avg       0.29      0.31      0.24      2708\n",
            "weighted avg       0.46      0.23      0.23      2708\n",
            "\n",
            "0.3115620755189979\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_5 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Global  (None, 768)         0           ['tf_bert_model_5[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 768)         3072        ['global_max_pooling1d_5[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 128)          98432       ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 128)         512         ['dense_10[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_232 (Dropout)          (None, 128)          0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 32)           4128        ['dropout_232[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 32)          128         ['dense_11[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_233 (Dropout)          (None, 32)           0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_233[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 97s 139ms/step - loss: 2.3156 - categorical_accuracy: 0.1964 - val_loss: 2.2257 - val_categorical_accuracy: 0.1976\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.0408 - categorical_accuracy: 0.2517 - val_loss: 2.2217 - val_categorical_accuracy: 0.1976\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 1.9585 - categorical_accuracy: 0.2750 - val_loss: 2.1188 - val_categorical_accuracy: 0.2077\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 1.9019 - categorical_accuracy: 0.2856 - val_loss: 2.1716 - val_categorical_accuracy: 0.2050\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.8508 - categorical_accuracy: 0.2927 - val_loss: 2.1347 - val_categorical_accuracy: 0.1857\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.8281 - categorical_accuracy: 0.3101 - val_loss: 2.1349 - val_categorical_accuracy: 0.2040\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.10      0.24      0.15       190\n",
            "         1.0       0.13      0.21      0.16       173\n",
            "         2.0       0.06      0.10      0.07       135\n",
            "         3.0       0.11      0.15      0.13       216\n",
            "         4.0       0.77      0.13      0.23      1036\n",
            "         5.0       0.11      0.55      0.18        76\n",
            "         6.0       0.78      0.67      0.72       195\n",
            "         7.0       0.12      0.01      0.02       290\n",
            "         8.0       0.09      0.30      0.14       139\n",
            "         9.0       0.29      0.29      0.29       258\n",
            "\n",
            "    accuracy                           0.21      2708\n",
            "   macro avg       0.26      0.27      0.21      2708\n",
            "weighted avg       0.43      0.21      0.22      2708\n",
            "\n",
            "0.2654221816348495\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "609/609 [==============================] - 205s 309ms/step - loss: 1.7954 - categorical_accuracy: 0.3219 - val_loss: 2.1125 - val_categorical_accuracy: 0.2096\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.7466 - categorical_accuracy: 0.3359 - val_loss: 2.0950 - val_categorical_accuracy: 0.2178\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.7378 - categorical_accuracy: 0.3284 - val_loss: 2.0792 - val_categorical_accuracy: 0.2197\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 305ms/step - loss: 1.7010 - categorical_accuracy: 0.3402 - val_loss: 2.0949 - val_categorical_accuracy: 0.2169\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.6919 - categorical_accuracy: 0.3474 - val_loss: 2.0743 - val_categorical_accuracy: 0.2298\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 305ms/step - loss: 1.6716 - categorical_accuracy: 0.3416 - val_loss: 2.1124 - val_categorical_accuracy: 0.2059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.12      0.28      0.16       190\n",
            "         1.0       0.18      0.31      0.23       173\n",
            "         2.0       0.05      0.07      0.06       135\n",
            "         3.0       0.12      0.18      0.15       216\n",
            "         4.0       0.78      0.10      0.18      1036\n",
            "         5.0       0.10      0.61      0.18        76\n",
            "         6.0       0.77      0.72      0.74       195\n",
            "         7.0       0.20      0.01      0.03       290\n",
            "         8.0       0.11      0.27      0.15       139\n",
            "         9.0       0.27      0.32      0.30       258\n",
            "\n",
            "    accuracy                           0.21      2708\n",
            "   macro avg       0.27      0.29      0.22      2708\n",
            "weighted avg       0.44      0.21      0.21      2708\n",
            "\n",
            "0.2872598061998872\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_6 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_6 (Global  (None, 768)         0           ['tf_bert_model_6[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 768)         3072        ['global_max_pooling1d_6[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 128)          98432       ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 128)         512         ['dense_12[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_271 (Dropout)          (None, 128)          0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 32)           4128        ['dropout_271[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 32)          128         ['dense_13[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_272 (Dropout)          (None, 32)           0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_272[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 97s 139ms/step - loss: 2.8612 - categorical_accuracy: 0.1195 - val_loss: 2.5250 - val_categorical_accuracy: 0.1278\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.5068 - categorical_accuracy: 0.1693 - val_loss: 2.4303 - val_categorical_accuracy: 0.1388\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.3766 - categorical_accuracy: 0.1899 - val_loss: 2.3434 - val_categorical_accuracy: 0.1452\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.3051 - categorical_accuracy: 0.1991 - val_loss: 2.3130 - val_categorical_accuracy: 0.1461\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.2154 - categorical_accuracy: 0.2097 - val_loss: 2.3032 - val_categorical_accuracy: 0.1535\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.1674 - categorical_accuracy: 0.2284 - val_loss: 2.2750 - val_categorical_accuracy: 0.1636\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.15      0.24      0.18       190\n",
            "         1.0       0.15      0.36      0.21       173\n",
            "         2.0       0.04      0.07      0.05       135\n",
            "         3.0       0.06      0.10      0.07       216\n",
            "         4.0       0.71      0.07      0.13      1036\n",
            "         5.0       0.06      0.43      0.10        76\n",
            "         6.0       0.66      0.67      0.67       195\n",
            "         7.0       0.07      0.04      0.05       290\n",
            "         8.0       0.09      0.09      0.09       139\n",
            "         9.0       0.20      0.14      0.17       258\n",
            "\n",
            "    accuracy                           0.16      2708\n",
            "   macro avg       0.22      0.22      0.17      2708\n",
            "weighted avg       0.38      0.16      0.16      2708\n",
            "\n",
            "0.22296853127733324\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 204s 310ms/step - loss: 2.1077 - categorical_accuracy: 0.2273 - val_loss: 2.2259 - val_categorical_accuracy: 0.1866\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.0115 - categorical_accuracy: 0.2469 - val_loss: 2.2951 - val_categorical_accuracy: 0.1682\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.9194 - categorical_accuracy: 0.2677 - val_loss: 2.1376 - val_categorical_accuracy: 0.2022\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.8540 - categorical_accuracy: 0.2891 - val_loss: 2.2112 - val_categorical_accuracy: 0.1774\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.7846 - categorical_accuracy: 0.3028 - val_loss: 2.1609 - val_categorical_accuracy: 0.1949\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.7195 - categorical_accuracy: 0.3129 - val_loss: 2.2028 - val_categorical_accuracy: 0.2068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.17      0.35      0.23       190\n",
            "         1.0       0.18      0.43      0.25       173\n",
            "         2.0       0.08      0.13      0.10       135\n",
            "         3.0       0.08      0.12      0.09       216\n",
            "         4.0       0.73      0.08      0.14      1036\n",
            "         5.0       0.10      0.59      0.17        76\n",
            "         6.0       0.75      0.76      0.75       195\n",
            "         7.0       0.10      0.05      0.06       290\n",
            "         8.0       0.13      0.14      0.13       139\n",
            "         9.0       0.27      0.25      0.26       258\n",
            "\n",
            "    accuracy                           0.21      2708\n",
            "   macro avg       0.26      0.29      0.22      2708\n",
            "weighted avg       0.41      0.21      0.20      2708\n",
            "\n",
            "0.2904218211763925\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_7 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_7 (Global  (None, 768)         0           ['tf_bert_model_7[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 768)         3072        ['global_max_pooling1d_7[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 128)          98432       ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 128)         512         ['dense_14[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_310 (Dropout)          (None, 128)          0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 32)           4128        ['dropout_310[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 32)          128         ['dense_15[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_311 (Dropout)          (None, 32)           0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_311[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 97s 138ms/step - loss: 2.8005 - categorical_accuracy: 0.1339 - val_loss: 2.4634 - val_categorical_accuracy: 0.1700\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.4986 - categorical_accuracy: 0.1806 - val_loss: 2.3876 - val_categorical_accuracy: 0.1903\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.3440 - categorical_accuracy: 0.2052 - val_loss: 2.2971 - val_categorical_accuracy: 0.2040\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.2853 - categorical_accuracy: 0.2234 - val_loss: 2.2796 - val_categorical_accuracy: 0.2096\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.1852 - categorical_accuracy: 0.2381 - val_loss: 2.2437 - val_categorical_accuracy: 0.2114\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.1409 - categorical_accuracy: 0.2457 - val_loss: 2.2394 - val_categorical_accuracy: 0.2142\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.11      0.12      0.11       190\n",
            "         1.0       0.14      0.26      0.18       173\n",
            "         2.0       0.06      0.13      0.08       135\n",
            "         3.0       0.10      0.21      0.14       216\n",
            "         4.0       0.69      0.16      0.27      1036\n",
            "         5.0       0.09      0.42      0.15        76\n",
            "         6.0       0.60      0.68      0.64       195\n",
            "         7.0       0.14      0.10      0.12       290\n",
            "         8.0       0.12      0.17      0.14       139\n",
            "         9.0       0.20      0.20      0.20       258\n",
            "\n",
            "    accuracy                           0.21      2708\n",
            "   macro avg       0.23      0.24      0.20      2708\n",
            "weighted avg       0.38      0.21      0.23      2708\n",
            "\n",
            "0.24498932464371395\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 205s 310ms/step - loss: 2.1046 - categorical_accuracy: 0.2474 - val_loss: 2.2138 - val_categorical_accuracy: 0.2270\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.0697 - categorical_accuracy: 0.2546 - val_loss: 2.1917 - val_categorical_accuracy: 0.2261\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.0566 - categorical_accuracy: 0.2596 - val_loss: 2.1867 - val_categorical_accuracy: 0.2362\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 186s 306ms/step - loss: 2.0326 - categorical_accuracy: 0.2617 - val_loss: 2.1664 - val_categorical_accuracy: 0.2390\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.0109 - categorical_accuracy: 0.2646 - val_loss: 2.1360 - val_categorical_accuracy: 0.2472\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.0128 - categorical_accuracy: 0.2675 - val_loss: 2.1535 - val_categorical_accuracy: 0.2316\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.15      0.12      0.14       190\n",
            "         1.0       0.17      0.35      0.22       173\n",
            "         2.0       0.07      0.13      0.09       135\n",
            "         3.0       0.12      0.21      0.15       216\n",
            "         4.0       0.74      0.19      0.31      1036\n",
            "         5.0       0.13      0.54      0.20        76\n",
            "         6.0       0.60      0.76      0.67       195\n",
            "         7.0       0.13      0.07      0.09       290\n",
            "         8.0       0.15      0.18      0.16       139\n",
            "         9.0       0.22      0.36      0.27       258\n",
            "\n",
            "    accuracy                           0.25      2708\n",
            "   macro avg       0.25      0.29      0.23      2708\n",
            "weighted avg       0.41      0.25      0.26      2708\n",
            "\n",
            "0.2907618196948229\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_accuracies_fine_tuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aantZPLaXT0b",
        "outputId": "3f7385b6-f1e7-40e0-d093-a4dcdc533594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3115620755189979,\n",
              " 0.2872598061998872,\n",
              " 0.2904218211763925,\n",
              " 0.2907618196948229]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "#drop_out_rates = [0.1, 0.2, 0.3,0.4, 0.5]\n",
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "#for drop_out_rate in drop_out_rates:\n",
        "\n",
        "drop_out_rate = 0.3\n",
        "learning_rate_transfer_learnings = [1e-3, 1e-4]\n",
        "learning_rate_fine_tunings = [1e-5, 1e-6]\n",
        "\n",
        "for learning_rate_transfer_learning in learning_rate_transfer_learnings:\n",
        "  for learning_rate_fine_tuning in learning_rate_fine_tunings:\n",
        "    print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        "    , learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "    #step1\n",
        "    bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "    embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "    model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "    model1.layers[2].trainable = False\n",
        "    print(model1.summary())\n",
        "\n",
        "    #step2\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "    #model2.summary() #Check trainable params increased.\n",
        "\n",
        "    #step3: transfer learning\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "    #step4: predict\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "    #step5: fine tune\n",
        "    print(\"Fine tuning---------------\")\n",
        "    model1.layers[2].trainable = True\n",
        "\n",
        "    # It's important to recompile your model after you make any changes\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "    print(\"----------------------------------------\")\n",
        "    del(model1)\n"
      ],
      "metadata": {
        "id": "kXi-tHm1z2jA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5268ee39-dd6b-49c7-ae34-07f4057d741e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_8 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_8 (Global  (None, 768)         0           ['tf_bert_model_8[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 768)         3072        ['global_max_pooling1d_8[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 128)          98432       ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 128)         512         ['dense_16[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_349 (Dropout)          (None, 128)          0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 32)           4128        ['dropout_349[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 32)          128         ['dense_17[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_350 (Dropout)          (None, 32)           0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_350[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 98s 138ms/step - loss: 2.4121 - categorical_accuracy: 0.2017 - val_loss: 2.2674 - val_categorical_accuracy: 0.1985\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.0881 - categorical_accuracy: 0.2397 - val_loss: 2.1933 - val_categorical_accuracy: 0.2022\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.9761 - categorical_accuracy: 0.2658 - val_loss: 2.1628 - val_categorical_accuracy: 0.2114\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.9366 - categorical_accuracy: 0.2794 - val_loss: 2.0763 - val_categorical_accuracy: 0.2417\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.9054 - categorical_accuracy: 0.2848 - val_loss: 2.0856 - val_categorical_accuracy: 0.2307\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.8884 - categorical_accuracy: 0.2879 - val_loss: 2.0979 - val_categorical_accuracy: 0.2279\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.12      0.26      0.16       190\n",
            "         1.0       0.16      0.36      0.22       173\n",
            "         2.0       0.07      0.13      0.09       135\n",
            "         3.0       0.13      0.28      0.18       216\n",
            "         4.0       0.78      0.15      0.25      1036\n",
            "         5.0       0.11      0.51      0.19        76\n",
            "         6.0       0.74      0.68      0.71       195\n",
            "         7.0       0.00      0.00      0.00       290\n",
            "         8.0       0.06      0.05      0.05       139\n",
            "         9.0       0.27      0.35      0.31       258\n",
            "\n",
            "    accuracy                           0.23      2708\n",
            "   macro avg       0.24      0.28      0.22      2708\n",
            "weighted avg       0.42      0.23      0.23      2708\n",
            "\n",
            "0.2778733948852413\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 208s 310ms/step - loss: 1.8343 - categorical_accuracy: 0.2994 - val_loss: 2.2037 - val_categorical_accuracy: 0.1994\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.7307 - categorical_accuracy: 0.3169 - val_loss: 1.9064 - val_categorical_accuracy: 0.2619\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.6662 - categorical_accuracy: 0.3398 - val_loss: 2.0382 - val_categorical_accuracy: 0.2279\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.5915 - categorical_accuracy: 0.3558 - val_loss: 2.0383 - val_categorical_accuracy: 0.2215\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.5239 - categorical_accuracy: 0.3779 - val_loss: 1.9862 - val_categorical_accuracy: 0.2362\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.18      0.34      0.23       190\n",
            "         1.0       0.20      0.50      0.29       173\n",
            "         2.0       0.10      0.18      0.13       135\n",
            "         3.0       0.16      0.25      0.19       216\n",
            "         4.0       0.80      0.07      0.12      1036\n",
            "         5.0       0.16      0.58      0.25        76\n",
            "         6.0       0.70      0.80      0.75       195\n",
            "         7.0       0.29      0.01      0.01       290\n",
            "         8.0       0.10      0.08      0.09       139\n",
            "         9.0       0.21      0.53      0.30       258\n",
            "\n",
            "    accuracy                           0.24      2708\n",
            "   macro avg       0.29      0.33      0.24      2708\n",
            "weighted avg       0.46      0.24      0.20      2708\n",
            "\n",
            "0.3324320388963505\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_9 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_9 (Global  (None, 768)         0           ['tf_bert_model_9[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 768)         3072        ['global_max_pooling1d_9[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 128)          98432       ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 128)         512         ['dense_18[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_388 (Dropout)          (None, 128)          0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 32)           4128        ['dropout_388[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 32)          128         ['dense_19[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_389 (Dropout)          (None, 32)           0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_389[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 97s 138ms/step - loss: 2.3808 - categorical_accuracy: 0.1932 - val_loss: 2.2764 - val_categorical_accuracy: 0.1654\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.0465 - categorical_accuracy: 0.2382 - val_loss: 2.2527 - val_categorical_accuracy: 0.1728\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.9715 - categorical_accuracy: 0.2616 - val_loss: 2.1512 - val_categorical_accuracy: 0.1976\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.9250 - categorical_accuracy: 0.2761 - val_loss: 2.1476 - val_categorical_accuracy: 0.1958\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.8976 - categorical_accuracy: 0.2789 - val_loss: 2.1235 - val_categorical_accuracy: 0.2004\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.8760 - categorical_accuracy: 0.2834 - val_loss: 2.1127 - val_categorical_accuracy: 0.2077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.13      0.33      0.18       190\n",
            "         1.0       0.15      0.31      0.20       173\n",
            "         2.0       0.06      0.09      0.07       135\n",
            "         3.0       0.13      0.25      0.17       216\n",
            "         4.0       0.77      0.14      0.23      1036\n",
            "         5.0       0.09      0.55      0.16        76\n",
            "         6.0       0.75      0.67      0.71       195\n",
            "         7.0       0.08      0.01      0.02       290\n",
            "         8.0       0.14      0.12      0.13       139\n",
            "         9.0       0.27      0.31      0.29       258\n",
            "\n",
            "    accuracy                           0.22      2708\n",
            "   macro avg       0.26      0.28      0.22      2708\n",
            "weighted avg       0.42      0.22      0.22      2708\n",
            "\n",
            "0.27730544011117053\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 208s 310ms/step - loss: 1.8327 - categorical_accuracy: 0.3060 - val_loss: 2.0812 - val_categorical_accuracy: 0.2270\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.7918 - categorical_accuracy: 0.3132 - val_loss: 2.0525 - val_categorical_accuracy: 0.2307\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.7861 - categorical_accuracy: 0.3106 - val_loss: 2.0619 - val_categorical_accuracy: 0.2233\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.7660 - categorical_accuracy: 0.3217 - val_loss: 2.0690 - val_categorical_accuracy: 0.2178\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.7363 - categorical_accuracy: 0.3228 - val_loss: 2.0349 - val_categorical_accuracy: 0.2344\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.7307 - categorical_accuracy: 0.3257 - val_loss: 2.0390 - val_categorical_accuracy: 0.2353\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.30      0.19       190\n",
            "         1.0       0.18      0.44      0.26       173\n",
            "         2.0       0.05      0.07      0.06       135\n",
            "         3.0       0.13      0.22      0.16       216\n",
            "         4.0       0.77      0.14      0.23      1036\n",
            "         5.0       0.11      0.61      0.19        76\n",
            "         6.0       0.74      0.77      0.76       195\n",
            "         7.0       0.17      0.02      0.04       290\n",
            "         8.0       0.09      0.08      0.08       139\n",
            "         9.0       0.26      0.36      0.30       258\n",
            "\n",
            "    accuracy                           0.24      2708\n",
            "   macro avg       0.26      0.30      0.23      2708\n",
            "weighted avg       0.43      0.24      0.23      2708\n",
            "\n",
            "0.3010568221612521\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_10 (TFBertModel)  TFBaseModelOutputWi  108310272  ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_10 (Globa  (None, 768)         0           ['tf_bert_model_10[0][0]']       \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 768)         3072        ['global_max_pooling1d_10[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 128)          98432       ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 128)         512         ['dense_20[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_427 (Dropout)          (None, 128)          0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 32)           4128        ['dropout_427[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 32)          128         ['dense_21[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_428 (Dropout)          (None, 32)           0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_428[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 137ms/step - loss: 2.9265 - categorical_accuracy: 0.1207 - val_loss: 2.6387 - val_categorical_accuracy: 0.1314\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.5878 - categorical_accuracy: 0.1638 - val_loss: 2.5226 - val_categorical_accuracy: 0.1452\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.4414 - categorical_accuracy: 0.1837 - val_loss: 2.4686 - val_categorical_accuracy: 0.1507\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.3513 - categorical_accuracy: 0.1921 - val_loss: 2.4257 - val_categorical_accuracy: 0.1553\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.2787 - categorical_accuracy: 0.2026 - val_loss: 2.3699 - val_categorical_accuracy: 0.1710\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.2213 - categorical_accuracy: 0.2128 - val_loss: 2.3352 - val_categorical_accuracy: 0.1774\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.11      0.15      0.12       190\n",
            "         1.0       0.15      0.32      0.21       173\n",
            "         2.0       0.07      0.10      0.08       135\n",
            "         3.0       0.11      0.20      0.14       216\n",
            "         4.0       0.73      0.05      0.09      1036\n",
            "         5.0       0.10      0.39      0.16        76\n",
            "         6.0       0.54      0.75      0.63       195\n",
            "         7.0       0.13      0.16      0.14       290\n",
            "         8.0       0.06      0.14      0.09       139\n",
            "         9.0       0.21      0.15      0.18       258\n",
            "\n",
            "    accuracy                           0.17      2708\n",
            "   macro avg       0.22      0.24      0.18      2708\n",
            "weighted avg       0.39      0.17      0.16      2708\n",
            "\n",
            "0.24146822679477356\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 207s 310ms/step - loss: 2.1536 - categorical_accuracy: 0.2260 - val_loss: 2.4214 - val_categorical_accuracy: 0.1517\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.0548 - categorical_accuracy: 0.2438 - val_loss: 2.3240 - val_categorical_accuracy: 0.1737\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.9990 - categorical_accuracy: 0.2594 - val_loss: 2.3548 - val_categorical_accuracy: 0.1847\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.9510 - categorical_accuracy: 0.2684 - val_loss: 2.3624 - val_categorical_accuracy: 0.1857\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.8778 - categorical_accuracy: 0.2891 - val_loss: 2.4155 - val_categorical_accuracy: 0.1801\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.8361 - categorical_accuracy: 0.2968 - val_loss: 2.1882 - val_categorical_accuracy: 0.2132\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.15      0.25      0.19       190\n",
            "         1.0       0.16      0.47      0.24       173\n",
            "         2.0       0.07      0.07      0.07       135\n",
            "         3.0       0.15      0.25      0.19       216\n",
            "         4.0       0.80      0.06      0.11      1036\n",
            "         5.0       0.14      0.53      0.22        76\n",
            "         6.0       0.60      0.84      0.70       195\n",
            "         7.0       0.14      0.10      0.12       290\n",
            "         8.0       0.10      0.12      0.11       139\n",
            "         9.0       0.25      0.37      0.30       258\n",
            "\n",
            "    accuracy                           0.22      2708\n",
            "   macro avg       0.26      0.31      0.23      2708\n",
            "weighted avg       0.43      0.22      0.19      2708\n",
            "\n",
            "0.3065836265014024\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_11 (TFBertModel)  TFBaseModelOutputWi  108310272  ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_11 (Globa  (None, 768)         0           ['tf_bert_model_11[0][0]']       \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 768)         3072        ['global_max_pooling1d_11[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 128)          98432       ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 128)         512         ['dense_22[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_466 (Dropout)          (None, 128)          0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 32)           4128        ['dropout_466[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 32)          128         ['dense_23[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_467 (Dropout)          (None, 32)           0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_467[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 138ms/step - loss: 2.8943 - categorical_accuracy: 0.1300 - val_loss: 2.5679 - val_categorical_accuracy: 0.1379\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.6080 - categorical_accuracy: 0.1679 - val_loss: 2.4681 - val_categorical_accuracy: 0.1682\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.4696 - categorical_accuracy: 0.1965 - val_loss: 2.3919 - val_categorical_accuracy: 0.1682\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.3670 - categorical_accuracy: 0.2045 - val_loss: 2.3464 - val_categorical_accuracy: 0.1801\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 2.2742 - categorical_accuracy: 0.2193 - val_loss: 2.3086 - val_categorical_accuracy: 0.1866\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 81s 132ms/step - loss: 2.2184 - categorical_accuracy: 0.2244 - val_loss: 2.2915 - val_categorical_accuracy: 0.1976\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.16      0.17      0.17       190\n",
            "         1.0       0.11      0.36      0.17       173\n",
            "         2.0       0.06      0.07      0.07       135\n",
            "         3.0       0.10      0.05      0.07       216\n",
            "         4.0       0.66      0.11      0.19      1036\n",
            "         5.0       0.09      0.53      0.15        76\n",
            "         6.0       0.55      0.71      0.62       195\n",
            "         7.0       0.11      0.19      0.14       290\n",
            "         8.0       0.12      0.14      0.13       139\n",
            "         9.0       0.23      0.15      0.18       258\n",
            "\n",
            "    accuracy                           0.19      2708\n",
            "   macro avg       0.22      0.25      0.19      2708\n",
            "weighted avg       0.36      0.19      0.19      2708\n",
            "\n",
            "0.24737444772168304\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_11/bert/pooler/dense/kernel:0', 'tf_bert_model_11/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_11/bert/pooler/dense/kernel:0', 'tf_bert_model_11/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 205s 310ms/step - loss: 2.1899 - categorical_accuracy: 0.2310 - val_loss: 2.2576 - val_categorical_accuracy: 0.2004\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.1636 - categorical_accuracy: 0.2354 - val_loss: 2.2333 - val_categorical_accuracy: 0.1985\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.1462 - categorical_accuracy: 0.2406 - val_loss: 2.2322 - val_categorical_accuracy: 0.1985\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.1242 - categorical_accuracy: 0.2385 - val_loss: 2.2225 - val_categorical_accuracy: 0.2040\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.0941 - categorical_accuracy: 0.2464 - val_loss: 2.2332 - val_categorical_accuracy: 0.1939\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.0884 - categorical_accuracy: 0.2530 - val_loss: 2.1772 - val_categorical_accuracy: 0.2132\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.17      0.16      0.17       190\n",
            "         1.0       0.13      0.41      0.19       173\n",
            "         2.0       0.06      0.05      0.06       135\n",
            "         3.0       0.07      0.03      0.04       216\n",
            "         4.0       0.67      0.16      0.26      1036\n",
            "         5.0       0.09      0.61      0.16        76\n",
            "         6.0       0.55      0.78      0.65       195\n",
            "         7.0       0.12      0.16      0.14       290\n",
            "         8.0       0.12      0.12      0.12       139\n",
            "         9.0       0.22      0.19      0.20       258\n",
            "\n",
            "    accuracy                           0.22      2708\n",
            "   macro avg       0.22      0.27      0.20      2708\n",
            "weighted avg       0.37      0.22      0.22      2708\n",
            "\n",
            "0.26727599062528523\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_accuracies_fine_tuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTDO3kj0Zdyv",
        "outputId": "4f97e10b-cdc9-4927-cb4b-bcc5849c034c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2978907996233228,\n",
              " 0.3072547196607738,\n",
              " 0.33623967038181124,\n",
              " 0.2984879713938942]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "#drop_out_rates = [0.1, 0.2, 0.3,0.4, 0.5]\n",
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "#for drop_out_rate in drop_out_rates:\n",
        "\n",
        "drop_out_rate = 0.4\n",
        "learning_rate_transfer_learnings = [1e-3, 1e-4]\n",
        "learning_rate_fine_tunings = [1e-5, 1e-6]\n",
        "\n",
        "for learning_rate_transfer_learning in learning_rate_transfer_learnings:\n",
        "  for learning_rate_fine_tuning in learning_rate_fine_tunings:\n",
        "    print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        "    , learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "    #step1\n",
        "    bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "    embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "    model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "    model1.layers[2].trainable = False\n",
        "    print(model1.summary())\n",
        "\n",
        "    #step2\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "    #model2.summary() #Check trainable params increased.\n",
        "\n",
        "    #step3: transfer learning\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "    #step4: predict\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "    #step5: fine tune\n",
        "    print(\"Fine tuning---------------\")\n",
        "    model1.layers[2].trainable = True\n",
        "\n",
        "    # It's important to recompile your model after you make any changes\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "    print(\"----------------------------------------\")\n",
        "    del(model1)\n"
      ],
      "metadata": {
        "id": "JLrUb4Nas5yh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3f52d2-2928-43bd-824b-2eae8af8f8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_12 (TFBertModel)  TFBaseModelOutputWi  108310272  ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_12 (Globa  (None, 768)         0           ['tf_bert_model_12[0][0]']       \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 768)         3072        ['global_max_pooling1d_12[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 128)          98432       ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 128)         512         ['dense_24[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_505 (Dropout)          (None, 128)          0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 32)           4128        ['dropout_505[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 32)          128         ['dense_25[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_506 (Dropout)          (None, 32)           0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_506[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 95s 136ms/step - loss: 2.4911 - categorical_accuracy: 0.1799 - val_loss: 2.1950 - val_categorical_accuracy: 0.1976\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.1066 - categorical_accuracy: 0.2338 - val_loss: 2.1802 - val_categorical_accuracy: 0.1994\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.0156 - categorical_accuracy: 0.2560 - val_loss: 2.1596 - val_categorical_accuracy: 0.2105\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - ETA: 0s - loss: 1.9544 - categorical_accuracy: 0.2668Epoch 5/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 1.9360 - categorical_accuracy: 0.2788 - val_loss: 2.1088 - val_categorical_accuracy: 0.2077\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.9138 - categorical_accuracy: 0.2785 - val_loss: 2.1060 - val_categorical_accuracy: 0.2132\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.12      0.17      0.14       190\n",
            "         1.0       0.16      0.49      0.24       173\n",
            "         2.0       0.06      0.16      0.09       135\n",
            "         3.0       0.15      0.17      0.16       216\n",
            "         4.0       0.78      0.13      0.22      1036\n",
            "         5.0       0.10      0.59      0.17        76\n",
            "         6.0       0.75      0.63      0.69       195\n",
            "         7.0       0.00      0.00      0.00       290\n",
            "         8.0       0.08      0.13      0.10       139\n",
            "         9.0       0.25      0.31      0.28       258\n",
            "\n",
            "    accuracy                           0.21      2708\n",
            "   macro avg       0.25      0.28      0.21      2708\n",
            "weighted avg       0.42      0.21      0.21      2708\n",
            "\n",
            "0.27754068258687276\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_12/bert/pooler/dense/kernel:0', 'tf_bert_model_12/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 206s 309ms/step - loss: 1.8742 - categorical_accuracy: 0.2843 - val_loss: 2.0269 - val_categorical_accuracy: 0.2086\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 305ms/step - loss: 1.7724 - categorical_accuracy: 0.3074 - val_loss: 1.9860 - val_categorical_accuracy: 0.2555\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7088 - categorical_accuracy: 0.3216 - val_loss: 2.0104 - val_categorical_accuracy: 0.2252\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.6375 - categorical_accuracy: 0.3403 - val_loss: 1.9102 - val_categorical_accuracy: 0.2463\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.5897 - categorical_accuracy: 0.3639 - val_loss: 2.0252 - val_categorical_accuracy: 0.2647\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.5456 - categorical_accuracy: 0.3622 - val_loss: 2.0124 - val_categorical_accuracy: 0.2344\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.16      0.24      0.19       190\n",
            "         1.0       0.18      0.60      0.27       173\n",
            "         2.0       0.05      0.11      0.07       135\n",
            "         3.0       0.15      0.12      0.14       216\n",
            "         4.0       0.85      0.08      0.15      1036\n",
            "         5.0       0.15      0.61      0.24        76\n",
            "         6.0       0.77      0.78      0.78       195\n",
            "         7.0       0.00      0.00      0.00       290\n",
            "         8.0       0.08      0.17      0.11       139\n",
            "         9.0       0.27      0.50      0.35       258\n",
            "\n",
            "    accuracy                           0.23      2708\n",
            "   macro avg       0.27      0.32      0.23      2708\n",
            "weighted avg       0.45      0.23      0.21      2708\n",
            "\n",
            "0.3210004381171597\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_13 (TFBertModel)  TFBaseModelOutputWi  108310272  ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_13 (Globa  (None, 768)         0           ['tf_bert_model_13[0][0]']       \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 768)         3072        ['global_max_pooling1d_13[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 128)          98432       ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 128)         512         ['dense_26[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_544 (Dropout)          (None, 128)          0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 32)           4128        ['dropout_544[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 32)          128         ['dense_27[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_545 (Dropout)          (None, 32)           0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_545[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 95s 136ms/step - loss: 2.5132 - categorical_accuracy: 0.1575 - val_loss: 2.2243 - val_categorical_accuracy: 0.1912\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.1290 - categorical_accuracy: 0.2193 - val_loss: 2.2062 - val_categorical_accuracy: 0.1636\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.0257 - categorical_accuracy: 0.2488 - val_loss: 2.1629 - val_categorical_accuracy: 0.1903\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 1.9690 - categorical_accuracy: 0.2580 - val_loss: 2.1512 - val_categorical_accuracy: 0.1930\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 1.9434 - categorical_accuracy: 0.2620 - val_loss: 2.1374 - val_categorical_accuracy: 0.1884\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 81s 133ms/step - loss: 1.9199 - categorical_accuracy: 0.2660 - val_loss: 2.1232 - val_categorical_accuracy: 0.2022\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.12      0.33      0.18       190\n",
            "         1.0       0.16      0.36      0.22       173\n",
            "         2.0       0.08      0.15      0.10       135\n",
            "         3.0       0.13      0.23      0.17       216\n",
            "         4.0       0.78      0.06      0.12      1036\n",
            "         5.0       0.11      0.57      0.18        76\n",
            "         6.0       0.74      0.70      0.72       195\n",
            "         7.0       0.50      0.00      0.01       290\n",
            "         8.0       0.08      0.09      0.08       139\n",
            "         9.0       0.26      0.34      0.29       258\n",
            "\n",
            "    accuracy                           0.20      2708\n",
            "   macro avg       0.30      0.28      0.21      2708\n",
            "weighted avg       0.47      0.20      0.18      2708\n",
            "\n",
            "0.28269384351074145\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_13/bert/pooler/dense/kernel:0', 'tf_bert_model_13/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_13/bert/pooler/dense/kernel:0', 'tf_bert_model_13/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 206s 310ms/step - loss: 1.8845 - categorical_accuracy: 0.2717 - val_loss: 2.0973 - val_categorical_accuracy: 0.1976\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.8655 - categorical_accuracy: 0.2795 - val_loss: 2.0901 - val_categorical_accuracy: 0.2096\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 305ms/step - loss: 1.8531 - categorical_accuracy: 0.2843 - val_loss: 2.0743 - val_categorical_accuracy: 0.2188\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 305ms/step - loss: 1.8373 - categorical_accuracy: 0.2847 - val_loss: 2.0611 - val_categorical_accuracy: 0.2096\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.8198 - categorical_accuracy: 0.2811 - val_loss: 2.0740 - val_categorical_accuracy: 0.2142\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7932 - categorical_accuracy: 0.2932 - val_loss: 2.0754 - val_categorical_accuracy: 0.2068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.13      0.37      0.19       190\n",
            "         1.0       0.18      0.42      0.25       173\n",
            "         2.0       0.07      0.10      0.09       135\n",
            "         3.0       0.12      0.19      0.15       216\n",
            "         4.0       0.81      0.09      0.16      1036\n",
            "         5.0       0.11      0.68      0.19        76\n",
            "         6.0       0.76      0.72      0.74       195\n",
            "         7.0       0.00      0.00      0.00       290\n",
            "         8.0       0.10      0.09      0.09       139\n",
            "         9.0       0.29      0.40      0.33       258\n",
            "\n",
            "    accuracy                           0.22      2708\n",
            "   macro avg       0.26      0.31      0.22      2708\n",
            "weighted avg       0.43      0.22      0.20      2708\n",
            "\n",
            "0.3061176624227499\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_14 (TFBertModel)  TFBaseModelOutputWi  108310272  ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_14 (Globa  (None, 768)         0           ['tf_bert_model_14[0][0]']       \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 768)         3072        ['global_max_pooling1d_14[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_28 (Dense)               (None, 128)          98432       ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 128)         512         ['dense_28[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_583 (Dropout)          (None, 128)          0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " dense_29 (Dense)               (None, 32)           4128        ['dropout_583[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 32)          128         ['dense_29[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_584 (Dropout)          (None, 32)           0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_584[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 136ms/step - loss: 3.0085 - categorical_accuracy: 0.1097 - val_loss: 2.6222 - val_categorical_accuracy: 0.1085\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.7296 - categorical_accuracy: 0.1390 - val_loss: 2.5043 - val_categorical_accuracy: 0.1232\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.5907 - categorical_accuracy: 0.1536 - val_loss: 2.4378 - val_categorical_accuracy: 0.1351\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.4539 - categorical_accuracy: 0.1732 - val_loss: 2.3871 - val_categorical_accuracy: 0.1388\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.3959 - categorical_accuracy: 0.1754 - val_loss: 2.3607 - val_categorical_accuracy: 0.1425\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.2946 - categorical_accuracy: 0.1937 - val_loss: 2.3271 - val_categorical_accuracy: 0.1379\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.10      0.23      0.14       190\n",
            "         1.0       0.12      0.38      0.18       173\n",
            "         2.0       0.06      0.06      0.06       135\n",
            "         3.0       0.11      0.04      0.06       216\n",
            "         4.0       0.63      0.03      0.06      1036\n",
            "         5.0       0.07      0.53      0.13        76\n",
            "         6.0       0.53      0.74      0.62       195\n",
            "         7.0       0.16      0.07      0.10       290\n",
            "         8.0       0.11      0.23      0.15       139\n",
            "         9.0       0.21      0.15      0.18       258\n",
            "\n",
            "    accuracy                           0.16      2708\n",
            "   macro avg       0.21      0.25      0.17      2708\n",
            "weighted avg       0.35      0.16      0.13      2708\n",
            "\n",
            "0.24651454626963992\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_14/bert/pooler/dense/kernel:0', 'tf_bert_model_14/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_14/bert/pooler/dense/kernel:0', 'tf_bert_model_14/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 205s 311ms/step - loss: 2.2454 - categorical_accuracy: 0.2037 - val_loss: 2.3167 - val_categorical_accuracy: 0.1517\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.1576 - categorical_accuracy: 0.2081 - val_loss: 2.2810 - val_categorical_accuracy: 0.1572\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.0999 - categorical_accuracy: 0.2238 - val_loss: 2.0879 - val_categorical_accuracy: 0.1958\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 305ms/step - loss: 2.0538 - categorical_accuracy: 0.2342 - val_loss: 2.1356 - val_categorical_accuracy: 0.1884\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.9923 - categorical_accuracy: 0.2401 - val_loss: 2.0899 - val_categorical_accuracy: 0.2059\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 305ms/step - loss: 1.9590 - categorical_accuracy: 0.2514 - val_loss: 1.9926 - val_categorical_accuracy: 0.2482\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.21      0.16       190\n",
            "         1.0       0.21      0.51      0.29       173\n",
            "         2.0       0.05      0.05      0.05       135\n",
            "         3.0       0.24      0.06      0.09       216\n",
            "         4.0       0.74      0.18      0.29      1036\n",
            "         5.0       0.12      0.63      0.20        76\n",
            "         6.0       0.71      0.79      0.75       195\n",
            "         7.0       0.14      0.08      0.10       290\n",
            "         8.0       0.12      0.29      0.17       139\n",
            "         9.0       0.24      0.40      0.30       258\n",
            "\n",
            "    accuracy                           0.26      2708\n",
            "   macro avg       0.27      0.32      0.24      2708\n",
            "weighted avg       0.43      0.26      0.26      2708\n",
            "\n",
            "0.3194046170510987\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_15 (TFBertModel)  TFBaseModelOutputWi  108310272  ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_15 (Globa  (None, 768)         0           ['tf_bert_model_15[0][0]']       \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 768)         3072        ['global_max_pooling1d_15[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_30 (Dense)               (None, 128)          98432       ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 128)         512         ['dense_30[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_622 (Dropout)          (None, 128)          0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " dense_31 (Dense)               (None, 32)           4128        ['dropout_622[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 32)          128         ['dense_31[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_623 (Dropout)          (None, 32)           0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_623[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 136ms/step - loss: 2.9780 - categorical_accuracy: 0.1295 - val_loss: 2.4494 - val_categorical_accuracy: 0.1562\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.6820 - categorical_accuracy: 0.1526 - val_loss: 2.3577 - val_categorical_accuracy: 0.1792\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.5377 - categorical_accuracy: 0.1758 - val_loss: 2.3176 - val_categorical_accuracy: 0.1774\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.4165 - categorical_accuracy: 0.1904 - val_loss: 2.2791 - val_categorical_accuracy: 0.1893\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.3631 - categorical_accuracy: 0.1863 - val_loss: 2.2355 - val_categorical_accuracy: 0.2031\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.3117 - categorical_accuracy: 0.2031 - val_loss: 2.2211 - val_categorical_accuracy: 0.1884\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.15      0.13      0.14       190\n",
            "         1.0       0.13      0.38      0.19       173\n",
            "         2.0       0.08      0.03      0.04       135\n",
            "         3.0       0.09      0.12      0.10       216\n",
            "         4.0       0.66      0.15      0.25      1036\n",
            "         5.0       0.09      0.37      0.15        76\n",
            "         6.0       0.50      0.79      0.62       195\n",
            "         7.0       0.12      0.07      0.09       290\n",
            "         8.0       0.08      0.19      0.11       139\n",
            "         9.0       0.18      0.22      0.20       258\n",
            "\n",
            "    accuracy                           0.21      2708\n",
            "   macro avg       0.21      0.25      0.19      2708\n",
            "weighted avg       0.36      0.21      0.21      2708\n",
            "\n",
            "0.2456107728611991\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_15/bert/pooler/dense/kernel:0', 'tf_bert_model_15/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_15/bert/pooler/dense/kernel:0', 'tf_bert_model_15/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 206s 309ms/step - loss: 2.2480 - categorical_accuracy: 0.2106 - val_loss: 2.1989 - val_categorical_accuracy: 0.1866\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.2344 - categorical_accuracy: 0.2115 - val_loss: 2.2070 - val_categorical_accuracy: 0.1958\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.1976 - categorical_accuracy: 0.2133 - val_loss: 2.1671 - val_categorical_accuracy: 0.2215\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.2025 - categorical_accuracy: 0.2226 - val_loss: 2.1564 - val_categorical_accuracy: 0.2151\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.1878 - categorical_accuracy: 0.2148 - val_loss: 2.1272 - val_categorical_accuracy: 0.2390\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.1710 - categorical_accuracy: 0.2250 - val_loss: 2.1497 - val_categorical_accuracy: 0.2142\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.13      0.13       190\n",
            "         1.0       0.13      0.33      0.19       173\n",
            "         2.0       0.10      0.04      0.05       135\n",
            "         3.0       0.12      0.16      0.14       216\n",
            "         4.0       0.68      0.19      0.30      1036\n",
            "         5.0       0.11      0.50      0.18        76\n",
            "         6.0       0.52      0.85      0.64       195\n",
            "         7.0       0.10      0.05      0.07       290\n",
            "         8.0       0.10      0.22      0.14       139\n",
            "         9.0       0.19      0.28      0.23       258\n",
            "\n",
            "    accuracy                           0.24      2708\n",
            "   macro avg       0.22      0.27      0.21      2708\n",
            "weighted avg       0.37      0.24      0.24      2708\n",
            "\n",
            "0.2739177141108749\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "#drop_out_rates = [0.1, 0.2, 0.3,0.4, 0.5]\n",
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "#for drop_out_rate in drop_out_rates:\n",
        "\n",
        "drop_out_rate = 0.5\n",
        "learning_rate_transfer_learnings = [1e-3, 1e-4]\n",
        "learning_rate_fine_tunings = [1e-5, 1e-6]\n",
        "\n",
        "for learning_rate_transfer_learning in learning_rate_transfer_learnings:\n",
        "  for learning_rate_fine_tuning in learning_rate_fine_tunings:\n",
        "    print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        "    , learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "    #step1\n",
        "    bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "    embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "    model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "    model1.layers[2].trainable = False\n",
        "    print(model1.summary())\n",
        "\n",
        "    #step2\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "    #model2.summary() #Check trainable params increased.\n",
        "\n",
        "    #step3: transfer learning\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "    #step4: predict\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "    #step5: fine tune\n",
        "    print(\"Fine tuning---------------\")\n",
        "    model1.layers[2].trainable = True\n",
        "\n",
        "    # It's important to recompile your model after you make any changes\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "    print(\"----------------------------------------\")\n",
        "    del(model1)\n"
      ],
      "metadata": {
        "id": "c5ZI3tTEs9FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f7a4e0-1423-40c0-c7de-7b1eb1795393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_16 (TFBertModel)  TFBaseModelOutputWi  108310272  ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_16 (Globa  (None, 768)         0           ['tf_bert_model_16[0][0]']       \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 768)         3072        ['global_max_pooling1d_16[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 128)          98432       ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 128)         512         ['dense_32[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_661 (Dropout)          (None, 128)          0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " dense_33 (Dense)               (None, 32)           4128        ['dropout_661[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 32)          128         ['dense_33[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_662 (Dropout)          (None, 32)           0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_662[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 136ms/step - loss: 2.5968 - categorical_accuracy: 0.1793 - val_loss: 2.3679 - val_categorical_accuracy: 0.1553\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.1655 - categorical_accuracy: 0.2103 - val_loss: 2.2613 - val_categorical_accuracy: 0.1664\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.0602 - categorical_accuracy: 0.2352 - val_loss: 2.2162 - val_categorical_accuracy: 0.1618\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.0097 - categorical_accuracy: 0.2413 - val_loss: 2.2120 - val_categorical_accuracy: 0.1710\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 1.9903 - categorical_accuracy: 0.2505 - val_loss: 2.1904 - val_categorical_accuracy: 0.1783\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.9652 - categorical_accuracy: 0.2616 - val_loss: 2.1645 - val_categorical_accuracy: 0.2004\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.11      0.08      0.09       190\n",
            "         1.0       0.13      0.46      0.20       173\n",
            "         2.0       0.06      0.06      0.06       135\n",
            "         3.0       0.11      0.25      0.15       216\n",
            "         4.0       0.75      0.12      0.21      1036\n",
            "         5.0       0.08      0.66      0.14        76\n",
            "         6.0       0.75      0.68      0.71       195\n",
            "         7.0       0.00      0.00      0.00       290\n",
            "         8.0       0.00      0.00      0.00       139\n",
            "         9.0       0.28      0.30      0.29       258\n",
            "\n",
            "    accuracy                           0.20      2708\n",
            "   macro avg       0.23      0.26      0.19      2708\n",
            "weighted avg       0.40      0.20      0.20      2708\n",
            "\n",
            "0.2612531614734267\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_16/bert/pooler/dense/kernel:0', 'tf_bert_model_16/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_16/bert/pooler/dense/kernel:0', 'tf_bert_model_16/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 205s 309ms/step - loss: 1.9112 - categorical_accuracy: 0.2781 - val_loss: 2.1209 - val_categorical_accuracy: 0.1829\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.8513 - categorical_accuracy: 0.2934 - val_loss: 2.0689 - val_categorical_accuracy: 0.2261\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7980 - categorical_accuracy: 0.3018 - val_loss: 1.9970 - val_categorical_accuracy: 0.2325\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7573 - categorical_accuracy: 0.3095 - val_loss: 1.9560 - val_categorical_accuracy: 0.2224\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7049 - categorical_accuracy: 0.3346 - val_loss: 2.0359 - val_categorical_accuracy: 0.2482\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.6667 - categorical_accuracy: 0.3391 - val_loss: 1.9463 - val_categorical_accuracy: 0.2629\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.15      0.19      0.17       190\n",
            "         1.0       0.16      0.68      0.25       173\n",
            "         2.0       0.07      0.06      0.07       135\n",
            "         3.0       0.14      0.17      0.15       216\n",
            "         4.0       0.75      0.17      0.28      1036\n",
            "         5.0       0.13      0.53      0.21        76\n",
            "         6.0       0.74      0.82      0.78       195\n",
            "         7.0       0.00      0.00      0.00       290\n",
            "         8.0       0.16      0.02      0.04       139\n",
            "         9.0       0.27      0.55      0.36       258\n",
            "\n",
            "    accuracy                           0.27      2708\n",
            "   macro avg       0.26      0.32      0.23      2708\n",
            "weighted avg       0.41      0.27      0.25      2708\n",
            "\n",
            "0.319121521314166\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_17 (TFBertModel)  TFBaseModelOutputWi  108310272  ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_17 (Globa  (None, 768)         0           ['tf_bert_model_17[0][0]']       \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 768)         3072        ['global_max_pooling1d_17[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 128)          98432       ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 128)         512         ['dense_34[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_700 (Dropout)          (None, 128)          0           ['batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 32)           4128        ['dropout_700[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 32)          128         ['dense_35[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_701 (Dropout)          (None, 32)           0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_701[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 95s 136ms/step - loss: 2.6100 - categorical_accuracy: 0.1753 - val_loss: 2.2526 - val_categorical_accuracy: 0.1710\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.1764 - categorical_accuracy: 0.2143 - val_loss: 2.1841 - val_categorical_accuracy: 0.1949\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.0700 - categorical_accuracy: 0.2389 - val_loss: 2.1879 - val_categorical_accuracy: 0.1893\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.0171 - categorical_accuracy: 0.2562 - val_loss: 2.1592 - val_categorical_accuracy: 0.1792\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.9876 - categorical_accuracy: 0.2489 - val_loss: 2.1447 - val_categorical_accuracy: 0.2123\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.9734 - categorical_accuracy: 0.2547 - val_loss: 2.0963 - val_categorical_accuracy: 0.2289\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.17      0.30      0.22       190\n",
            "         1.0       0.14      0.51      0.23       173\n",
            "         2.0       0.05      0.09      0.06       135\n",
            "         3.0       0.14      0.30      0.19       216\n",
            "         4.0       0.77      0.15      0.25      1036\n",
            "         5.0       0.10      0.46      0.16        76\n",
            "         6.0       0.73      0.71      0.72       195\n",
            "         7.0       0.00      0.00      0.00       290\n",
            "         8.0       0.05      0.02      0.03       139\n",
            "         9.0       0.27      0.26      0.27       258\n",
            "\n",
            "    accuracy                           0.23      2708\n",
            "   macro avg       0.24      0.28      0.21      2708\n",
            "weighted avg       0.41      0.23      0.23      2708\n",
            "\n",
            "0.2806780154781661\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_17/bert/pooler/dense/kernel:0', 'tf_bert_model_17/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_17/bert/pooler/dense/kernel:0', 'tf_bert_model_17/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 206s 309ms/step - loss: 1.9333 - categorical_accuracy: 0.2635 - val_loss: 2.0794 - val_categorical_accuracy: 0.2353\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.9181 - categorical_accuracy: 0.2625 - val_loss: 2.0607 - val_categorical_accuracy: 0.2316\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.9049 - categorical_accuracy: 0.2707 - val_loss: 2.0705 - val_categorical_accuracy: 0.2289\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.8966 - categorical_accuracy: 0.2720 - val_loss: 2.0582 - val_categorical_accuracy: 0.2381\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.8827 - categorical_accuracy: 0.2766 - val_loss: 2.0461 - val_categorical_accuracy: 0.2399\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.8563 - categorical_accuracy: 0.2852 - val_loss: 2.0504 - val_categorical_accuracy: 0.2344\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.19      0.23      0.21       190\n",
            "         1.0       0.15      0.57      0.24       173\n",
            "         2.0       0.04      0.06      0.05       135\n",
            "         3.0       0.12      0.23      0.16       216\n",
            "         4.0       0.72      0.16      0.27      1036\n",
            "         5.0       0.12      0.67      0.20        76\n",
            "         6.0       0.73      0.76      0.75       195\n",
            "         7.0       0.00      0.00      0.00       290\n",
            "         8.0       0.05      0.02      0.03       139\n",
            "         9.0       0.27      0.31      0.29       258\n",
            "\n",
            "    accuracy                           0.24      2708\n",
            "   macro avg       0.24      0.30      0.22      2708\n",
            "weighted avg       0.40      0.24      0.24      2708\n",
            "\n",
            "0.301697653771437\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_18 (TFBertModel)  TFBaseModelOutputWi  108310272  ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_18 (Globa  (None, 768)         0           ['tf_bert_model_18[0][0]']       \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 768)         3072        ['global_max_pooling1d_18[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 128)          98432       ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 128)         512         ['dense_36[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_739 (Dropout)          (None, 128)          0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " dense_37 (Dense)               (None, 32)           4128        ['dropout_739[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 32)          128         ['dense_37[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_740 (Dropout)          (None, 32)           0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_740[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 136ms/step - loss: 3.2902 - categorical_accuracy: 0.1105 - val_loss: 2.6549 - val_categorical_accuracy: 0.1002\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.9812 - categorical_accuracy: 0.1307 - val_loss: 2.5858 - val_categorical_accuracy: 0.1268\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.7652 - categorical_accuracy: 0.1459 - val_loss: 2.4925 - val_categorical_accuracy: 0.1296\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.6322 - categorical_accuracy: 0.1609 - val_loss: 2.4609 - val_categorical_accuracy: 0.1397\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.5285 - categorical_accuracy: 0.1701 - val_loss: 2.4126 - val_categorical_accuracy: 0.1379\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.4527 - categorical_accuracy: 0.1755 - val_loss: 2.3769 - val_categorical_accuracy: 0.1443\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.06      0.08      0.07       190\n",
            "         1.0       0.10      0.34      0.16       173\n",
            "         2.0       0.04      0.07      0.05       135\n",
            "         3.0       0.11      0.11      0.11       216\n",
            "         4.0       0.88      0.04      0.08      1036\n",
            "         5.0       0.07      0.26      0.11        76\n",
            "         6.0       0.56      0.66      0.60       195\n",
            "         7.0       0.14      0.15      0.14       290\n",
            "         8.0       0.08      0.24      0.13       139\n",
            "         9.0       0.20      0.09      0.13       258\n",
            "\n",
            "    accuracy                           0.15      2708\n",
            "   macro avg       0.22      0.20      0.16      2708\n",
            "weighted avg       0.44      0.15      0.14      2708\n",
            "\n",
            "0.20471191944586872\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_18/bert/pooler/dense/kernel:0', 'tf_bert_model_18/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_18/bert/pooler/dense/kernel:0', 'tf_bert_model_18/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 206s 309ms/step - loss: 2.3584 - categorical_accuracy: 0.1923 - val_loss: 2.2391 - val_categorical_accuracy: 0.1682\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.2737 - categorical_accuracy: 0.1996 - val_loss: 2.4010 - val_categorical_accuracy: 0.1654\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 305ms/step - loss: 2.2404 - categorical_accuracy: 0.2072 - val_loss: 2.4347 - val_categorical_accuracy: 0.1599\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.1597 - categorical_accuracy: 0.2181 - val_loss: 2.4060 - val_categorical_accuracy: 0.1719\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.1290 - categorical_accuracy: 0.2300 - val_loss: 2.5178 - val_categorical_accuracy: 0.1562\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.1129 - categorical_accuracy: 0.2258 - val_loss: 2.0889 - val_categorical_accuracy: 0.2316\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.15      0.11      0.12       190\n",
            "         1.0       0.17      0.36      0.23       173\n",
            "         2.0       0.07      0.05      0.06       135\n",
            "         3.0       0.11      0.09      0.10       216\n",
            "         4.0       0.82      0.17      0.28      1036\n",
            "         5.0       0.10      0.70      0.17        76\n",
            "         6.0       0.61      0.83      0.70       195\n",
            "         7.0       0.20      0.11      0.14       290\n",
            "         8.0       0.15      0.32      0.21       139\n",
            "         9.0       0.25      0.44      0.32       258\n",
            "\n",
            "    accuracy                           0.26      2708\n",
            "   macro avg       0.26      0.32      0.23      2708\n",
            "weighted avg       0.45      0.26      0.25      2708\n",
            "\n",
            "0.31775165082193035\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_19 (TFBertModel)  TFBaseModelOutputWi  108310272  ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_19 (Globa  (None, 768)         0           ['tf_bert_model_19[0][0]']       \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 768)         3072        ['global_max_pooling1d_19[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_38 (Dense)               (None, 128)          98432       ['batch_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 128)         512         ['dense_38[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_778 (Dropout)          (None, 128)          0           ['batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " dense_39 (Dense)               (None, 32)           4128        ['dropout_778[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 32)          128         ['dense_39[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_779 (Dropout)          (None, 32)           0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_779[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 136ms/step - loss: 3.2400 - categorical_accuracy: 0.1301 - val_loss: 2.5474 - val_categorical_accuracy: 0.1213\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.9411 - categorical_accuracy: 0.1521 - val_loss: 2.4689 - val_categorical_accuracy: 0.1434\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.7930 - categorical_accuracy: 0.1595 - val_loss: 2.4184 - val_categorical_accuracy: 0.1590\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.6223 - categorical_accuracy: 0.1712 - val_loss: 2.3623 - val_categorical_accuracy: 0.1728\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.5058 - categorical_accuracy: 0.1790 - val_loss: 2.3154 - val_categorical_accuracy: 0.1811\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.4368 - categorical_accuracy: 0.1833 - val_loss: 2.2844 - val_categorical_accuracy: 0.1682\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.11      0.16      0.13       190\n",
            "         1.0       0.09      0.25      0.13       173\n",
            "         2.0       0.03      0.02      0.02       135\n",
            "         3.0       0.10      0.06      0.07       216\n",
            "         4.0       0.70      0.09      0.17      1036\n",
            "         5.0       0.05      0.50      0.10        76\n",
            "         6.0       0.55      0.77      0.64       195\n",
            "         7.0       0.11      0.02      0.03       290\n",
            "         8.0       0.08      0.12      0.10       139\n",
            "         9.0       0.19      0.24      0.21       258\n",
            "\n",
            "    accuracy                           0.17      2708\n",
            "   macro avg       0.20      0.22      0.16      2708\n",
            "weighted avg       0.37      0.17      0.17      2708\n",
            "\n",
            "0.22382161797882238\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_19/bert/pooler/dense/kernel:0', 'tf_bert_model_19/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_19/bert/pooler/dense/kernel:0', 'tf_bert_model_19/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 206s 309ms/step - loss: 2.3864 - categorical_accuracy: 0.1940 - val_loss: 2.3040 - val_categorical_accuracy: 0.1581\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.3871 - categorical_accuracy: 0.1925 - val_loss: 2.2908 - val_categorical_accuracy: 0.1553\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.3389 - categorical_accuracy: 0.2001 - val_loss: 2.2656 - val_categorical_accuracy: 0.1553\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.3403 - categorical_accuracy: 0.2043 - val_loss: 2.2647 - val_categorical_accuracy: 0.1590\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.3109 - categorical_accuracy: 0.2046 - val_loss: 2.2260 - val_categorical_accuracy: 0.1765\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 2.3329 - categorical_accuracy: 0.2005 - val_loss: 2.2375 - val_categorical_accuracy: 0.1673\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.10      0.12      0.11       190\n",
            "         1.0       0.10      0.30      0.15       173\n",
            "         2.0       0.05      0.04      0.05       135\n",
            "         3.0       0.15      0.08      0.10       216\n",
            "         4.0       0.70      0.09      0.17      1036\n",
            "         5.0       0.07      0.59      0.12        76\n",
            "         6.0       0.50      0.83      0.62       195\n",
            "         7.0       0.22      0.02      0.04       290\n",
            "         8.0       0.11      0.12      0.11       139\n",
            "         9.0       0.21      0.31      0.25       258\n",
            "\n",
            "    accuracy                           0.19      2708\n",
            "   macro avg       0.22      0.25      0.17      2708\n",
            "weighted avg       0.38      0.19      0.17      2708\n",
            "\n",
            "0.2509626384731621\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "McNemar_BERT_batch_normalize_and_dropout_split0 =pd.DataFrame(data=McNemar[(True, 0.3,1e-3,1e-5)])\n",
        "McNemar_BERT_batch_normalize_and_dropout_split0.to_csv(DIR + 'McNemar_BERT_batch_normalize_and_dropout_split0_fine_tune.csv')\n",
        "\n",
        "McNemar_BERT_batch_normalize_and_dropout_split0 =pd.DataFrame(data=McNemar[(False, 0.3,1e-3,1e-5)])\n",
        "McNemar_BERT_batch_normalize_and_dropout_split0.to_csv(DIR + 'McNemar_BERT_batch_normalize_and_dropout_split0_transfer_learning.csv')"
      ],
      "metadata": {
        "id": "fQGQ5tadFgvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resume from the last cell"
      ],
      "metadata": {
        "id": "sErIpS6Ed4-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "#drop_out_rates = [0.1, 0.2, 0.3,0.4, 0.5]\n",
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "#for drop_out_rate in drop_out_rates:\n",
        "\n",
        "drop_out_rate = 0.3\n",
        "learning_rate_transfer_learnings = [ 1e-4]\n",
        "learning_rate_fine_tunings = [1e-6]\n",
        "\n",
        "for learning_rate_transfer_learning in learning_rate_transfer_learnings:\n",
        "  for learning_rate_fine_tuning in learning_rate_fine_tunings:\n",
        "    print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        "    , learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "    #step1\n",
        "    bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "    embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "    y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "    model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "    model1.layers[2].trainable = False\n",
        "    #model2.summary()\n",
        "\n",
        "    #step2\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "    #model2.summary() #Check trainable params increased.\n",
        "\n",
        "    #step3: transfer learning\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "    #step4: predict\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "    #step5: fine tune\n",
        "    print(\"Fine tuning---------------\")\n",
        "    model1.layers[2].trainable = True\n",
        "\n",
        "    # It's important to recompile your model after you make any changes\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "    print(\"----------------------------------------\")\n",
        "    del(model1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d272c4ca403b4a47b1ece3efc81b4c60",
            "1e504b3e620e44a5b2524210523370b9",
            "fa78b894e25c444db500eb6f586219e7",
            "4dce5494f1a544bfb236d816e815156c",
            "a6baa10cad3c4cf297400c2a6c26836c",
            "48bea4ef9e0b4ef7862cd2e3adedae03",
            "af6987c103ca420a906441fb26a6ae34",
            "7d826796faf64e0bae02f9f62cbdae0c",
            "b8dde7f29f7740c58477e18a3b9a3ca6",
            "3d265b0a6d7f46d98d37f5f00c9ac6e0",
            "3f6a1352f8dc4e6cb4d20628323e6d87"
          ]
        },
        "id": "vcEwHX6Rd6xi",
        "outputId": "f247b4a8-ccf6-42fe-fff9-fc100be48ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d272c4ca403b4a47b1ece3efc81b4c60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tf_model.h5:   0%|          | 0.00/502M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 95s 135ms/step - loss: 2.4495 - categorical_accuracy: 0.1297 - val_loss: 2.3345 - val_categorical_accuracy: 0.1406\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.2148 - categorical_accuracy: 0.1918 - val_loss: 2.2765 - val_categorical_accuracy: 0.1691\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.1319 - categorical_accuracy: 0.2072 - val_loss: 2.2452 - val_categorical_accuracy: 0.1765\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.0613 - categorical_accuracy: 0.2320 - val_loss: 2.2178 - val_categorical_accuracy: 0.1921\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 132ms/step - loss: 2.0293 - categorical_accuracy: 0.2409 - val_loss: 2.1922 - val_categorical_accuracy: 0.2022\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.9828 - categorical_accuracy: 0.2560 - val_loss: 2.1917 - val_categorical_accuracy: 0.2040\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.12      0.31      0.17       190\n",
            "         1.0       0.13      0.25      0.17       173\n",
            "         2.0       0.05      0.09      0.06       135\n",
            "         3.0       0.11      0.13      0.12       216\n",
            "         4.0       0.74      0.11      0.19      1036\n",
            "         5.0       0.10      0.39      0.15        76\n",
            "         6.0       0.68      0.70      0.69       195\n",
            "         7.0       0.08      0.03      0.04       290\n",
            "         8.0       0.10      0.21      0.13       139\n",
            "         9.0       0.24      0.31      0.27       258\n",
            "\n",
            "    accuracy                           0.20      2708\n",
            "   macro avg       0.23      0.25      0.20      2708\n",
            "weighted avg       0.40      0.20      0.20      2708\n",
            "\n",
            "0.25284298650124826\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 204s 309ms/step - loss: 1.9627 - categorical_accuracy: 0.2607 - val_loss: 2.1839 - val_categorical_accuracy: 0.1893\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.9341 - categorical_accuracy: 0.2688 - val_loss: 2.1393 - val_categorical_accuracy: 0.2123\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.9075 - categorical_accuracy: 0.2677 - val_loss: 2.1315 - val_categorical_accuracy: 0.2123\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 186s 306ms/step - loss: 1.8853 - categorical_accuracy: 0.2781 - val_loss: 2.1393 - val_categorical_accuracy: 0.2086\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 186s 306ms/step - loss: 1.8665 - categorical_accuracy: 0.2827 - val_loss: 2.1286 - val_categorical_accuracy: 0.2178\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 186s 305ms/step - loss: 1.8527 - categorical_accuracy: 0.2840 - val_loss: 2.1103 - val_categorical_accuracy: 0.2178\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.13      0.29      0.18       190\n",
            "         1.0       0.16      0.33      0.22       173\n",
            "         2.0       0.05      0.07      0.05       135\n",
            "         3.0       0.14      0.12      0.13       216\n",
            "         4.0       0.76      0.12      0.21      1036\n",
            "         5.0       0.11      0.53      0.19        76\n",
            "         6.0       0.71      0.77      0.74       195\n",
            "         7.0       0.09      0.04      0.05       290\n",
            "         8.0       0.12      0.24      0.16       139\n",
            "         9.0       0.24      0.40      0.30       258\n",
            "\n",
            "    accuracy                           0.23      2708\n",
            "   macro avg       0.25      0.29      0.22      2708\n",
            "weighted avg       0.42      0.23      0.22      2708\n",
            "\n",
            "0.29133796886832763\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seen_parameter = []#(drop_out_rate, initial_rate, fine_tune_learning_rate)"
      ],
      "metadata": {
        "id": "UWwkO5pxTANW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "McNemar={}\n",
        "balanced_accuracies_transfer_learning=[]\n",
        "balanced_accuracies_fine_tuning = []"
      ],
      "metadata": {
        "id": "V1ErLQ5mTANW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "#drop_out_rates = [0.1, 0.2, 0.3,0.4, 0.5]\n",
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "#for drop_out_rate in drop_out_rates:\n",
        "\n",
        "drop_out_rate = 0.4\n",
        "learning_rate_transfer_learnings = [1e-3, 1e-4]\n",
        "learning_rate_fine_tunings = [1e-5, 1e-6]\n",
        "\n",
        "for learning_rate_transfer_learning in learning_rate_transfer_learnings:\n",
        "  for learning_rate_fine_tuning in learning_rate_fine_tunings:\n",
        "    print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        "    , learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "    #step1\n",
        "    bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "    embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "    y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "    model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "    model1.layers[2].trainable = False\n",
        "    #model2.summary()\n",
        "\n",
        "    #step2\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "    #model2.summary() #Check trainable params increased.\n",
        "\n",
        "    #step3: transfer learning\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "    #step4: predict\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "    #step5: fine tune\n",
        "    print(\"Fine tuning---------------\")\n",
        "    model1.layers[2].trainable = True\n",
        "\n",
        "    # It's important to recompile your model after you make any changes\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "    print(\"----------------------------------------\")\n",
        "    del(model1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "494f3523b997428a8e07e2ff9f777278",
            "b7cc9f333c2b43c984854bf2a05c216b",
            "799f47064001473b9e2926d8b9f2a1c3",
            "60f03dcd1f494937924689fbabcb165c",
            "ab592251173e4fdeb9db0ac8037abf05",
            "069364167ee44547b1d841e3d488898c",
            "7c4eabafda57425980d8b661c5c4d4ef",
            "6bb505313711439c87d7ffe87c59b58b",
            "70a006728ee3429aa61eb3763bd2d601",
            "dd77628f26774bf4a5c03d5d92e6919b",
            "eb67aaba5d62417d94b683b2f8254d31"
          ]
        },
        "id": "uPIxk3ck0FPH",
        "outputId": "427dc0cf-3efd-444e-fa7d-4efca8bd7828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tf_model.h5:   0%|          | 0.00/502M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "494f3523b997428a8e07e2ff9f777278"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 94s 134ms/step - loss: 2.2317 - categorical_accuracy: 0.1972 - val_loss: 2.1380 - val_categorical_accuracy: 0.1912\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 79s 129ms/step - loss: 2.0176 - categorical_accuracy: 0.2512 - val_loss: 2.1589 - val_categorical_accuracy: 0.2086\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9597 - categorical_accuracy: 0.2723 - val_loss: 2.1580 - val_categorical_accuracy: 0.1930\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 79s 129ms/step - loss: 1.9069 - categorical_accuracy: 0.2855 - val_loss: 2.1173 - val_categorical_accuracy: 0.2491\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 79s 129ms/step - loss: 1.8727 - categorical_accuracy: 0.2930 - val_loss: 2.0645 - val_categorical_accuracy: 0.2408\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.8497 - categorical_accuracy: 0.3043 - val_loss: 2.0751 - val_categorical_accuracy: 0.2261\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.13      0.27      0.17       190\n",
            "         1.0       0.13      0.50      0.21       173\n",
            "         2.0       0.07      0.04      0.05       135\n",
            "         3.0       0.09      0.19      0.13       216\n",
            "         4.0       0.73      0.13      0.22      1036\n",
            "         5.0       0.13      0.42      0.20        76\n",
            "         6.0       0.75      0.71      0.73       195\n",
            "         7.0       0.12      0.02      0.04       290\n",
            "         8.0       0.12      0.04      0.06       139\n",
            "         9.0       0.21      0.32      0.25       258\n",
            "\n",
            "    accuracy                           0.22      2708\n",
            "   macro avg       0.25      0.26      0.20      2708\n",
            "weighted avg       0.40      0.22      0.21      2708\n",
            "\n",
            "0.26467282256848795\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 202s 305ms/step - loss: 1.8155 - categorical_accuracy: 0.2956 - val_loss: 2.0693 - val_categorical_accuracy: 0.2417\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 183s 301ms/step - loss: 1.6760 - categorical_accuracy: 0.3289 - val_loss: 2.0994 - val_categorical_accuracy: 0.2463\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 183s 301ms/step - loss: 1.5883 - categorical_accuracy: 0.3566 - val_loss: 1.9835 - val_categorical_accuracy: 0.2840\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 183s 301ms/step - loss: 1.5029 - categorical_accuracy: 0.3783 - val_loss: 1.9055 - val_categorical_accuracy: 0.2987\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 183s 301ms/step - loss: 1.4270 - categorical_accuracy: 0.4063 - val_loss: 1.9359 - val_categorical_accuracy: 0.3107\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 183s 301ms/step - loss: 1.3398 - categorical_accuracy: 0.4276 - val_loss: 1.8814 - val_categorical_accuracy: 0.3254\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.44      0.22       190\n",
            "         1.0       0.19      0.60      0.29       173\n",
            "         2.0       0.10      0.09      0.10       135\n",
            "         3.0       0.19      0.13      0.15       216\n",
            "         4.0       0.74      0.28      0.41      1036\n",
            "         5.0       0.23      0.39      0.29        76\n",
            "         6.0       0.84      0.77      0.81       195\n",
            "         7.0       0.20      0.03      0.06       290\n",
            "         8.0       0.16      0.05      0.08       139\n",
            "         9.0       0.26      0.53      0.35       258\n",
            "\n",
            "    accuracy                           0.31      2708\n",
            "   macro avg       0.31      0.33      0.27      2708\n",
            "weighted avg       0.45      0.31      0.32      2708\n",
            "\n",
            "0.33237379521894855\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 94s 135ms/step - loss: 2.2068 - categorical_accuracy: 0.2057 - val_loss: 2.2205 - val_categorical_accuracy: 0.1461\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.0150 - categorical_accuracy: 0.2399 - val_loss: 2.1568 - val_categorical_accuracy: 0.1930\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9432 - categorical_accuracy: 0.2668 - val_loss: 2.1433 - val_categorical_accuracy: 0.2123\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9005 - categorical_accuracy: 0.2740 - val_loss: 2.0847 - val_categorical_accuracy: 0.2132\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.8612 - categorical_accuracy: 0.2830 - val_loss: 2.1234 - val_categorical_accuracy: 0.2123\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.8466 - categorical_accuracy: 0.2885 - val_loss: 2.1262 - val_categorical_accuracy: 0.2096\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.11      0.18      0.13       190\n",
            "         1.0       0.13      0.53      0.20       173\n",
            "         2.0       0.05      0.06      0.05       135\n",
            "         3.0       0.13      0.20      0.16       216\n",
            "         4.0       0.75      0.14      0.24      1036\n",
            "         5.0       0.10      0.58      0.17        76\n",
            "         6.0       0.78      0.68      0.73       195\n",
            "         7.0       0.11      0.01      0.01       290\n",
            "         8.0       0.06      0.04      0.04       139\n",
            "         9.0       0.26      0.28      0.27       258\n",
            "\n",
            "    accuracy                           0.21      2708\n",
            "   macro avg       0.25      0.27      0.20      2708\n",
            "weighted avg       0.42      0.21      0.22      2708\n",
            "\n",
            "0.26917697605329705\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 202s 306ms/step - loss: 1.7734 - categorical_accuracy: 0.3100 - val_loss: 2.1188 - val_categorical_accuracy: 0.2142\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 183s 301ms/step - loss: 1.7511 - categorical_accuracy: 0.3136 - val_loss: 2.0763 - val_categorical_accuracy: 0.2371\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 183s 301ms/step - loss: 1.7396 - categorical_accuracy: 0.3172 - val_loss: 2.0898 - val_categorical_accuracy: 0.2233\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 183s 301ms/step - loss: 1.7041 - categorical_accuracy: 0.3252 - val_loss: 2.0878 - val_categorical_accuracy: 0.2353\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 183s 301ms/step - loss: 1.6946 - categorical_accuracy: 0.3273 - val_loss: 2.0737 - val_categorical_accuracy: 0.2261\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.11      0.21      0.15       190\n",
            "         1.0       0.15      0.58      0.24       173\n",
            "         2.0       0.05      0.05      0.05       135\n",
            "         3.0       0.12      0.15      0.13       216\n",
            "         4.0       0.77      0.16      0.26      1036\n",
            "         5.0       0.10      0.58      0.17        76\n",
            "         6.0       0.75      0.77      0.76       195\n",
            "         7.0       0.08      0.00      0.01       290\n",
            "         8.0       0.04      0.02      0.03       139\n",
            "         9.0       0.27      0.35      0.30       258\n",
            "\n",
            "    accuracy                           0.23      2708\n",
            "   macro avg       0.25      0.29      0.21      2708\n",
            "weighted avg       0.42      0.23      0.23      2708\n",
            "\n",
            "0.28817625318114837\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 94s 135ms/step - loss: 2.4721 - categorical_accuracy: 0.1269 - val_loss: 2.3431 - val_categorical_accuracy: 0.1369\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.2818 - categorical_accuracy: 0.1688 - val_loss: 2.2817 - val_categorical_accuracy: 0.1452\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.1810 - categorical_accuracy: 0.1978 - val_loss: 2.2541 - val_categorical_accuracy: 0.1535\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.1360 - categorical_accuracy: 0.2097 - val_loss: 2.2206 - val_categorical_accuracy: 0.1645\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.0921 - categorical_accuracy: 0.2238 - val_loss: 2.1995 - val_categorical_accuracy: 0.1728\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.0520 - categorical_accuracy: 0.2474 - val_loss: 2.1727 - val_categorical_accuracy: 0.1820\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.13      0.25      0.17       190\n",
            "         1.0       0.11      0.39      0.17       173\n",
            "         2.0       0.06      0.09      0.07       135\n",
            "         3.0       0.11      0.11      0.11       216\n",
            "         4.0       0.66      0.13      0.21      1036\n",
            "         5.0       0.09      0.62      0.16        76\n",
            "         6.0       0.67      0.76      0.71       195\n",
            "         7.0       0.09      0.02      0.03       290\n",
            "         8.0       0.07      0.05      0.06       139\n",
            "         9.0       0.21      0.18      0.19       258\n",
            "\n",
            "    accuracy                           0.20      2708\n",
            "   macro avg       0.22      0.26      0.19      2708\n",
            "weighted avg       0.37      0.20      0.20      2708\n",
            "\n",
            "0.25899548553031365\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 202s 305ms/step - loss: 1.9919 - categorical_accuracy: 0.2618 - val_loss: 1.9537 - val_categorical_accuracy: 0.2895\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 183s 301ms/step - loss: 1.8824 - categorical_accuracy: 0.2885 - val_loss: 2.0117 - val_categorical_accuracy: 0.2675\n",
            "Epoch 3/6\n",
            "392/609 [==================>...........] - ETA: 1:02 - loss: 1.7924 - categorical_accuracy: 0.3181"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resume from the last"
      ],
      "metadata": {
        "id": "2i6AHbv-xvWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "#drop_out_rates = [0.1, 0.2, 0.3,0.4, 0.5]\n",
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "#for drop_out_rate in drop_out_rates:\n",
        "\n",
        "drop_out_rate = 0.4\n",
        "learning_rate_transfer_learnings = [1e-4]\n",
        "learning_rate_fine_tunings = [1e-5, 1e-6]\n",
        "\n",
        "for learning_rate_transfer_learning in learning_rate_transfer_learnings:\n",
        "  for learning_rate_fine_tuning in learning_rate_fine_tunings:\n",
        "    print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        "    , learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "    #step1\n",
        "    bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "    embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "    y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "    model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "    model1.layers[2].trainable = False\n",
        "    #model2.summary()\n",
        "\n",
        "    #step2\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "    #model2.summary() #Check trainable params increased.\n",
        "\n",
        "    #step3: transfer learning\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "    #step4: predict\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "    #step5: fine tune\n",
        "    print(\"Fine tuning---------------\")\n",
        "    model1.layers[2].trainable = True\n",
        "\n",
        "    # It's important to recompile your model after you make any changes\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "    print(\"----------------------------------------\")\n",
        "    del(model1)\n"
      ],
      "metadata": {
        "id": "7SHRahRtxxRf",
        "outputId": "9e7e90f6-610b-4466-a483-56512f259544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "05ce66e4bd7045e5bfca4025886d219d",
            "909c9d4b575543879765d871847f2f6a",
            "7e7090853d2d444b8f011659f2147ac3",
            "fdf96e564ee4446caec4471bef691679",
            "30bbac742239451b9f17c52b98695ec6",
            "763420132a35401c8865005eb4c3eb09",
            "c3ae1df16fd9472696aedb4a64485167",
            "8329ae8f3efe4931b1ae3690e1127319",
            "7dda33ea712045baba20dff1f1f99512",
            "e2671f8891a04b6aa692330f01d994f7",
            "0ffd1a44c92544988c0a299b379ebbc8"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05ce66e4bd7045e5bfca4025886d219d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tf_model.h5:   0%|          | 0.00/502M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 96s 136ms/step - loss: 2.5474 - categorical_accuracy: 0.1922 - val_loss: 2.2609 - val_categorical_accuracy: 0.1507\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.2904 - categorical_accuracy: 0.1937 - val_loss: 2.2255 - val_categorical_accuracy: 0.1627\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.1950 - categorical_accuracy: 0.2088 - val_loss: 2.2120 - val_categorical_accuracy: 0.1756\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.1501 - categorical_accuracy: 0.2247 - val_loss: 2.1856 - val_categorical_accuracy: 0.1710\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.0989 - categorical_accuracy: 0.2344 - val_loss: 2.1620 - val_categorical_accuracy: 0.1838\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.0665 - categorical_accuracy: 0.2503 - val_loss: 2.1514 - val_categorical_accuracy: 0.1921\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.12      0.28      0.17       190\n",
            "         1.0       0.14      0.20      0.17       173\n",
            "         2.0       0.04      0.03      0.03       135\n",
            "         3.0       0.13      0.27      0.17       216\n",
            "         4.0       0.67      0.10      0.17      1036\n",
            "         5.0       0.11      0.29      0.16        76\n",
            "         6.0       0.68      0.72      0.70       195\n",
            "         7.0       0.12      0.10      0.11       290\n",
            "         8.0       0.09      0.20      0.13       139\n",
            "         9.0       0.17      0.25      0.20       258\n",
            "\n",
            "    accuracy                           0.20      2708\n",
            "   macro avg       0.23      0.24      0.20      2708\n",
            "weighted avg       0.37      0.20      0.20      2708\n",
            "\n",
            "0.24355273227399327\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "609/609 [==============================] - 205s 308ms/step - loss: 2.0187 - categorical_accuracy: 0.2574 - val_loss: 2.1205 - val_categorical_accuracy: 0.2096\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.9338 - categorical_accuracy: 0.2912 - val_loss: 2.0420 - val_categorical_accuracy: 0.2537\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.8401 - categorical_accuracy: 0.3116 - val_loss: 2.1483 - val_categorical_accuracy: 0.2233\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7726 - categorical_accuracy: 0.3328 - val_loss: 2.0875 - val_categorical_accuracy: 0.2482\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.6934 - categorical_accuracy: 0.3549 - val_loss: 2.0820 - val_categorical_accuracy: 0.2472\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.28      0.19       190\n",
            "         1.0       0.17      0.44      0.24       173\n",
            "         2.0       0.11      0.07      0.08       135\n",
            "         3.0       0.14      0.19      0.16       216\n",
            "         4.0       0.77      0.17      0.28      1036\n",
            "         5.0       0.15      0.49      0.23        76\n",
            "         6.0       0.65      0.86      0.74       195\n",
            "         7.0       0.10      0.05      0.07       290\n",
            "         8.0       0.13      0.18      0.15       139\n",
            "         9.0       0.25      0.41      0.31       258\n",
            "\n",
            "    accuracy                           0.26      2708\n",
            "   macro avg       0.26      0.31      0.25      2708\n",
            "weighted avg       0.42      0.26      0.26      2708\n",
            "\n",
            "0.3130289740927619\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 94s 135ms/step - loss: 2.4859 - categorical_accuracy: 0.1369 - val_loss: 2.2595 - val_categorical_accuracy: 0.1664\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.2612 - categorical_accuracy: 0.1911 - val_loss: 2.2091 - val_categorical_accuracy: 0.1783\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.1590 - categorical_accuracy: 0.2080 - val_loss: 2.1692 - val_categorical_accuracy: 0.1912\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.1282 - categorical_accuracy: 0.2221 - val_loss: 2.1616 - val_categorical_accuracy: 0.1875\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.0859 - categorical_accuracy: 0.2359 - val_loss: 2.1383 - val_categorical_accuracy: 0.1958\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.0407 - categorical_accuracy: 0.2432 - val_loss: 2.1390 - val_categorical_accuracy: 0.2068\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.08      0.05      0.06       190\n",
            "         1.0       0.13      0.43      0.20       173\n",
            "         2.0       0.04      0.13      0.06       135\n",
            "         3.0       0.12      0.16      0.14       216\n",
            "         4.0       0.64      0.10      0.18      1036\n",
            "         5.0       0.08      0.42      0.14        76\n",
            "         6.0       0.64      0.75      0.69       195\n",
            "         7.0       0.11      0.04      0.06       290\n",
            "         8.0       0.07      0.09      0.08       139\n",
            "         9.0       0.19      0.16      0.17       258\n",
            "\n",
            "    accuracy                           0.18      2708\n",
            "   macro avg       0.21      0.23      0.18      2708\n",
            "weighted avg       0.35      0.18      0.18      2708\n",
            "\n",
            "0.23448652349259164\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "609/609 [==============================] - 205s 308ms/step - loss: 2.0157 - categorical_accuracy: 0.2485 - val_loss: 2.1258 - val_categorical_accuracy: 0.2151\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 303ms/step - loss: 1.9896 - categorical_accuracy: 0.2572 - val_loss: 2.1042 - val_categorical_accuracy: 0.2096\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 303ms/step - loss: 1.9563 - categorical_accuracy: 0.2642 - val_loss: 2.0781 - val_categorical_accuracy: 0.2233\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 303ms/step - loss: 1.9456 - categorical_accuracy: 0.2640 - val_loss: 2.1164 - val_categorical_accuracy: 0.2224\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 303ms/step - loss: 1.9211 - categorical_accuracy: 0.2731 - val_loss: 2.0909 - val_categorical_accuracy: 0.2381\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 303ms/step - loss: 1.9165 - categorical_accuracy: 0.2787 - val_loss: 2.0890 - val_categorical_accuracy: 0.2353\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.13      0.06      0.08       190\n",
            "         1.0       0.15      0.53      0.23       173\n",
            "         2.0       0.04      0.12      0.06       135\n",
            "         3.0       0.14      0.14      0.14       216\n",
            "         4.0       0.70      0.16      0.26      1036\n",
            "         5.0       0.11      0.63      0.19        76\n",
            "         6.0       0.65      0.79      0.71       195\n",
            "         7.0       0.10      0.04      0.06       290\n",
            "         8.0       0.07      0.05      0.06       139\n",
            "         9.0       0.23      0.24      0.24       258\n",
            "\n",
            "    accuracy                           0.22      2708\n",
            "   macro avg       0.23      0.28      0.20      2708\n",
            "weighted avg       0.39      0.22      0.22      2708\n",
            "\n",
            "0.2776146034984809\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "#drop_out_rates = [0.1, 0.2, 0.3,0.4, 0.5]\n",
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "#for drop_out_rate in drop_out_rates:\n",
        "\n",
        "drop_out_rate = 0.5\n",
        "learning_rate_transfer_learnings = [1e-3, 1e-4]\n",
        "learning_rate_fine_tunings = [1e-5, 1e-6]\n",
        "\n",
        "for learning_rate_transfer_learning in learning_rate_transfer_learnings:\n",
        "  for learning_rate_fine_tuning in learning_rate_fine_tunings:\n",
        "    print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        "    , learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "    #step1\n",
        "    bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "    embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "    X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "    X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "    y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "    model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "    model1.layers[2].trainable = False\n",
        "    #model2.summary()\n",
        "\n",
        "    #step2\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "    #model2.summary() #Check trainable params increased.\n",
        "\n",
        "    #step3: transfer learning\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "    #step4: predict\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "    #step5: fine tune\n",
        "    print(\"Fine tuning---------------\")\n",
        "    model1.layers[2].trainable = True\n",
        "\n",
        "    # It's important to recompile your model after you make any changes\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "    metrics = []\n",
        "    metrics.append(\n",
        "        tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "    model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "    history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "    balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "    balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "    print(\"----------------------------------------\")\n",
        "    del(model1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccg_gvURl_qJ",
        "outputId": "b50b164a-5662-4c06-f141-c15f49b200be"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 94s 135ms/step - loss: 2.2785 - categorical_accuracy: 0.1874 - val_loss: 2.1913 - val_categorical_accuracy: 0.1756\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.0369 - categorical_accuracy: 0.2328 - val_loss: 2.1467 - val_categorical_accuracy: 0.2013\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9804 - categorical_accuracy: 0.2607 - val_loss: 2.0973 - val_categorical_accuracy: 0.2243\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9389 - categorical_accuracy: 0.2745 - val_loss: 2.0932 - val_categorical_accuracy: 0.2472\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9070 - categorical_accuracy: 0.2903 - val_loss: 2.0543 - val_categorical_accuracy: 0.2482\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 1.8789 - categorical_accuracy: 0.2850 - val_loss: 2.0647 - val_categorical_accuracy: 0.2270\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.13      0.28      0.18       190\n",
            "         1.0       0.13      0.58      0.21       173\n",
            "         2.0       0.05      0.03      0.04       135\n",
            "         3.0       0.11      0.12      0.11       216\n",
            "         4.0       0.72      0.18      0.29      1036\n",
            "         5.0       0.13      0.49      0.20        76\n",
            "         6.0       0.72      0.77      0.75       195\n",
            "         7.0       0.09      0.02      0.03       290\n",
            "         8.0       0.07      0.07      0.07       139\n",
            "         9.0       0.28      0.27      0.27       258\n",
            "\n",
            "    accuracy                           0.24      2708\n",
            "   macro avg       0.24      0.28      0.22      2708\n",
            "weighted avg       0.40      0.24      0.24      2708\n",
            "\n",
            "0.2813021595195978\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 205s 308ms/step - loss: 1.8338 - categorical_accuracy: 0.3029 - val_loss: 1.9680 - val_categorical_accuracy: 0.2619\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7193 - categorical_accuracy: 0.3211 - val_loss: 1.9922 - val_categorical_accuracy: 0.2656\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.6199 - categorical_accuracy: 0.3390 - val_loss: 1.9621 - val_categorical_accuracy: 0.2574\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.5442 - categorical_accuracy: 0.3506 - val_loss: 1.9326 - val_categorical_accuracy: 0.2739\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.4593 - categorical_accuracy: 0.3778 - val_loss: 1.9253 - val_categorical_accuracy: 0.3015\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.3994 - categorical_accuracy: 0.3975 - val_loss: 1.8392 - val_categorical_accuracy: 0.3235\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.16      0.42      0.23       190\n",
            "         1.0       0.19      0.56      0.28       173\n",
            "         2.0       0.10      0.09      0.09       135\n",
            "         3.0       0.18      0.14      0.16       216\n",
            "         4.0       0.72      0.32      0.44      1036\n",
            "         5.0       0.22      0.50      0.30        76\n",
            "         6.0       0.82      0.77      0.79       195\n",
            "         7.0       0.12      0.03      0.05       290\n",
            "         8.0       0.13      0.08      0.10       139\n",
            "         9.0       0.29      0.49      0.36       258\n",
            "\n",
            "    accuracy                           0.33      2708\n",
            "   macro avg       0.29      0.34      0.28      2708\n",
            "weighted avg       0.43      0.33      0.33      2708\n",
            "\n",
            "0.3395749899869377\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 95s 135ms/step - loss: 2.2802 - categorical_accuracy: 0.1978 - val_loss: 2.1871 - val_categorical_accuracy: 0.1710\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.0456 - categorical_accuracy: 0.2475 - val_loss: 2.1839 - val_categorical_accuracy: 0.1700\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9930 - categorical_accuracy: 0.2526 - val_loss: 2.1838 - val_categorical_accuracy: 0.1857\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9502 - categorical_accuracy: 0.2663 - val_loss: 2.1174 - val_categorical_accuracy: 0.1903\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9132 - categorical_accuracy: 0.2797 - val_loss: 2.0839 - val_categorical_accuracy: 0.2188\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9079 - categorical_accuracy: 0.2763 - val_loss: 2.1154 - val_categorical_accuracy: 0.2132\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.07      0.05      0.06       190\n",
            "         1.0       0.12      0.24      0.16       173\n",
            "         2.0       0.06      0.04      0.04       135\n",
            "         3.0       0.10      0.06      0.08       216\n",
            "         4.0       0.73      0.14      0.24      1036\n",
            "         5.0       0.10      0.33      0.15        76\n",
            "         6.0       0.71      0.69      0.70       195\n",
            "         7.0       0.00      0.00      0.00       290\n",
            "         8.0       0.08      0.59      0.14       139\n",
            "         9.0       0.26      0.33      0.29       258\n",
            "\n",
            "    accuracy                           0.20      2708\n",
            "   macro avg       0.22      0.25      0.19      2708\n",
            "weighted avg       0.39      0.20      0.20      2708\n",
            "\n",
            "0.24759405268985804\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 205s 308ms/step - loss: 1.8427 - categorical_accuracy: 0.2944 - val_loss: 2.0906 - val_categorical_accuracy: 0.2188\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 303ms/step - loss: 1.8244 - categorical_accuracy: 0.2971 - val_loss: 2.0959 - val_categorical_accuracy: 0.2132\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.8051 - categorical_accuracy: 0.3015 - val_loss: 2.0840 - val_categorical_accuracy: 0.2243\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7809 - categorical_accuracy: 0.3086 - val_loss: 2.0792 - val_categorical_accuracy: 0.2252\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7633 - categorical_accuracy: 0.3132 - val_loss: 2.0558 - val_categorical_accuracy: 0.2335\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7394 - categorical_accuracy: 0.3190 - val_loss: 2.0807 - val_categorical_accuracy: 0.2307\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.11      0.12      0.12       190\n",
            "         1.0       0.17      0.31      0.22       173\n",
            "         2.0       0.03      0.01      0.02       135\n",
            "         3.0       0.08      0.04      0.05       216\n",
            "         4.0       0.76      0.18      0.29      1036\n",
            "         5.0       0.11      0.63      0.19        76\n",
            "         6.0       0.72      0.77      0.74       195\n",
            "         7.0       0.08      0.00      0.01       290\n",
            "         8.0       0.09      0.50      0.15       139\n",
            "         9.0       0.27      0.38      0.32       258\n",
            "\n",
            "    accuracy                           0.23      2708\n",
            "   macro avg       0.24      0.29      0.21      2708\n",
            "weighted avg       0.41      0.23      0.23      2708\n",
            "\n",
            "0.2944448546129229\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 94s 135ms/step - loss: 2.5205 - categorical_accuracy: 0.1425 - val_loss: 2.3374 - val_categorical_accuracy: 0.1480\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.3057 - categorical_accuracy: 0.1819 - val_loss: 2.2816 - val_categorical_accuracy: 0.1664\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.2312 - categorical_accuracy: 0.2128 - val_loss: 2.2311 - val_categorical_accuracy: 0.1811\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.1599 - categorical_accuracy: 0.2236 - val_loss: 2.2317 - val_categorical_accuracy: 0.1866\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.1126 - categorical_accuracy: 0.2339 - val_loss: 2.2113 - val_categorical_accuracy: 0.1847\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.0773 - categorical_accuracy: 0.2416 - val_loss: 2.1986 - val_categorical_accuracy: 0.1884\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.06      0.09      0.08       190\n",
            "         1.0       0.12      0.33      0.18       173\n",
            "         2.0       0.10      0.03      0.05       135\n",
            "         3.0       0.13      0.22      0.16       216\n",
            "         4.0       0.70      0.16      0.26      1036\n",
            "         5.0       0.09      0.55      0.15        76\n",
            "         6.0       0.61      0.73      0.67       195\n",
            "         7.0       0.07      0.01      0.02       290\n",
            "         8.0       0.09      0.14      0.11       139\n",
            "         9.0       0.21      0.25      0.22       258\n",
            "\n",
            "    accuracy                           0.21      2708\n",
            "   macro avg       0.22      0.25      0.19      2708\n",
            "weighted avg       0.37      0.21      0.21      2708\n",
            "\n",
            "0.2522822893698141\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 204s 308ms/step - loss: 2.0316 - categorical_accuracy: 0.2578 - val_loss: 2.2454 - val_categorical_accuracy: 0.1811\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.9390 - categorical_accuracy: 0.2811 - val_loss: 1.9728 - val_categorical_accuracy: 0.2969\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.8722 - categorical_accuracy: 0.3072 - val_loss: 2.1670 - val_categorical_accuracy: 0.2289\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.8112 - categorical_accuracy: 0.3215 - val_loss: 2.1646 - val_categorical_accuracy: 0.2335\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.7418 - categorical_accuracy: 0.3276 - val_loss: 2.0594 - val_categorical_accuracy: 0.2757\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.19      0.16       190\n",
            "         1.0       0.19      0.50      0.28       173\n",
            "         2.0       0.17      0.02      0.04       135\n",
            "         3.0       0.16      0.32      0.21       216\n",
            "         4.0       0.72      0.28      0.40      1036\n",
            "         5.0       0.10      0.68      0.18        76\n",
            "         6.0       0.71      0.82      0.76       195\n",
            "         7.0       0.10      0.00      0.01       290\n",
            "         8.0       0.18      0.10      0.13       139\n",
            "         9.0       0.27      0.34      0.30       258\n",
            "\n",
            "    accuracy                           0.30      2708\n",
            "   macro avg       0.27      0.33      0.25      2708\n",
            "weighted avg       0.42      0.30      0.30      2708\n",
            "\n",
            "0.3266763703144977\n",
            "----------------------------------------\n",
            "learning_rate_transfer_learning:  0.0001 learning_rate_fine_tuning:  1e-06 drop_out_rate: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 94s 135ms/step - loss: 2.5813 - categorical_accuracy: 0.1196 - val_loss: 2.2888 - val_categorical_accuracy: 0.1287\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 79s 131ms/step - loss: 2.3399 - categorical_accuracy: 0.1607 - val_loss: 2.2221 - val_categorical_accuracy: 0.1535\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.2385 - categorical_accuracy: 0.1825 - val_loss: 2.1884 - val_categorical_accuracy: 0.1700\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.1680 - categorical_accuracy: 0.2026 - val_loss: 2.1762 - val_categorical_accuracy: 0.1765\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.1307 - categorical_accuracy: 0.2115 - val_loss: 2.1658 - val_categorical_accuracy: 0.1829\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 80s 131ms/step - loss: 2.1001 - categorical_accuracy: 0.2184 - val_loss: 2.1603 - val_categorical_accuracy: 0.1884\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.18      0.16       190\n",
            "         1.0       0.10      0.13      0.12       173\n",
            "         2.0       0.04      0.03      0.03       135\n",
            "         3.0       0.08      0.15      0.10       216\n",
            "         4.0       0.72      0.07      0.13      1036\n",
            "         5.0       0.10      0.42      0.16        76\n",
            "         6.0       0.64      0.68      0.66       195\n",
            "         7.0       0.07      0.01      0.02       290\n",
            "         8.0       0.10      0.38      0.16       139\n",
            "         9.0       0.18      0.35      0.24       258\n",
            "\n",
            "    accuracy                           0.18      2708\n",
            "   macro avg       0.22      0.24      0.18      2708\n",
            "weighted avg       0.38      0.18      0.16      2708\n",
            "\n",
            "0.2411930216346533\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 205s 308ms/step - loss: 2.0619 - categorical_accuracy: 0.2253 - val_loss: 2.1327 - val_categorical_accuracy: 0.1939\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.0486 - categorical_accuracy: 0.2334 - val_loss: 2.1031 - val_categorical_accuracy: 0.1967\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.0223 - categorical_accuracy: 0.2395 - val_loss: 2.1161 - val_categorical_accuracy: 0.1903\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 2.0064 - categorical_accuracy: 0.2426 - val_loss: 2.0986 - val_categorical_accuracy: 0.2013\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.9984 - categorical_accuracy: 0.2461 - val_loss: 2.1002 - val_categorical_accuracy: 0.1994\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 185s 304ms/step - loss: 1.9790 - categorical_accuracy: 0.2593 - val_loss: 2.0882 - val_categorical_accuracy: 0.2105\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.12      0.13      0.13       190\n",
            "         1.0       0.12      0.16      0.13       173\n",
            "         2.0       0.13      0.07      0.09       135\n",
            "         3.0       0.09      0.13      0.11       216\n",
            "         4.0       0.76      0.12      0.21      1036\n",
            "         5.0       0.11      0.51      0.18        76\n",
            "         6.0       0.65      0.79      0.71       195\n",
            "         7.0       0.02      0.00      0.01       290\n",
            "         8.0       0.11      0.41      0.17       139\n",
            "         9.0       0.18      0.39      0.25       258\n",
            "\n",
            "    accuracy                           0.21      2708\n",
            "   macro avg       0.23      0.27      0.20      2708\n",
            "weighted avg       0.39      0.21      0.20      2708\n",
            "\n",
            "0.27124955494885356\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "McNemar.keys()"
      ],
      "metadata": {
        "id": "76PuGBfmdQQc",
        "outputId": "884d4b07-754d-4fcf-89c0-c64d3fa61707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([(False, 0.4, 0.0001, 1e-05), (True, 0.4, 0.0001, 1e-05), (False, 0.4, 0.0001, 1e-06), (True, 0.4, 0.0001, 1e-06), (False, 0.5, 0.001, 1e-05), (True, 0.5, 0.001, 1e-05), (False, 0.5, 0.001, 1e-06), (True, 0.5, 0.001, 1e-06), (False, 0.5, 0.0001, 1e-05), (True, 0.5, 0.0001, 1e-05), (False, 0.5, 0.0001, 1e-06), (True, 0.5, 0.0001, 1e-06)])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_McNemar_BERT_split0_fine_tune = pd.DataFrame(data=McNemar[(True,0.5, 1e-3, 1e-5)])\n",
        "df_McNemar_BERT_split0_fine_tune.to_csv(DIR + 'McNemar_BERT_split0_fine_tune.csv')"
      ],
      "metadata": {
        "id": "7nr0UnKZLe1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_McNemar_BERT_split0_transfer_learning = pd.DataFrame(data=McNemar[(False,0.5, 1e-3, 1e-5)])\n",
        "df_McNemar_BERT_split0_transfer_learning.to_csv(DIR + 'McNemar_BERT_split0_transfer_learning.csv')"
      ],
      "metadata": {
        "id": "atoiZaZmdYuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_accuracies_fine_tuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzJC6ZwWom0p",
        "outputId": "aedb6da4-a92d-4eec-d7cd-26313df67bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3545180413326312,\n",
              " 0.32561995928565546,\n",
              " 0.3198687138125366,\n",
              " 0.2922621302487614]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_accuracies_fine_tuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSg4e4zOQhb6",
        "outputId": "0943eba2-a421-4cd5-f3f5-b90fd45bc57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3545180413326312,\n",
              " 0.32561995928565546,\n",
              " 0.3198687138125366,\n",
              " 0.2922621302487614,\n",
              " 0.35156416694413817,\n",
              " 0.3320729042652105,\n",
              " 0.3001390737921044]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t7bOgI9tlF9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best parameter is: drop out rate= 0.3, initial learn rate= 1e-3 , fine tune rate= 1e-5"
      ],
      "metadata": {
        "id": "-0ZyPSRTmH3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KFold test"
      ],
      "metadata": {
        "id": "M150mwbAM6j0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best parameter set is: initial learning rate=1e-3, fine tuning learning rate=1e-5, drop out rate=0.2 "
      ],
      "metadata": {
        "id": "YYykbCsOMvbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del(dataset_train)\n",
        "del(dataset_test)\n",
        "del(train)\n",
        "del(val)\n",
        "del(test)"
      ],
      "metadata": {
        "id": "nXI5kNu300m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fold1: intentionally separate cells for check point purpose."
      ],
      "metadata": {
        "id": "UL1HA0eCNFuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "split0=splits[1]\n",
        "split0['X_train'] = prepare_lyrics(split0['X_train'] )\n",
        "split0['X_test'] = prepare_lyrics(split0['X_test'] )\n",
        "SEQ_LEN=256\n",
        "print(split0['X_train'].shape, split0['X_test'].shape)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "Xids_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xmask_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xids_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "Xmask_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "\n",
        "for i, lyric in enumerate(split0['X_train']):\n",
        "  tokens = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\", add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_train[i,:], Xmask_train[i,:] = tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "for i, lyric in enumerate(split0['X_test']):\n",
        "  tokens_test = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\"\n",
        "    , add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_test[i,:], Xmask_test[i,:] = tokens_test['input_ids'], tokens_test['attention_mask']\n",
        "\n",
        "print(\"Xids_train.shape, Xids_test.shape: \",Xids_train.shape, Xids_test.shape)\n",
        "\n",
        "labels_train = np.zeros((split0['y_train'].shape[0], 10))\n",
        "labels_train[ np.arange(split0['y_train'].shape[0]), split0['y_train'].values] =1\n",
        "labels_test = np.zeros((split0['y_test'].shape[0], 10))\n",
        "labels_test[ np.arange(split0['y_test'].shape[0]), split0['y_test'].values] =1\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((Xids_train, Xmask_train, labels_train))\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((Xids_test, Xmask_test, labels_test))\n",
        "\n",
        "dataset_train = dataset_train.map(map_func)\n",
        "dataset_test = dataset_test.map(map_func)\n",
        "dataset_train = dataset_train.shuffle(42).batch(16)\n",
        "\n",
        "train = dataset_train.take(round(DS_LEN*SPLIT))\n",
        "val = dataset_train.skip(round(DS_LEN*SPLIT))\n",
        "test = dataset_test.batch(16)"
      ],
      "metadata": {
        "id": "LT8x-u6d00m3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf322259-cf49-44dc-942b-6a952714f80c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10832,) (2708,)\n",
            "Xids_train.shape, Xids_test.shape:  (10832, 256) (2708, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop_out_rate = 0.3\n",
        "learning_rate_transfer_learning =1e-3\n",
        "learning_rate_fine_tuning = 1e-5"
      ],
      "metadata": {
        "id": "zZFY4Qxs00m3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "McNemar={}"
      ],
      "metadata": {
        "id": "Jd3ZQYIIOK78"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "\n",
        "#balanced_accuracies_transfer_learning = []\n",
        "#balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "\n",
        "\n",
        "print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        ", learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "#step1\n",
        "bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "X = tf.keras.layers.BatchNormalization()(X)\n",
        "X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "model1.layers[2].trainable = False\n",
        "print(model1.summary())\n",
        "\n",
        "#step2\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "#model2.summary() #Check trainable params increased.\n",
        "\n",
        "#step3: transfer learning\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "#step4: predict\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "#step5: fine tune\n",
        "print(\"Fine tuning---------------\")\n",
        "model1.layers[2].trainable = True\n",
        "\n",
        "# It's important to recompile your model after you make any changes\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "print(\"----------------------------------------\")\n",
        "del(model1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b8bf1f2345234645926a17891a323b57",
            "cb69fdaa7f51452b82e992b42dc8f6f2",
            "3ce60140a2f54f6e8e94511f436a45e2",
            "1f506ec8d87142f9af36732e82cbfcac",
            "3c5d59492b1848b981924edf5e7377dc",
            "1167bd08c9fd4a64bb95ef503a0f280d",
            "490a9ea2be2b4021bff975cd43fc6078",
            "3cdbb72baaaa4fa2a82061e751fe039f",
            "d54f090c4c34495b9084d4d468de4a5d",
            "f78b3337fd884d68b282b84e919a7afa",
            "917c4c2363d84bd6b5b8da904bcebcb4"
          ]
        },
        "id": "zqoBAnlt00m3",
        "outputId": "67193c6f-673e-4e09-a9ce-8f46d01c6f22"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tf_model.h5:   0%|          | 0.00/502M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8bf1f2345234645926a17891a323b57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          98432       ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 32)           4128        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,234\n",
            "Trainable params: 104,426\n",
            "Non-trainable params: 108,311,808\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 159s 234ms/step - loss: 2.2033 - categorical_accuracy: 0.2197 - val_loss: 2.1678 - val_categorical_accuracy: 0.1967\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 139s 228ms/step - loss: 1.9994 - categorical_accuracy: 0.2627 - val_loss: 2.1333 - val_categorical_accuracy: 0.2013\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 139s 228ms/step - loss: 1.9214 - categorical_accuracy: 0.2903 - val_loss: 2.1161 - val_categorical_accuracy: 0.2132\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 138s 227ms/step - loss: 1.8932 - categorical_accuracy: 0.2896 - val_loss: 2.0872 - val_categorical_accuracy: 0.2142\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 139s 227ms/step - loss: 1.8505 - categorical_accuracy: 0.2955 - val_loss: 2.0343 - val_categorical_accuracy: 0.2243\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 138s 227ms/step - loss: 1.8281 - categorical_accuracy: 0.3028 - val_loss: 2.0654 - val_categorical_accuracy: 0.2096\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.38      0.21       191\n",
            "         1.0       0.16      0.48      0.24       173\n",
            "         2.0       0.06      0.03      0.04       134\n",
            "         3.0       0.11      0.19      0.14       216\n",
            "         4.0       0.70      0.14      0.23      1036\n",
            "         5.0       0.11      0.50      0.18        76\n",
            "         6.0       0.84      0.72      0.77       196\n",
            "         7.0       0.10      0.03      0.05       289\n",
            "         8.0       0.06      0.04      0.05       139\n",
            "         9.0       0.23      0.31      0.26       258\n",
            "\n",
            "    accuracy                           0.23      2708\n",
            "   macro avg       0.25      0.28      0.22      2708\n",
            "weighted avg       0.40      0.23      0.22      2708\n",
            "\n",
            "0.2820752636173121\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 354s 548ms/step - loss: 1.7822 - categorical_accuracy: 0.3125 - val_loss: 2.0159 - val_categorical_accuracy: 0.2426\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 330s 543ms/step - loss: 1.6509 - categorical_accuracy: 0.3402 - val_loss: 1.9121 - val_categorical_accuracy: 0.2785\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 331s 543ms/step - loss: 1.5396 - categorical_accuracy: 0.3649 - val_loss: 1.9414 - val_categorical_accuracy: 0.2675\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 331s 543ms/step - loss: 1.4338 - categorical_accuracy: 0.4022 - val_loss: 1.9224 - val_categorical_accuracy: 0.3051\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 331s 543ms/step - loss: 1.3592 - categorical_accuracy: 0.4243 - val_loss: 1.9451 - val_categorical_accuracy: 0.2610\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 331s 543ms/step - loss: 1.2797 - categorical_accuracy: 0.4475 - val_loss: 1.8610 - val_categorical_accuracy: 0.3364\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.19      0.44      0.26       191\n",
            "         1.0       0.21      0.45      0.28       173\n",
            "         2.0       0.11      0.06      0.08       134\n",
            "         3.0       0.20      0.20      0.20       216\n",
            "         4.0       0.71      0.29      0.41      1036\n",
            "         5.0       0.14      0.54      0.22        76\n",
            "         6.0       0.66      0.90      0.76       196\n",
            "         7.0       0.13      0.04      0.06       289\n",
            "         8.0       0.16      0.09      0.11       139\n",
            "         9.0       0.27      0.48      0.34       258\n",
            "\n",
            "    accuracy                           0.32      2708\n",
            "   macro avg       0.28      0.35      0.27      2708\n",
            "weighted avg       0.42      0.32      0.32      2708\n",
            "\n",
            "0.3480776420033124\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fold2: "
      ],
      "metadata": {
        "id": "qWLHwgPUO8eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "split0=splits[2]\n",
        "split0['X_train'] = prepare_lyrics(split0['X_train'] )\n",
        "split0['X_test'] = prepare_lyrics(split0['X_test'] )\n",
        "SEQ_LEN=256\n",
        "print(split0['X_train'].shape, split0['X_test'].shape)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "Xids_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xmask_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xids_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "Xmask_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "\n",
        "for i, lyric in enumerate(split0['X_train']):\n",
        "  tokens = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\", add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_train[i,:], Xmask_train[i,:] = tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "for i, lyric in enumerate(split0['X_test']):\n",
        "  tokens_test = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\"\n",
        "    , add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_test[i,:], Xmask_test[i,:] = tokens_test['input_ids'], tokens_test['attention_mask']\n",
        "\n",
        "print(\"Xids_train.shape, Xids_test.shape: \",Xids_train.shape, Xids_test.shape)\n",
        "\n",
        "labels_train = np.zeros((split0['y_train'].shape[0], 10))\n",
        "labels_train[ np.arange(split0['y_train'].shape[0]), split0['y_train'].values] =1\n",
        "labels_test = np.zeros((split0['y_test'].shape[0], 10))\n",
        "labels_test[ np.arange(split0['y_test'].shape[0]), split0['y_test'].values] =1\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((Xids_train, Xmask_train, labels_train))\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((Xids_test, Xmask_test, labels_test))\n",
        "\n",
        "dataset_train = dataset_train.map(map_func)\n",
        "dataset_test = dataset_test.map(map_func)\n",
        "dataset_train = dataset_train.shuffle(42).batch(16)\n",
        "\n",
        "train = dataset_train.take(round(DS_LEN*SPLIT))\n",
        "val = dataset_train.skip(round(DS_LEN*SPLIT))\n",
        "test = dataset_test.batch(16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a6b7b6-ad20-4335-c88a-72d64432b78f",
        "id": "d5c_5sLxO8eY"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10832,) (2708,)\n",
            "Xids_train.shape, Xids_test.shape:  (10832, 256) (2708, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop_out_rate = 0.3\n",
        "learning_rate_transfer_learning =1e-3\n",
        "learning_rate_fine_tuning = 1e-5"
      ],
      "metadata": {
        "id": "TrvcQkbpO8eY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "\n",
        "#balanced_accuracies_transfer_learning = []\n",
        "#balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "\n",
        "\n",
        "print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        ", learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "#step1\n",
        "bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "X = tf.keras.layers.BatchNormalization()(X)\n",
        "X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "model1.layers[2].trainable = False\n",
        "print(model1.summary())\n",
        "\n",
        "#step2\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "#model2.summary() #Check trainable params increased.\n",
        "\n",
        "#step3: transfer learning\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "#step4: predict\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "#step5: fine tune\n",
        "print(\"Fine tuning---------------\")\n",
        "model1.layers[2].trainable = True\n",
        "\n",
        "# It's important to recompile your model after you make any changes\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "print(\"----------------------------------------\")\n",
        "del(model1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98eda6a-a595-4671-a52e-4880ed719e9e",
        "id": "BS9lQpP9O8eZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 768)         3072        ['global_max_pooling1d_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 128)          98432       ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 128)         512         ['dense_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_75 (Dropout)           (None, 128)          0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 32)           4128        ['dropout_75[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32)          128         ['dense_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_76 (Dropout)           (None, 32)           0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_76[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 158s 234ms/step - loss: 2.3873 - categorical_accuracy: 0.1996 - val_loss: 2.2250 - val_categorical_accuracy: 0.2077\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 139s 228ms/step - loss: 2.0747 - categorical_accuracy: 0.2461 - val_loss: 2.1866 - val_categorical_accuracy: 0.1958\n",
            "Epoch 3/6\n",
            "347/609 [================>.............] - ETA: 54s - loss: 1.9789 - categorical_accuracy: 0.2718"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fold3: intentionally separate cells for check point purpose."
      ],
      "metadata": {
        "id": "ivGPMia1TAUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_out_rate = 0.3\n",
        "learning_rate_transfer_learning =1e-3\n",
        "learning_rate_fine_tuning = 1e-5"
      ],
      "metadata": {
        "id": "WHomEMx1A1jw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "McNemar={}"
      ],
      "metadata": {
        "id": "A0OxrdLLA1jw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "split0=splits[3]\n",
        "split0['X_train'] = prepare_lyrics(split0['X_train'] )\n",
        "split0['X_test'] = prepare_lyrics(split0['X_test'] )\n",
        "SEQ_LEN=256\n",
        "print(split0['X_train'].shape, split0['X_test'].shape)\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained('roberta-base')bert-base-cased\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "Xids_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xmask_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xids_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "Xmask_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "\n",
        "for i, lyric in enumerate(split0['X_train']):\n",
        "  tokens = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\", add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_train[i,:], Xmask_train[i,:] = tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "for i, lyric in enumerate(split0['X_test']):\n",
        "  tokens_test = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\"\n",
        "    , add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_test[i,:], Xmask_test[i,:] = tokens_test['input_ids'], tokens_test['attention_mask']\n",
        "\n",
        "print(\"Xids_train.shape, Xids_test.shape: \",Xids_train.shape, Xids_test.shape)\n",
        "\n",
        "labels_train = np.zeros((split0['y_train'].shape[0], 10))\n",
        "labels_train[ np.arange(split0['y_train'].shape[0]), split0['y_train'].values] =1\n",
        "labels_test = np.zeros((split0['y_test'].shape[0], 10))\n",
        "labels_test[ np.arange(split0['y_test'].shape[0]), split0['y_test'].values] =1\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((Xids_train, Xmask_train, labels_train))\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((Xids_test, Xmask_test, labels_test))\n",
        "\n",
        "dataset_train = dataset_train.map(map_func)\n",
        "dataset_test = dataset_test.map(map_func)\n",
        "dataset_train = dataset_train.shuffle(42).batch(16)\n",
        "\n",
        "train = dataset_train.take(round(DS_LEN*SPLIT))\n",
        "val = dataset_train.skip(round(DS_LEN*SPLIT))\n",
        "test = dataset_test.batch(16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a429e80-91ba-4fcd-d3b2-b8e67eb79b7b",
        "id": "RZ9AHi3gTAUv"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10832,) (2708,)\n",
            "Xids_train.shape, Xids_test.shape:  (10832, 256) (2708, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "\n",
        "#balanced_accuracies_transfer_learning = []\n",
        "#balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "\n",
        "\n",
        "print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        ", learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "#step1\n",
        "bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "X = tf.keras.layers.BatchNormalization()(X)\n",
        "X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "model1.layers[2].trainable = False\n",
        "print(model1.summary())\n",
        "\n",
        "#step2\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "#model2.summary() #Check trainable params increased.\n",
        "\n",
        "#step3: transfer learning\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "#step4: predict\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "#step5: fine tune\n",
        "print(\"Fine tuning---------------\")\n",
        "model1.layers[2].trainable = True\n",
        "\n",
        "# It's important to recompile your model after you make any changes\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "print(\"----------------------------------------\")\n",
        "del(model1)\n"
      ],
      "metadata": {
        "id": "LO0ech7zTAUw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "38ca8c3bfdbd4f068cd0a0f071d5aafe",
            "c5dbf5a5e0ab41f591856404d580bfdb",
            "ed5a91cdf95247d097e0ef4a217e20ac",
            "b67897bb04f247a6bb13872e70ef36bb",
            "b8dbb65b5543426097b2e68dc5dd8f25",
            "f3c4f29027c641aa986ec6cacdcc2f99",
            "7447a858a8f348c4accaffe9d4117b8c",
            "630395be415f489ea019cb02992c8559",
            "cf40f129430941d7bfcb223da1727b2e",
            "b827dfe746444047a320a2512001eff6",
            "d8cfad220b604bd1bd9b86f85d0ec571"
          ]
        },
        "outputId": "b72ec24f-9fd9-42a4-9900-f1d532671fd4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tf_model.h5:   0%|          | 0.00/502M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38ca8c3bfdbd4f068cd0a0f071d5aafe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          98432       ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 32)           4128        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,234\n",
            "Trainable params: 104,426\n",
            "Non-trainable params: 108,311,808\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 95s 135ms/step - loss: 2.2137 - categorical_accuracy: 0.2176 - val_loss: 2.1461 - val_categorical_accuracy: 0.1664\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 2.0224 - categorical_accuracy: 0.2636 - val_loss: 2.1541 - val_categorical_accuracy: 0.1599\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.9377 - categorical_accuracy: 0.2695 - val_loss: 2.1142 - val_categorical_accuracy: 0.2059\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.8817 - categorical_accuracy: 0.2749 - val_loss: 2.1438 - val_categorical_accuracy: 0.1728\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 79s 130ms/step - loss: 1.8503 - categorical_accuracy: 0.2808 - val_loss: 2.0705 - val_categorical_accuracy: 0.1838\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 79s 129ms/step - loss: 1.8095 - categorical_accuracy: 0.2946 - val_loss: 2.1171 - val_categorical_accuracy: 0.1949\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.12      0.20      0.15       191\n",
            "         1.0       0.17      0.18      0.17       172\n",
            "         2.0       0.05      0.19      0.09       134\n",
            "         3.0       0.14      0.14      0.14       216\n",
            "         4.0       0.76      0.09      0.16      1036\n",
            "         5.0       0.12      0.35      0.18        77\n",
            "         6.0       0.85      0.76      0.80       196\n",
            "         7.0       0.14      0.02      0.04       290\n",
            "         8.0       0.10      0.39      0.15       139\n",
            "         9.0       0.26      0.37      0.30       257\n",
            "\n",
            "    accuracy                           0.20      2708\n",
            "   macro avg       0.27      0.27      0.22      2708\n",
            "weighted avg       0.43      0.20      0.20      2708\n",
            "\n",
            "0.27012037475807266\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 203s 308ms/step - loss: 1.7695 - categorical_accuracy: 0.2835 - val_loss: 2.1862 - val_categorical_accuracy: 0.2050\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 184s 303ms/step - loss: 1.6028 - categorical_accuracy: 0.3236 - val_loss: 2.0537 - val_categorical_accuracy: 0.2335\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 184s 303ms/step - loss: 1.4946 - categorical_accuracy: 0.3557 - val_loss: 2.0228 - val_categorical_accuracy: 0.2509\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 184s 302ms/step - loss: 1.3826 - categorical_accuracy: 0.3920 - val_loss: 1.9838 - val_categorical_accuracy: 0.2601\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 184s 303ms/step - loss: 1.3038 - categorical_accuracy: 0.4153 - val_loss: 1.9181 - val_categorical_accuracy: 0.3097\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 184s 302ms/step - loss: 1.1962 - categorical_accuracy: 0.4612 - val_loss: 1.8710 - val_categorical_accuracy: 0.3143\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.59      0.23       191\n",
            "         1.0       0.25      0.23      0.24       172\n",
            "         2.0       0.14      0.22      0.17       134\n",
            "         3.0       0.16      0.11      0.13       216\n",
            "         4.0       0.70      0.27      0.39      1036\n",
            "         5.0       0.15      0.44      0.23        77\n",
            "         6.0       0.86      0.84      0.85       196\n",
            "         7.0       0.17      0.04      0.06       290\n",
            "         8.0       0.16      0.13      0.14       139\n",
            "         9.0       0.29      0.44      0.35       257\n",
            "\n",
            "    accuracy                           0.30      2708\n",
            "   macro avg       0.30      0.33      0.28      2708\n",
            "weighted avg       0.44      0.30      0.31      2708\n",
            "\n",
            "0.33132053264155187\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fold4: intentionally separate cells for check point purpose."
      ],
      "metadata": {
        "id": "6K_0Ui6OTMAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "split0=splits[4]\n",
        "split0['X_train'] = prepare_lyrics(split0['X_train'] )\n",
        "split0['X_test'] = prepare_lyrics(split0['X_test'] )\n",
        "SEQ_LEN=256\n",
        "print(split0['X_train'].shape, split0['X_test'].shape)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "Xids_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xmask_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xids_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "Xmask_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "\n",
        "for i, lyric in enumerate(split0['X_train']):\n",
        "  tokens = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\", add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_train[i,:], Xmask_train[i,:] = tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "for i, lyric in enumerate(split0['X_test']):\n",
        "  tokens_test = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\"\n",
        "    , add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_test[i,:], Xmask_test[i,:] = tokens_test['input_ids'], tokens_test['attention_mask']\n",
        "\n",
        "print(\"Xids_train.shape, Xids_test.shape: \",Xids_train.shape, Xids_test.shape)\n",
        "\n",
        "labels_train = np.zeros((split0['y_train'].shape[0], 10))\n",
        "labels_train[ np.arange(split0['y_train'].shape[0]), split0['y_train'].values] =1\n",
        "labels_test = np.zeros((split0['y_test'].shape[0], 10))\n",
        "labels_test[ np.arange(split0['y_test'].shape[0]), split0['y_test'].values] =1\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((Xids_train, Xmask_train, labels_train))\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((Xids_test, Xmask_test, labels_test))\n",
        "\n",
        "dataset_train = dataset_train.map(map_func)\n",
        "dataset_test = dataset_test.map(map_func)\n",
        "dataset_train = dataset_train.shuffle(42).batch(16)\n",
        "\n",
        "train = dataset_train.take(round(DS_LEN*SPLIT))\n",
        "val = dataset_train.skip(round(DS_LEN*SPLIT))\n",
        "test = dataset_test.batch(16)"
      ],
      "metadata": {
        "id": "JkyzsU7ITMAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458a617e-5565-4e28-90ad-5002ccb3efab"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10832,) (2708,)\n",
            "Xids_train.shape, Xids_test.shape:  (10832, 256) (2708, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "\n",
        "#balanced_accuracies_transfer_learning = []\n",
        "#balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "\n",
        "\n",
        "print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        ", learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "#step1\n",
        "bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "X = tf.keras.layers.BatchNormalization()(X)\n",
        "X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "model1.layers[2].trainable = False\n",
        "print(model1.summary())\n",
        "\n",
        "#step2\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "#model2.summary() #Check trainable params increased.\n",
        "\n",
        "#step3: transfer learning\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "#step4: predict\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "#step5: fine tune\n",
        "print(\"Fine tuning---------------\")\n",
        "model1.layers[2].trainable = True\n",
        "\n",
        "# It's important to recompile your model after you make any changes\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "print(\"----------------------------------------\")\n",
        "del(model1)\n"
      ],
      "metadata": {
        "id": "PgCaNNP6TMAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be0284b-f7cd-4c19-af8b-a5757bbab85d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 768)         3072        ['global_max_pooling1d_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 128)          98432       ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_75 (Dropout)           (None, 128)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 32)           4128        ['dropout_75[0][0]']             \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,234\n",
            "Trainable params: 104,426\n",
            "Non-trainable params: 108,311,808\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 93s 134ms/step - loss: 2.2048 - categorical_accuracy: 0.2096 - val_loss: 2.1872 - val_categorical_accuracy: 0.1976\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 79s 129ms/step - loss: 2.0060 - categorical_accuracy: 0.2610 - val_loss: 2.1279 - val_categorical_accuracy: 0.2031\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 79s 129ms/step - loss: 1.9357 - categorical_accuracy: 0.2720 - val_loss: 2.1136 - val_categorical_accuracy: 0.2132\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 79s 129ms/step - loss: 1.8910 - categorical_accuracy: 0.2778 - val_loss: 2.1101 - val_categorical_accuracy: 0.2142\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 79s 129ms/step - loss: 1.8513 - categorical_accuracy: 0.2914 - val_loss: 2.0961 - val_categorical_accuracy: 0.2059\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 79s 129ms/step - loss: 1.8141 - categorical_accuracy: 0.3077 - val_loss: 2.0792 - val_categorical_accuracy: 0.2344\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.18      0.16      0.17       190\n",
            "         1.0       0.18      0.24      0.21       173\n",
            "         2.0       0.08      0.31      0.13       135\n",
            "         3.0       0.14      0.25      0.18       217\n",
            "         4.0       0.66      0.18      0.29      1035\n",
            "         5.0       0.13      0.61      0.21        76\n",
            "         6.0       0.79      0.74      0.76       195\n",
            "         7.0       0.12      0.04      0.06       290\n",
            "         8.0       0.09      0.15      0.11       139\n",
            "         9.0       0.25      0.25      0.25       258\n",
            "\n",
            "    accuracy                           0.24      2708\n",
            "   macro avg       0.26      0.29      0.24      2708\n",
            "weighted avg       0.39      0.24      0.25      2708\n",
            "\n",
            "0.29267378364825314\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 202s 306ms/step - loss: 1.7621 - categorical_accuracy: 0.3111 - val_loss: 2.1942 - val_categorical_accuracy: 0.2031\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 184s 302ms/step - loss: 1.6183 - categorical_accuracy: 0.3514 - val_loss: 2.0047 - val_categorical_accuracy: 0.2684\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 184s 302ms/step - loss: 1.5139 - categorical_accuracy: 0.3808 - val_loss: 1.9747 - val_categorical_accuracy: 0.2739\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 184s 302ms/step - loss: 1.4029 - categorical_accuracy: 0.4114 - val_loss: 1.9859 - val_categorical_accuracy: 0.2610\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 184s 302ms/step - loss: 1.3195 - categorical_accuracy: 0.4396 - val_loss: 1.9352 - val_categorical_accuracy: 0.2932\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 184s 302ms/step - loss: 1.2276 - categorical_accuracy: 0.4723 - val_loss: 1.9448 - val_categorical_accuracy: 0.3070\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.16      0.44      0.23       190\n",
            "         1.0       0.32      0.31      0.31       173\n",
            "         2.0       0.10      0.25      0.14       135\n",
            "         3.0       0.17      0.32      0.22       217\n",
            "         4.0       0.70      0.26      0.37      1035\n",
            "         5.0       0.17      0.59      0.27        76\n",
            "         6.0       0.88      0.75      0.81       195\n",
            "         7.0       0.27      0.06      0.10       290\n",
            "         8.0       0.12      0.09      0.11       139\n",
            "         9.0       0.33      0.35      0.34       258\n",
            "\n",
            "    accuracy                           0.30      2708\n",
            "   macro avg       0.32      0.34      0.29      2708\n",
            "weighted avg       0.45      0.30      0.32      2708\n",
            "\n",
            "0.3424170775321077\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Take average"
      ],
      "metadata": {
        "id": "pe8-xF5cTaOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_accuracies_fine_tuning"
      ],
      "metadata": {
        "id": "G_d4-AXfTbbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47cdff0a-a0cf-4798-e617-56453655b22b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33132053264155187, 0.3424170775321077]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_accuracy_fine_tune.append(0.3560173405159383)"
      ],
      "metadata": {
        "id": "VigXG8PeTctM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(balanced_accuracy_fine_tune)/5"
      ],
      "metadata": {
        "id": "cyLDOdzQTuIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conduct McNemar testing with the best parameter"
      ],
      "metadata": {
        "id": "YlyEV7MauGPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_out_rate = 0.2\n",
        "learning_rate_transfer_learning = 1e-3\n",
        "learning_rate_fine_tuning = 1e-5\n",
        "McNemar = {}# (is_fine_tuning?, dropout_rate, learning_rate_transfer_learning, learning_rate_fine_tuning)"
      ],
      "metadata": {
        "id": "fPOHXTZ5uJtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "balanced_accuracies_transfer_learning = []\n",
        "balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=256\n",
        "\n",
        "drop_out_rate = 0.2\n",
        "learning_rate_transfer_learning = 1e-3\n",
        "learning_rate_fine_tuning = 1e-5\n",
        "\n",
        "\n",
        "print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        ", learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "#step1\n",
        "roberta = TFAutoModel.from_pretrained('roberta-base')\n",
        "input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "embeddings = roberta(input_ids, attention_mask= mask)[0]\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "X = tf.keras.layers.BatchNormalization()(X)\n",
        "X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "model2 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "model2.layers[2].trainable = False\n",
        "#model2.summary()\n",
        "\n",
        "#step2\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model2.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "#model2.summary() #Check trainable params increased.\n",
        "\n",
        "#step3: transfer learning\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "history = model2.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "#step4: predict\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model2, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "#step5: fine tune\n",
        "print(\"Fine tuning---------------\")\n",
        "model2.layers[2].trainable = True\n",
        "\n",
        "# It's important to recompile your model after you make any changes\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model2.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "history = model2.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model2, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "print(\"----------------------------------------\")\n",
        "del(model2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6crn297uKT5",
        "outputId": "b43e454b-24e1-46b7-e20d-aaee587f697a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "609/609 [==============================] - 158s 234ms/step - loss: 2.1384 - categorical_accuracy: 0.2122 - val_loss: 2.8267 - val_categorical_accuracy: 0.0901\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 138s 227ms/step - loss: 1.9793 - categorical_accuracy: 0.2636 - val_loss: 2.3489 - val_categorical_accuracy: 0.1480\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 138s 227ms/step - loss: 1.9232 - categorical_accuracy: 0.2825 - val_loss: 2.4198 - val_categorical_accuracy: 0.1379\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 138s 227ms/step - loss: 1.8901 - categorical_accuracy: 0.2974 - val_loss: 2.3096 - val_categorical_accuracy: 0.1581\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 138s 227ms/step - loss: 1.8674 - categorical_accuracy: 0.2979 - val_loss: 2.2364 - val_categorical_accuracy: 0.1801\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 138s 227ms/step - loss: 1.8519 - categorical_accuracy: 0.3018 - val_loss: 2.2021 - val_categorical_accuracy: 0.1691\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.08      0.07      0.07       190\n",
            "         1.0       0.13      0.46      0.20       173\n",
            "         2.0       0.06      0.45      0.11       135\n",
            "         3.0       0.15      0.25      0.19       216\n",
            "         4.0       0.73      0.05      0.10      1036\n",
            "         5.0       0.13      0.16      0.14        76\n",
            "         6.0       0.76      0.53      0.62       195\n",
            "         7.0       0.12      0.02      0.03       290\n",
            "         8.0       0.00      0.00      0.00       139\n",
            "         9.0       0.27      0.18      0.21       258\n",
            "\n",
            "    accuracy                           0.16      2708\n",
            "   macro avg       0.24      0.22      0.17      2708\n",
            "weighted avg       0.41      0.16      0.15      2708\n",
            "\n",
            "0.21715377119415202\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "609/609 [==============================] - 357s 553ms/step - loss: 1.7825 - categorical_accuracy: 0.3348 - val_loss: 1.8293 - val_categorical_accuracy: 0.2960\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 333s 547ms/step - loss: 1.6608 - categorical_accuracy: 0.3509 - val_loss: 1.8948 - val_categorical_accuracy: 0.2923\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 333s 547ms/step - loss: 1.5689 - categorical_accuracy: 0.3829 - val_loss: 2.0517 - val_categorical_accuracy: 0.2491\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 333s 547ms/step - loss: 1.4769 - categorical_accuracy: 0.4039 - val_loss: 1.9109 - val_categorical_accuracy: 0.2978\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 333s 547ms/step - loss: 1.3818 - categorical_accuracy: 0.4348 - val_loss: 1.7944 - val_categorical_accuracy: 0.3529\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 333s 547ms/step - loss: 1.2942 - categorical_accuracy: 0.4637 - val_loss: 1.7972 - val_categorical_accuracy: 0.3438\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.28      0.28      0.28       190\n",
            "         1.0       0.20      0.61      0.30       173\n",
            "         2.0       0.08      0.12      0.10       135\n",
            "         3.0       0.28      0.11      0.15       216\n",
            "         4.0       0.68      0.35      0.46      1036\n",
            "         5.0       0.31      0.34      0.32        76\n",
            "         6.0       0.55      0.90      0.69       195\n",
            "         7.0       0.10      0.01      0.01       290\n",
            "         8.0       0.18      0.14      0.16       139\n",
            "         9.0       0.25      0.63      0.36       258\n",
            "\n",
            "    accuracy                           0.35      2708\n",
            "   macro avg       0.29      0.35      0.28      2708\n",
            "weighted avg       0.41      0.35      0.33      2708\n",
            "\n",
            "0.34869950762677604\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "McNemar_RoBERTa_split0_transfer_learning = pd.DataFrame(data= McNemar[(False,0.2, 1e-3,1e-5)] )\n",
        "McNemar_RoBERTa_split0_transfer_learning.to_csv(DIR + 'McNemar_RoBERTa_split0_transfer_learning.csv')\n",
        "\n",
        "McNemar_RoBERTa_split0_fine_tuning = pd.DataFrame(data= McNemar[(True, 0.2, 1e-3,1e-5)] )\n",
        "McNemar_RoBERTa_split0_fine_tuning.to_csv(DIR + 'McNemar_RoBERTa_split0_fine_tuning.csv')"
      ],
      "metadata": {
        "id": "EYcXdx2XxsgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Miscelaneous what if more than 512 tokens"
      ],
      "metadata": {
        "id": "0Xal5Rav6TNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "split0=splits[0]\n",
        "split0['X_train'] = prepare_lyrics(split0['X_train'] )\n",
        "split0['X_test'] = prepare_lyrics(split0['X_test'] )\n",
        "#SEQ_LEN=256\n",
        "SEQ_LEN=768\n",
        "print(split0['X_train'].shape, split0['X_test'].shape)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "Xids_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xmask_train = np.zeros((split0['X_train'].shape[0], SEQ_LEN))\n",
        "Xids_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "Xmask_test = np.zeros((split0['X_test'].shape[0], SEQ_LEN))\n",
        "\n",
        "for i, lyric in enumerate(split0['X_train']):\n",
        "  tokens = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\", add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_train[i,:], Xmask_train[i,:] = tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "for i, lyric in enumerate(split0['X_test']):\n",
        "  tokens_test = tokenizer.encode_plus(lyric, max_length=SEQ_LEN, truncation =True, padding=\"max_length\"\n",
        "    , add_special_tokens = True, return_token_type_ids= False, return_attention_mask = True, return_tensors= 'tf')\n",
        "  Xids_test[i,:], Xmask_test[i,:] = tokens_test['input_ids'], tokens_test['attention_mask']\n",
        "\n",
        "print(\"Xids_train.shape, Xids_test.shape: \",Xids_train.shape, Xids_test.shape)\n",
        "\n",
        "labels_train = np.zeros((split0['y_train'].shape[0], 10))\n",
        "labels_train[ np.arange(split0['y_train'].shape[0]), split0['y_train'].values] =1\n",
        "labels_test = np.zeros((split0['y_test'].shape[0], 10))\n",
        "labels_test[ np.arange(split0['y_test'].shape[0]), split0['y_test'].values] =1\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((Xids_train, Xmask_train, labels_train))\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((Xids_test, Xmask_test, labels_test))\n",
        "\n",
        "dataset_train = dataset_train.map(map_func)\n",
        "dataset_test = dataset_test.map(map_func)\n",
        "dataset_train = dataset_train.shuffle(42).batch(16)\n",
        "\n",
        "train = dataset_train.take(round(DS_LEN*SPLIT))\n",
        "val = dataset_train.skip(round(DS_LEN*SPLIT))\n",
        "test = dataset_test.batch(16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a576a01-3157-4b99-c8a0-362c7891c7c7",
        "id": "_KvyFk766Zat"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10832,) (2708,)\n",
            "Xids_train.shape, Xids_test.shape:  (10832, 768) (2708, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop_out_rate = 0.3\n",
        "learning_rate_transfer_learning =1e-3\n",
        "learning_rate_fine_tuning = 1e-5"
      ],
      "metadata": {
        "id": "EuC_yVed6Zat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "McNemar.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7adcd7-9fe4-4c17-9674-cd4a5ed870bc",
        "id": "SuSu7pHb6Zau"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([(False, 0.1, 0.001, 1e-05), (True, 0.1, 0.001, 1e-05), (False, 0.1, 0.001, 1e-06), (True, 0.1, 0.001, 1e-06), (False, 0.1, 0.0001, 1e-05), (True, 0.1, 0.0001, 1e-05), (False, 0.1, 0.0001, 1e-06), (True, 0.1, 0.0001, 1e-06), (False, 0.2, 0.001, 1e-05), (True, 0.2, 0.001, 1e-05), (False, 0.2, 0.001, 1e-06), (True, 0.2, 0.001, 1e-06), (False, 0.2, 0.0001, 1e-05), (True, 0.2, 0.0001, 1e-05), (False, 0.2, 0.0001, 1e-06), (True, 0.2, 0.0001, 1e-06), (False, 0.3, 0.001, 1e-05), (True, 0.3, 0.001, 1e-05), (False, 0.3, 0.001, 1e-06), (True, 0.3, 0.001, 1e-06), (False, 0.3, 0.0001, 1e-05), (True, 0.3, 0.0001, 1e-05), (False, 0.3, 0.0001, 1e-06), (True, 0.3, 0.0001, 1e-06), (False, 0.4, 0.001, 1e-05), (True, 0.4, 0.001, 1e-05), (False, 0.4, 0.001, 1e-06), (True, 0.4, 0.001, 1e-06), (False, 0.4, 0.0001, 1e-05), (True, 0.4, 0.0001, 1e-05), (False, 0.4, 0.0001, 1e-06), (True, 0.4, 0.0001, 1e-06), (False, 0.5, 0.001, 1e-05), (True, 0.5, 0.001, 1e-05), (False, 0.5, 0.001, 1e-06), (True, 0.5, 0.001, 1e-06), (False, 0.5, 0.0001, 1e-05), (True, 0.5, 0.0001, 1e-05), (False, 0.5, 0.0001, 1e-06), (True, 0.5, 0.0001, 1e-06)])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "\n",
        "#balanced_accuracies_transfer_learning = []\n",
        "#balanced_accuracies_fine_tuning = []\n",
        "SEQ_LEN2=SEQ_LEN\n",
        "\n",
        "\n",
        "print(\"learning_rate_transfer_learning: \",learning_rate_transfer_learning, \"learning_rate_fine_tuning: \"\n",
        ", learning_rate_fine_tuning, \"drop_out_rate:\", drop_out_rate)\n",
        "#step1\n",
        "bert_base = TFAutoModel.from_pretrained('bert-base-cased')\n",
        "input_ids = tf.keras.layers.Input(shape=(SEQ_LEN2,), name= 'input_ids', dtype='int32')\n",
        "mask = tf.keras.layers.Input(shape=(SEQ_LEN2,), name='attention_mask')\n",
        "\n",
        "embeddings = bert_base(input_ids, attention_mask= mask)[0]\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "X = tf.keras.layers.BatchNormalization()(X)\n",
        "X = tf.keras.layers.Dense(128, activation = 'relu')(X)\n",
        "X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "X = tf.keras.layers.Dense(32, activation = 'relu')(X)\n",
        "X = tf.keras.layers.BatchNormalization()(X)#added\n",
        "X = tf.keras.layers.Dropout(drop_out_rate)(X)\n",
        "y= tf.keras.layers.Dense(10, activation = 'softmax' , name= 'outputs')(X)\n",
        "\n",
        "model1 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "model1.layers[2].trainable = False\n",
        "print(model1.summary())\n",
        "\n",
        "#step2\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_transfer_learning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "#model2.summary() #Check trainable params increased.\n",
        "\n",
        "#step3: transfer learning\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3)\n",
        "history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "\n",
        "#step4: predict\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, False, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_transfer_learning.append( balanced_acc )\n",
        "\n",
        "#step5: fine tune\n",
        "print(\"Fine tuning---------------\")\n",
        "model1.layers[2].trainable = True\n",
        "\n",
        "# It's important to recompile your model after you make any changes\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate_fine_tuning)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(name='categorical_crossentropy')#from_logits=False,label_smoothing=0.0,axis=-1,\n",
        "metrics = []\n",
        "metrics.append(\n",
        "    tf.metrics.CategoricalAccuracy(name='categorical_accuracy', dtype=None))\n",
        "model1.compile(optimizer=optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n",
        "balanced_acc, McNemar=get_balanced_accuracy(model1, McNemar, True, drop_out_rate,learning_rate_transfer_learning, learning_rate_fine_tuning )\n",
        "balanced_accuracies_fine_tuning.append( balanced_acc )\n",
        "print(\"----------------------------------------\")\n",
        "del(model1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11a71750-5a44-474c-a475-82abf2eea418",
        "id": "xXxpxTZA6Zau"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate_transfer_learning:  0.001 learning_rate_fine_tuning:  1e-05 drop_out_rate: 0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 768)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 768)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_4 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 768,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Global  (None, 768)         0           ['tf_bert_model_4[0][0]']        \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 768)         3072        ['global_max_pooling1d_4[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 128)          98432       ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 128)         512         ['dense_8[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_193 (Dropout)          (None, 128)          0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 32)           4128        ['dropout_193[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 32)          128         ['dense_9[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_194 (Dropout)          (None, 32)           0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, 10)           330         ['dropout_194[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,416,874\n",
            "Trainable params: 104,746\n",
            "Non-trainable params: 108,312,128\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "609/609 [==============================] - 299s 470ms/step - loss: 2.4748 - categorical_accuracy: 0.1727 - val_loss: 2.2160 - val_categorical_accuracy: 0.1884\n",
            "Epoch 2/6\n",
            "609/609 [==============================] - 284s 466ms/step - loss: 2.2227 - categorical_accuracy: 0.2014 - val_loss: 2.1895 - val_categorical_accuracy: 0.1985\n",
            "Epoch 3/6\n",
            "609/609 [==============================] - 284s 466ms/step - loss: 2.1475 - categorical_accuracy: 0.2145 - val_loss: 2.0959 - val_categorical_accuracy: 0.2426\n",
            "Epoch 4/6\n",
            "609/609 [==============================] - 284s 466ms/step - loss: 2.1111 - categorical_accuracy: 0.2224 - val_loss: 2.1753 - val_categorical_accuracy: 0.1976\n",
            "Epoch 5/6\n",
            "609/609 [==============================] - 284s 466ms/step - loss: 2.1053 - categorical_accuracy: 0.2264 - val_loss: 2.0744 - val_categorical_accuracy: 0.2307\n",
            "Epoch 6/6\n",
            "609/609 [==============================] - 284s 467ms/step - loss: 2.0861 - categorical_accuracy: 0.2438 - val_loss: 2.1009 - val_categorical_accuracy: 0.2583\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.14      0.12      0.13       190\n",
            "         1.0       0.13      0.30      0.18       173\n",
            "         2.0       0.05      0.19      0.08       135\n",
            "         3.0       0.13      0.12      0.13       216\n",
            "         4.0       0.62      0.36      0.46      1036\n",
            "         5.0       0.07      0.32      0.11        76\n",
            "         6.0       0.55      0.70      0.61       195\n",
            "         7.0       0.14      0.02      0.03       290\n",
            "         8.0       0.10      0.04      0.05       139\n",
            "         9.0       0.13      0.07      0.09       258\n",
            "\n",
            "    accuracy                           0.25      2708\n",
            "   macro avg       0.20      0.22      0.19      2708\n",
            "weighted avg       0.34      0.25      0.27      2708\n",
            "\n",
            "0.22272934082829982\n",
            "Fine tuning---------------\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-b9b33deaf26b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_weight2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mbalanced_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMcNemar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_balanced_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMcNemar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_out_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate_transfer_learning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_fine_tuning\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mbalanced_accuracies_fine_tuning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbalanced_acc\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_4/tf_bert_model_4/bert/encoder/layer_._4/output/dropout_171/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1233, in inner\n      self.run()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 1147, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n      user_expressions, allow_stdin,\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 326, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-49-b9b33deaf26b>\", line 61, in <module>\n      history = model1.fit(train, validation_data=val, epochs=6, class_weight=my_weight2 ,callbacks=[ early_stopping])\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py\", line 1075, in run_call_with_unpacked_inputs\n      Prepare the output of the saved model. Each model must implement this function.\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 1103, in call\n      outputs = self.bert(\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py\", line 1075, in run_call_with_unpacked_inputs\n      Prepare the output of the saved model. Each model must implement this function.\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 863, in call\n      encoder_outputs = self.encoder(\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 511, in call\n      layer_output = self.bert_output(\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 431, in call\n      hidden_states = self.dropout(inputs=hidden_states, training=training)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/layers/core/dropout.py\", line 112, in call\n      lambda: tf.identity(inputs))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/control_flow_util.py\", line 106, in smart_cond\n      pred, true_fn=true_fn, false_fn=false_fn, name=name)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/layers/core/dropout.py\", line 109, in dropped_inputs\n      inputs, self.rate, noise_shape=self._get_noise_shape(inputs))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 1941, in dropout\n      seed=self.make_legacy_seed())\nNode: 'model_4/tf_bert_model_4/bert/encoder/layer_._4/output/dropout_171/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[16,768,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_4/tf_bert_model_4/bert/encoder/layer_._4/output/dropout_171/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_420764]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DWL5DlwVTHgV",
        "hWxfNJqYBfjD",
        "nONmunMa_h7T",
        "hb0KcNXhbUSN",
        "tkFQndSlD2y6",
        "sErIpS6Ed4-X",
        "UL1HA0eCNFuA",
        "6K_0Ui6OTMAn",
        "YlyEV7MauGPQ"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "background_execution": "on",
      "mount_file_id": "1_9VvPL8tPbXhyDvtVoKJzK1d-nx-oToD",
      "authorship_tag": "ABX9TyPoJ3TC4CO7dKR7CnIEZeZN",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d272c4ca403b4a47b1ece3efc81b4c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e504b3e620e44a5b2524210523370b9",
              "IPY_MODEL_fa78b894e25c444db500eb6f586219e7",
              "IPY_MODEL_4dce5494f1a544bfb236d816e815156c"
            ],
            "layout": "IPY_MODEL_a6baa10cad3c4cf297400c2a6c26836c"
          }
        },
        "1e504b3e620e44a5b2524210523370b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48bea4ef9e0b4ef7862cd2e3adedae03",
            "placeholder": "​",
            "style": "IPY_MODEL_af6987c103ca420a906441fb26a6ae34",
            "value": "Downloading tf_model.h5: 100%"
          }
        },
        "fa78b894e25c444db500eb6f586219e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d826796faf64e0bae02f9f62cbdae0c",
            "max": 526681800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8dde7f29f7740c58477e18a3b9a3ca6",
            "value": 526681800
          }
        },
        "4dce5494f1a544bfb236d816e815156c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d265b0a6d7f46d98d37f5f00c9ac6e0",
            "placeholder": "​",
            "style": "IPY_MODEL_3f6a1352f8dc4e6cb4d20628323e6d87",
            "value": " 502M/502M [00:07&lt;00:00, 68.1MB/s]"
          }
        },
        "a6baa10cad3c4cf297400c2a6c26836c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48bea4ef9e0b4ef7862cd2e3adedae03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af6987c103ca420a906441fb26a6ae34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d826796faf64e0bae02f9f62cbdae0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8dde7f29f7740c58477e18a3b9a3ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d265b0a6d7f46d98d37f5f00c9ac6e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f6a1352f8dc4e6cb4d20628323e6d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "494f3523b997428a8e07e2ff9f777278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7cc9f333c2b43c984854bf2a05c216b",
              "IPY_MODEL_799f47064001473b9e2926d8b9f2a1c3",
              "IPY_MODEL_60f03dcd1f494937924689fbabcb165c"
            ],
            "layout": "IPY_MODEL_ab592251173e4fdeb9db0ac8037abf05"
          }
        },
        "b7cc9f333c2b43c984854bf2a05c216b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_069364167ee44547b1d841e3d488898c",
            "placeholder": "​",
            "style": "IPY_MODEL_7c4eabafda57425980d8b661c5c4d4ef",
            "value": "Downloading tf_model.h5: 100%"
          }
        },
        "799f47064001473b9e2926d8b9f2a1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bb505313711439c87d7ffe87c59b58b",
            "max": 526681800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70a006728ee3429aa61eb3763bd2d601",
            "value": 526681800
          }
        },
        "60f03dcd1f494937924689fbabcb165c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd77628f26774bf4a5c03d5d92e6919b",
            "placeholder": "​",
            "style": "IPY_MODEL_eb67aaba5d62417d94b683b2f8254d31",
            "value": " 502M/502M [00:08&lt;00:00, 65.9MB/s]"
          }
        },
        "ab592251173e4fdeb9db0ac8037abf05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "069364167ee44547b1d841e3d488898c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c4eabafda57425980d8b661c5c4d4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bb505313711439c87d7ffe87c59b58b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70a006728ee3429aa61eb3763bd2d601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd77628f26774bf4a5c03d5d92e6919b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb67aaba5d62417d94b683b2f8254d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05ce66e4bd7045e5bfca4025886d219d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_909c9d4b575543879765d871847f2f6a",
              "IPY_MODEL_7e7090853d2d444b8f011659f2147ac3",
              "IPY_MODEL_fdf96e564ee4446caec4471bef691679"
            ],
            "layout": "IPY_MODEL_30bbac742239451b9f17c52b98695ec6"
          }
        },
        "909c9d4b575543879765d871847f2f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_763420132a35401c8865005eb4c3eb09",
            "placeholder": "​",
            "style": "IPY_MODEL_c3ae1df16fd9472696aedb4a64485167",
            "value": "Downloading tf_model.h5: 100%"
          }
        },
        "7e7090853d2d444b8f011659f2147ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8329ae8f3efe4931b1ae3690e1127319",
            "max": 526681800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7dda33ea712045baba20dff1f1f99512",
            "value": 526681800
          }
        },
        "fdf96e564ee4446caec4471bef691679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2671f8891a04b6aa692330f01d994f7",
            "placeholder": "​",
            "style": "IPY_MODEL_0ffd1a44c92544988c0a299b379ebbc8",
            "value": " 502M/502M [00:13&lt;00:00, 41.5MB/s]"
          }
        },
        "30bbac742239451b9f17c52b98695ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "763420132a35401c8865005eb4c3eb09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ae1df16fd9472696aedb4a64485167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8329ae8f3efe4931b1ae3690e1127319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dda33ea712045baba20dff1f1f99512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2671f8891a04b6aa692330f01d994f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ffd1a44c92544988c0a299b379ebbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a5dda3e451d4d858613f4d9c9b86d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c80020e048704b6baa9b4ca1c13fc511",
              "IPY_MODEL_112a6e4e0cba4acfaf914daf6fab9326",
              "IPY_MODEL_8bcd45b9e75d46c8b75503490e1b7b5d"
            ],
            "layout": "IPY_MODEL_cc733a6be3d341429436714ea26e949d"
          }
        },
        "c80020e048704b6baa9b4ca1c13fc511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a0792ee3d142aeb84f23125cd93790",
            "placeholder": "​",
            "style": "IPY_MODEL_580987b69f5c40738440211f9ae9cd84",
            "value": "Downloading tf_model.h5: 100%"
          }
        },
        "112a6e4e0cba4acfaf914daf6fab9326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f95af0bf95a4ee8b373395a621a11e2",
            "max": 526681800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e526f46d89b642fb8a2dd9a111f6e304",
            "value": 526681800
          }
        },
        "8bcd45b9e75d46c8b75503490e1b7b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78dcfc55a0dd4cdb8388c5afdb7f364a",
            "placeholder": "​",
            "style": "IPY_MODEL_0692e45fe98e433d92a78dd78c3246e5",
            "value": " 502M/502M [00:08&lt;00:00, 62.4MB/s]"
          }
        },
        "cc733a6be3d341429436714ea26e949d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a0792ee3d142aeb84f23125cd93790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "580987b69f5c40738440211f9ae9cd84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f95af0bf95a4ee8b373395a621a11e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e526f46d89b642fb8a2dd9a111f6e304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78dcfc55a0dd4cdb8388c5afdb7f364a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0692e45fe98e433d92a78dd78c3246e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c1765645a5248c19050d3aa66f45bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_549ea2b3fc784e76ae6318f67aadd130",
              "IPY_MODEL_ecff414594e943c0a608872a58484157",
              "IPY_MODEL_75e741671d414502acb7be89fb232eee"
            ],
            "layout": "IPY_MODEL_ad9dc752011e4dac88a41adf35e2a311"
          }
        },
        "549ea2b3fc784e76ae6318f67aadd130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_887c57500fa840fd9ea8764297bf865e",
            "placeholder": "​",
            "style": "IPY_MODEL_ba32373701ae4e96bbc46b0bde5858aa",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "ecff414594e943c0a608872a58484157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9c6c3933dd444a1b7bfadb2f07a47dd",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26d9077e2a974caa8a4fd0097eaa8706",
            "value": 29
          }
        },
        "75e741671d414502acb7be89fb232eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d21f0bf361434ef089b4e07f5990650f",
            "placeholder": "​",
            "style": "IPY_MODEL_77fe338695c24fe59a1c6b63ccfafb07",
            "value": " 29.0/29.0 [00:00&lt;00:00, 1.11kB/s]"
          }
        },
        "ad9dc752011e4dac88a41adf35e2a311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "887c57500fa840fd9ea8764297bf865e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba32373701ae4e96bbc46b0bde5858aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9c6c3933dd444a1b7bfadb2f07a47dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d9077e2a974caa8a4fd0097eaa8706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d21f0bf361434ef089b4e07f5990650f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77fe338695c24fe59a1c6b63ccfafb07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "922cd02f672d4e6b93fa43dd6496b72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89476f0f30f04203977b78b763f972e7",
              "IPY_MODEL_8dd855e7caf242ef94285d4b01499c1a",
              "IPY_MODEL_7ee4b508b7054255b7b62a6263ba7355"
            ],
            "layout": "IPY_MODEL_d2a7e285bb774938b04437a29e79f3db"
          }
        },
        "89476f0f30f04203977b78b763f972e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eafb86fe78a4aa986e2469f0386dab7",
            "placeholder": "​",
            "style": "IPY_MODEL_201ca7f3d2cf484485f807fecc065ec7",
            "value": "Downloading config.json: 100%"
          }
        },
        "8dd855e7caf242ef94285d4b01499c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2711411380834d158bdf6ac911aab51c",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6a527d3443949a0a611238b9ad71f03",
            "value": 570
          }
        },
        "7ee4b508b7054255b7b62a6263ba7355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8569aee413f845ef8fb9e2f54ca628b8",
            "placeholder": "​",
            "style": "IPY_MODEL_cbd3fc46977545b8bce278feb2c4f98e",
            "value": " 570/570 [00:00&lt;00:00, 24.2kB/s]"
          }
        },
        "d2a7e285bb774938b04437a29e79f3db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eafb86fe78a4aa986e2469f0386dab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "201ca7f3d2cf484485f807fecc065ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2711411380834d158bdf6ac911aab51c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6a527d3443949a0a611238b9ad71f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8569aee413f845ef8fb9e2f54ca628b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbd3fc46977545b8bce278feb2c4f98e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a68a5eda68b44daa744ee5b1509061d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ab65500833a4447bc6895d5033da58e",
              "IPY_MODEL_0fa0aba9b120410fa40babcd0c7919eb",
              "IPY_MODEL_5f555ea72bd94802b8d1b7b8bdcd88d3"
            ],
            "layout": "IPY_MODEL_ff5d27fe27cb4a72ad57ef058b65de4b"
          }
        },
        "6ab65500833a4447bc6895d5033da58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78138e4b3889467392a1964ac82d7b46",
            "placeholder": "​",
            "style": "IPY_MODEL_9d30cd6c250e4f84aa5d8dfa60b81d0d",
            "value": "Downloading vocab.txt: 100%"
          }
        },
        "0fa0aba9b120410fa40babcd0c7919eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0f9139412ea4867921dc3500945fc64",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed704d0e80a8470191b7b9b6c6211171",
            "value": 213450
          }
        },
        "5f555ea72bd94802b8d1b7b8bdcd88d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_226d0996b5ef4011b9281c5489dc59ad",
            "placeholder": "​",
            "style": "IPY_MODEL_941a467e91144143ac20a020c8e66061",
            "value": " 208k/208k [00:00&lt;00:00, 589kB/s]"
          }
        },
        "ff5d27fe27cb4a72ad57ef058b65de4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78138e4b3889467392a1964ac82d7b46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d30cd6c250e4f84aa5d8dfa60b81d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0f9139412ea4867921dc3500945fc64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed704d0e80a8470191b7b9b6c6211171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "226d0996b5ef4011b9281c5489dc59ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "941a467e91144143ac20a020c8e66061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa1704bfa1fa4915a1b3ab1635a44f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e19654922b841d79743ac7b8468ff70",
              "IPY_MODEL_4cd010eee588436e999f19c48f111be6",
              "IPY_MODEL_e3c313eade3a4bcda5eed3f17046aa83"
            ],
            "layout": "IPY_MODEL_9f77f0ba84a84972a80e8d584b9c642b"
          }
        },
        "8e19654922b841d79743ac7b8468ff70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6403a492ffc3402a906f07a76bb3b8ed",
            "placeholder": "​",
            "style": "IPY_MODEL_170cbac985ae47bf803ed7f514f7951f",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "4cd010eee588436e999f19c48f111be6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c316ba86dbf4d128bd954e7cbb163a4",
            "max": 435797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_273698cfffaf4a7cbc5764a3b99fca09",
            "value": 435797
          }
        },
        "e3c313eade3a4bcda5eed3f17046aa83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e92a070bb3d45e6a8d21ab377a375df",
            "placeholder": "​",
            "style": "IPY_MODEL_e4aceffa4e69459bb13bd2249a5f321d",
            "value": " 426k/426k [00:00&lt;00:00, 690kB/s]"
          }
        },
        "9f77f0ba84a84972a80e8d584b9c642b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6403a492ffc3402a906f07a76bb3b8ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "170cbac985ae47bf803ed7f514f7951f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c316ba86dbf4d128bd954e7cbb163a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "273698cfffaf4a7cbc5764a3b99fca09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e92a070bb3d45e6a8d21ab377a375df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4aceffa4e69459bb13bd2249a5f321d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8bf1f2345234645926a17891a323b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb69fdaa7f51452b82e992b42dc8f6f2",
              "IPY_MODEL_3ce60140a2f54f6e8e94511f436a45e2",
              "IPY_MODEL_1f506ec8d87142f9af36732e82cbfcac"
            ],
            "layout": "IPY_MODEL_3c5d59492b1848b981924edf5e7377dc"
          }
        },
        "cb69fdaa7f51452b82e992b42dc8f6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1167bd08c9fd4a64bb95ef503a0f280d",
            "placeholder": "​",
            "style": "IPY_MODEL_490a9ea2be2b4021bff975cd43fc6078",
            "value": "Downloading tf_model.h5: 100%"
          }
        },
        "3ce60140a2f54f6e8e94511f436a45e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cdbb72baaaa4fa2a82061e751fe039f",
            "max": 526681800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d54f090c4c34495b9084d4d468de4a5d",
            "value": 526681800
          }
        },
        "1f506ec8d87142f9af36732e82cbfcac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f78b3337fd884d68b282b84e919a7afa",
            "placeholder": "​",
            "style": "IPY_MODEL_917c4c2363d84bd6b5b8da904bcebcb4",
            "value": " 502M/502M [00:11&lt;00:00, 44.5MB/s]"
          }
        },
        "3c5d59492b1848b981924edf5e7377dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1167bd08c9fd4a64bb95ef503a0f280d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490a9ea2be2b4021bff975cd43fc6078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cdbb72baaaa4fa2a82061e751fe039f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d54f090c4c34495b9084d4d468de4a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f78b3337fd884d68b282b84e919a7afa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "917c4c2363d84bd6b5b8da904bcebcb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38ca8c3bfdbd4f068cd0a0f071d5aafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5dbf5a5e0ab41f591856404d580bfdb",
              "IPY_MODEL_ed5a91cdf95247d097e0ef4a217e20ac",
              "IPY_MODEL_b67897bb04f247a6bb13872e70ef36bb"
            ],
            "layout": "IPY_MODEL_b8dbb65b5543426097b2e68dc5dd8f25"
          }
        },
        "c5dbf5a5e0ab41f591856404d580bfdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3c4f29027c641aa986ec6cacdcc2f99",
            "placeholder": "​",
            "style": "IPY_MODEL_7447a858a8f348c4accaffe9d4117b8c",
            "value": "Downloading tf_model.h5: 100%"
          }
        },
        "ed5a91cdf95247d097e0ef4a217e20ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_630395be415f489ea019cb02992c8559",
            "max": 526681800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf40f129430941d7bfcb223da1727b2e",
            "value": 526681800
          }
        },
        "b67897bb04f247a6bb13872e70ef36bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b827dfe746444047a320a2512001eff6",
            "placeholder": "​",
            "style": "IPY_MODEL_d8cfad220b604bd1bd9b86f85d0ec571",
            "value": " 502M/502M [01:03&lt;00:00, 3.76MB/s]"
          }
        },
        "b8dbb65b5543426097b2e68dc5dd8f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3c4f29027c641aa986ec6cacdcc2f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7447a858a8f348c4accaffe9d4117b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "630395be415f489ea019cb02992c8559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf40f129430941d7bfcb223da1727b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b827dfe746444047a320a2512001eff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8cfad220b604bd1bd9b86f85d0ec571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}